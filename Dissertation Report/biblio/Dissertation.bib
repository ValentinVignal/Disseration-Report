
@book{noauthor_gansynth:_nodate,
	title = {{GANSynth}: {Making} music with {GANs}},
	shorttitle = {{GANSynth}},
	url = {https://magenta.tensorflow.org/gansynth},
	abstract = {In this post, we introduce GANSynth, a method for generating high-fidelity audio with Generative Adversarial Networks (GANs). Colab Notebook üéµAudio E...},
	language = {en},
	urldate = {2019-03-12},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\3\\gansynth.html:text/html}
}

@book{noauthor_music_nodate,
	title = {Music {Transformer}: {Generating} {Music} with {Long}-{Term} {Structure}},
	shorttitle = {Music {Transformer}},
	url = {https://magenta.tensorflow.org/music-transformer},
	abstract = {Generating long pieces of music is a challenging problem, as music containsstructure at multiple timescales, from milisecond timings to motifs to phrases t...},
	language = {en},
	urldate = {2019-03-12},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\5\\music-transformer.html:text/html}
}

@article{engel_gansynth:_2019,
	title = {{GANSYNTH}: {ADVERSARIAL} {NEURAL} {AUDIO} {SYNTHESIS}},
	language = {en},
	author = {Engel, Jesse and Agrawal, Kumar Krishna and Chen, Shuo and Gulrajani, Ishaan and Donahue, Chris and Roberts, Adam},
	year = {2019},
	pages = {17},
	file = {Engel et al. - 2019 - GANSYNTH ADVERSARIAL NEURAL AUDIO SYNTHESIS.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\6\\Engel et al. - 2019 - GANSYNTH ADVERSARIAL NEURAL AUDIO SYNTHESIS.pdf:application/pdf}
}

@book{noauthor_musicvae:_nodate,
	title = {{MusicVAE}: {Creating} a palette for musical scores with machine learning.},
	shorttitle = {{MusicVAE}},
	url = {https://magenta.tensorflow.org/music-vae},
	abstract = {When a painter creates a work of art, she first blends and explores color options on an artist‚Äôs palette before applying them to the canvas. This process is ...},
	language = {en},
	urldate = {2019-03-12},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\9\\music-vae.html:text/html}
}

@article{huang_music_2018,
	title = {Music {Transformer}},
	url = {http://arxiv.org/abs/1809.04281},
	abstract = {Music relies heavily on repetition to build structure and meaning. Self-reference occurs on multiple timescales, from motifs to phrases to reusing of entire sections of music, such as in pieces with ABA structure. The Transformer (Vaswani et al., 2017), a sequence model based on self-attention, has achieved compelling results in many generation tasks that require maintaining long-range coherence. This suggests that self-attention might also be well-suited to modeling music. In musical composition and performance, however, relative timing is critically important. Existing approaches for representing relative positional information in the Transformer modulate attention based on pairwise distance (Shaw et al., 2018). This is impractical for long sequences such as musical compositions since their memory complexity for intermediate relative information is quadratic in the sequence length. We propose an algorithm that reduces their intermediate memory requirement to linear in the sequence length. This enables us to demonstrate that a Transformer with our modified relative attention mechanism can generate minute-long compositions (thousands of steps, four times the length modeled in Oore et al., 2018) with compelling structure, generate continuations that coherently elaborate on a given motif, and in a seq2seq setup generate accompaniments conditioned on melodies. We evaluate the Transformer with our relative attention mechanism on two datasets, JSB Chorales and Piano-e-Competition, and obtain state-of-the-art results on the latter.},
	urldate = {2019-03-12},
	journal = {arXiv:1809.04281 [cs, eess, stat]},
	author = {Huang, Cheng-Zhi Anna and Vaswani, Ashish and Uszkoreit, Jakob and Shazeer, Noam and Simon, Ian and Hawthorne, Curtis and Dai, Andrew M. and Hoffman, Matthew D. and Dinculescu, Monica and Eck, Douglas},
	month = sep,
	year = {2018},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	annote = {arXiv: 1809.04281},
	annote = {Comment: Improved skewing section and accompanying figures. Previous titles are "An Improved Relative Self-Attention Mechanism for Transformer with Application to Music Generation" and "Music Transformer"},
	file = {arXiv\:1809.04281 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\13\\Huang et al. - 2018 - Music Transformer.pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\15\\1809.html:text/html}
}

@book{noauthor_gan_2017,
	title = {{GAN} {Deep} {Learning} {Architectures} - review},
	url = {https://sigmoidal.io/beginners-review-of-gan-architectures/},
	abstract = {GAN Deep Learning Architectures overview aims to give a comprehensive introduction to general ideas behind Generative Adversarial Networks, show you the main architectures that would be good starting points and provide you with an armory of tricks that would significantly improve your results.},
	language = {en-US},
	urldate = {2019-03-12},
	month = sep,
	year = {2017},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\14\\beginners-review-of-gan-architectures.html:text/html}
}

@article{nayebi_gruv:_nodate,
	title = {{GRUV}: {Algorithmic} {Music} {Generation} using {Recurrent} {Neural} {Networks}},
	abstract = {We compare the performance of two different types of recurrent neural networks (RNNs) for the task of algorithmic music generation, with audio waveforms as input. In particular, we focus on RNNs that have a sophisticated gating mechanism, namely, the Long Short-Term Memory (LSTM) network and the recently introduced Gated Recurrent Unit (GRU). Our results indicate that the generated outputs of the LSTM network were signiÔ¨Åcantly more musically plausible than those of the GRU.},
	language = {en},
	author = {Nayebi, Aran and Vitelli, Matt},
	pages = {6},
	file = {Nayebi et Vitelli - GRUV Algorithmic Music Generation using Recurrent.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\16\\Nayebi et Vitelli - GRUV Algorithmic Music Generation using Recurrent.pdf:application/pdf}
}

@book{noauthor_what_2018,
	title = {What the heck is time-series data (and why do {I} need a time-series database)?},
	url = {https://blog.timescale.com/what-the-heck-is-time-series-data-and-why-do-i-need-a-time-series-database-dcf3b1b18563/},
	abstract = {This article is a primer on time-series data and why you may not want to use a ‚Äúnormal‚Äù database to store it.},
	language = {en},
	urldate = {2019-03-12},
	month = nov,
	year = {2018},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\19\\what-the-heck-is-time-series-data-and-why-do-i-need-a-time-series-database-dcf3b1b18563.html:text/html}
}

@article{karras_progressive_2017,
	title = {Progressive {Growing} of {GANs} for {Improved} {Quality}, {Stability}, and {Variation}},
	url = {http://arxiv.org/abs/1710.10196},
	abstract = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024ÀÜ2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.},
	urldate = {2019-03-12},
	journal = {arXiv:1710.10196 [cs, stat]},
	author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
	month = oct,
	year = {2017},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {arXiv: 1710.10196},
	annote = {Comment: Final ICLR 2018 version},
	file = {arXiv\:1710.10196 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\22\\Karras et al. - 2017 - Progressive Growing of GANs for Improved Quality, .pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\23\\1710.html:text/html}
}

@article{goodfellow_nips_2016,
	title = {{NIPS} 2016 {Tutorial}: {Generative} {Adversarial} {Networks}},
	shorttitle = {{NIPS} 2016 {Tutorial}},
	url = {http://arxiv.org/abs/1701.00160},
	abstract = {This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
	urldate = {2019-03-12},
	journal = {arXiv:1701.00160 [cs]},
	author = {Goodfellow, Ian},
	month = dec,
	year = {2016},
	keywords = {Computer Science - Machine Learning},
	annote = {arXiv: 1701.00160},
	annote = {Comment: v2-v4 are all typo fixes. No substantive changes relative to v1},
	file = {arXiv\:1701.00160 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\29\\Goodfellow - 2016 - NIPS 2016 Tutorial Generative Adversarial Network.pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\30\\1701.html:text/html}
}

@article{doersch_tutorial_2016,
	title = {Tutorial on {Variational} {Autoencoders}},
	url = {http://arxiv.org/abs/1606.05908},
	abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
	urldate = {2019-03-12},
	journal = {arXiv:1606.05908 [cs, stat]},
	author = {Doersch, Carl},
	month = jun,
	year = {2016},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {arXiv: 1606.05908},
	file = {arXiv\:1606.05908 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\27\\Doersch - 2016 - Tutorial on Variational Autoencoders.pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\28\\1606.html:text/html}
}

@article{goodfellow_generative_2016,
	title = {Generative {Adversarial} {Networks} ({GANs})},
	language = {en},
	author = {Goodfellow, Ian},
	year = {2016},
	pages = {86},
	annote = {Slides de tutorial de GANs},
	file = {Goodfellow - 2016 - Generative Adversarial Networks (GANs).pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\32\\Goodfellow - 2016 - Generative Adversarial Networks (GANs).pdf:application/pdf}
}

@article{doersch_tutorial_2016-1,
	title = {Tutorial on {Variational} {Autoencoders}},
	url = {http://arxiv.org/abs/1606.05908},
	abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
	urldate = {2019-03-19},
	journal = {arXiv:1606.05908 [cs, stat]},
	author = {Doersch, Carl},
	month = jun,
	year = {2016},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {arXiv: 1606.05908},
	file = {arXiv\:1606.05908 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\37\\Doersch - 2016 - Tutorial on Variational Autoencoders.pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\38\\1606.html:text/html}
}

@article{oord_conditional_2016,
	title = {Conditional {Image} {Generation} with {PixelCNN} {Decoders}},
	url = {http://arxiv.org/abs/1606.05328},
	abstract = {This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.},
	urldate = {2019-03-19},
	journal = {arXiv:1606.05328 [cs]},
	author = {Oord, Aaron van den and Kalchbrenner, Nal and Vinyals, Oriol and Espeholt, Lasse and Graves, Alex and Kavukcuoglu, Koray},
	month = jun,
	year = {2016},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv: 1606.05328},
	file = {arXiv\:1606.05328 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\40\\Oord et al. - 2016 - Conditional Image Generation with PixelCNN Decoder.pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\41\\1606.html:text/html}
}

@article{yang_convolutional_2018,
	title = {Convolutional {Self}-{Attention} {Network}},
	url = {http://arxiv.org/abs/1810.13320},
	abstract = {Self-attention network (SAN) has recently attracted increasing interest due to its fully parallelized computation and flexibility in modeling dependencies. It can be further enhanced with multi-headed attention mechanism by allowing the model to jointly attend to information from different representation subspaces at different positions (Vaswani et al., 2017). In this work, we propose a novel convolutional self-attention network (CSAN), which offers SAN the abilities to 1) capture neighboring dependencies, and 2) model the interaction between multiple attention heads. Experimental results on WMT14 English-to-German translation task demonstrate that the proposed approach outperforms both the strong Transformer baseline and other existing works on enhancing the locality of SAN. Comparing with previous work, our model does not introduce any new parameters.},
	urldate = {2019-03-19},
	journal = {arXiv:1810.13320 [cs]},
	author = {Yang, Baosong and Wang, Longyue and Wong, Derek F. and Chao, Lidia S. and Tu, Zhaopeng},
	month = oct,
	year = {2018},
	keywords = {Computer Science - Computation and Language},
	annote = {arXiv: 1810.13320},
	file = {arXiv\:1810.13320 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\44\\Yang et al. - 2018 - Convolutional Self-Attention Network.pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\45\\1810.html:text/html}
}

@book{noauthor_self-attention_nodate,
	title = {Self-{Attention} {Mechanisms} in {Natural} {Language} {Processing}},
	url = {https://www.alibabacloud.com/blog/self-attention-mechanisms-in-natural-language-processing_593968},
	abstract = {This article explores various forms of Attention Mechanisms in Natural Language Processing (NLP) and their applications in multiple areas such as machine translation tasks.},
	language = {en},
	urldate = {2019-03-19},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\47\\self-attention-mechanisms-in-natural-language-processing_593968.html:text/html}
}

@book{jay_understanding_2018,
	title = {Understanding and {Implementing} {Architectures} of {ResNet} and {ResNeXt} for state-of-the-art {Image}‚Ä¶},
	url = {https://medium.com/@14prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-cf51669e1624},
	abstract = {In this two part blog post we will explore Residual networks. More specifically we will discuss three papers released by Microsoft Research‚Ä¶},
	urldate = {2019-03-20},
	author = {Jay, Prakash},
	month = feb,
	year = {2018},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\50\\understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-c.html:text/html}
}

@incollection{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf},
	urldate = {2019-03-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, ≈Åukasz and Polosukhin, Illia},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	pages = {5998--6008},
	file = {NIPS Full Text PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\52\\Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf;NIPS Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\53\\7181-attention-is-all-you-need.html:text/html}
}

@book{noauthor_attention_nodate,
	title = {Attention in {Neural} {Networks} and {How} to {Use} {It}},
	url = {http://akosiorek.github.io/ml/2017/10/14/visual-attention.html},
	urldate = {2019-03-20},
	file = {Attention in Neural Networks and How to Use It:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\56\\visual-attention.html:text/html}
}

@book{noauthor_attention?_nodate,
	title = {Attention? {Attention}!},
	url = {https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html},
	urldate = {2019-03-20},
	file = {Attention? Attention!:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\58\\attention-attention.html:text/html}
}

@book{synced_brief_2017,
	title = {A {Brief} {Overview} of {Attention} {Mechanism}},
	url = {https://medium.com/syncedreview/a-brief-overview-of-attention-mechanism-13c578ba9129},
	abstract = {What is Attention?},
	urldate = {2019-03-20},
	author = {{Synced}},
	month = sep,
	year = {2017},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\60\\a-brief-overview-of-attention-mechanism-13c578ba9129.html:text/html}
}

@book{britz_attention_2016,
	title = {Attention and {Memory} in {Deep} {Learning} and {NLP}},
	url = {http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/},
	abstract = {A recent trend in Deep Learning are Attention Mechanisms. In an interview, Ilya Sutskever, now the research director of OpenAI, mentioned that Attention Mechanisms are one of the most exciting adva‚Ä¶},
	language = {en-US},
	urldate = {2019-03-21},
	author = {Britz, Denny},
	month = jan,
	year = {2016},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\63\\attention-and-memory-in-deep-learning-and-nlp.html:text/html}
}

@article{huang_visualizing_nodate,
	title = {Visualizing {Music} {Self}-{Attention}},
	abstract = {Like language, music can be represented as a sequence of discrete symbols that form a hierarchical syntax, with notes being roughly like characters and motifs of notes like words. Unlike text however, music relies heavily on repetition on multiple timescales to build structure and meaning. The Music Transformer has shown compelling results in generating music with structure [3]. In this paper, we introduce a tool for visualizing self-attention on polyphonic music with an interactive pianoroll. We use music transformer as both a descriptive tool and a generative model. For the former, we use it to analyze existing music to see if the resulting self-attention structure corroborates with the musical structure known from music theory. For the latter, we inspect the model‚Äôs self-attention during generation, in order to understand how past notes affect future ones. We also compare and contrast the attention structure of regular attention to that of relative attention [6, 3], and examine its impact on the resulting generated music. For example, for the JSB Chorales dataset, a model trained with relative attention is more consistent in attending to all the voices in the preceding timestep and the chords before, and at cadences to the beginning of a phrase, allowing it to create an arc. We hope that our analyses will offer more evidence for relative self-attention as a powerful inductive bias for modeling music. We invite the reader to view our video animations of music attention and to interact with the visualizations at https:// storage.googleapis.com/nips-workshop-visualization/index.html.},
	language = {en},
	author = {Huang, Anna and Dinculescu, Monica and Eck, Douglas and Vaswani, Ashish},
	pages = {5},
	file = {Huang et al. - Visualizing Music Self-Attention.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\64\\Huang et al. - Visualizing Music Self-Attention.pdf:application/pdf}
}

@article{oord_wavenet:_2016,
	title = {{WaveNet}: {A} {Generative} {Model} for {Raw} {Audio}},
	shorttitle = {{WaveNet}},
	url = {http://arxiv.org/abs/1609.03499},
	abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	urldate = {2019-03-22},
	journal = {arXiv:1609.03499 [cs]},
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	month = sep,
	year = {2016},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
	annote = {arXiv: 1609.03499},
	file = {arXiv\:1609.03499 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\68\\Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\69\\1609.html:text/html}
}

@article{devlin_bert:_2018,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4\% (7.6\% absolute improvement), MultiNLI accuracy to 86.7 (5.6\% absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5\% absolute improvement), outperforming human performance by 2.0\%.},
	urldate = {2019-03-22},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = oct,
	year = {2018},
	keywords = {Computer Science - Computation and Language},
	annote = {arXiv: 1810.04805},
	annote = {Comment: 13 pages},
	file = {arXiv\:1810.04805 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\72\\Devlin et al. - 2018 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\73\\1810.html:text/html}
}

@article{huang_densely_2016,
	title = {Densely {Connected} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1608.06993},
	abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
	urldate = {2019-03-24},
	journal = {arXiv:1608.06993 [cs]},
	author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
	month = aug,
	year = {2016},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv: 1608.06993},
	annote = {Comment: CVPR 2017},
	file = {arXiv\:1608.06993 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\77\\Huang et al. - 2016 - Densely Connected Convolutional Networks.pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\78\\1608.html:text/html}
}

@book{chablani_densenet_2017,
	title = {{DenseNet}},
	url = {https://towardsdatascience.com/densenet-2810936aeebb},
	abstract = {Many papers:},
	urldate = {2019-03-24},
	author = {Chablani, Manish},
	month = aug,
	year = {2017},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\80\\densenet-2810936aeebb.html:text/html}
}

@book{noauthor_wavenet:_nodate,
	title = {{WaveNet}: {A} {Generative} {Model} for {Raw} {Audio}},
	shorttitle = {{WaveNet}},
	url = {https://deepmind.com/blog/wavenet-generative-model-raw-audio/},
	abstract = {This post presents WaveNet, a deep generative model of raw audio waveforms. We show that WaveNets are able to generate speech which mimics any human voice and which sounds more natural than the best existing Text-to-Speech systems, reducing the gap with human performance by over 50\%. We also demonstrate that the same network can be used to synthesize other audio signals such as music, and present some striking samples of automatically generated piano pieces.},
	urldate = {2019-03-28},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\83\\wavenet-generative-model-raw-audio.html:text/html}
}

@article{oord_wavenet:_2016-1,
	title = {{WaveNet}: {A} {Generative} {Model} for {Raw} {Audio}},
	shorttitle = {{WaveNet}},
	url = {http://arxiv.org/abs/1609.03499},
	abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	urldate = {2019-03-28},
	journal = {arXiv:1609.03499 [cs]},
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	month = sep,
	year = {2016},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
	annote = {arXiv: 1609.03499},
	file = {arXiv\:1609.03499 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\85\\Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\86\\1609.html:text/html}
}

@article{heusel_gans_2017,
	title = {{GANs} {Trained} by a {Two} {Time}-{Scale} {Update} {Rule} {Converge} to a {Local} {Nash} {Equilibrium}},
	url = {http://arxiv.org/abs/1706.08500},
	abstract = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the "Fr{\textbackslash}textbackslash'echet Inception Distance" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.},
	urldate = {2019-03-28},
	journal = {arXiv:1706.08500 [cs, stat]},
	author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
	month = jun,
	year = {2017},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {arXiv: 1706.08500},
	annote = {Comment: Implementations are available at: https://github.com/bioinf-jku/TTUR},
	file = {arXiv\:1706.08500 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\89\\Heusel et al. - 2017 - GANs Trained by a Two Time-Scale Update Rule Conve.pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\90\\1706.html:text/html}
}

@inproceedings{kikuchi_automatic_2014,
	address = {Porto, Portugal},
	title = {Automatic melody generation considering chord progression by genetic algorithm},
	isbn = {978-1-4799-5937-2 978-1-4799-5936-5},
	url = {http://ieeexplore.ieee.org/document/6921876/},
	doi = {10.1109/NaBIC.2014.6921876},
	abstract = {In this research, an automatic melody generation system considering chord progression by genetic algorithm is proposed. In the proposed automatic melody generation system, initial population are generated based on features on rhythm, pitch and chord progression of trained melody. In this system, the trained sample melody is divided into some melody blocks. Here, melody blocks mean verse, bridge, chorus and so on. And some new melodies are generated considering melody features in each block. The features on rhythm and pitch in each melody block of the sample melody are trained in some N-gram models, and they are used in order to calculate fitness in the melody generation by genetic algorithm. Some melodies are generated using the proposed system and confirmed that the proposed system can generate melodies considering features in each melody block of the trained sample melody.},
	language = {en},
	urldate = {2019-03-28},
	booktitle = {2014 {Sixth} {World} {Congress} on {Nature} and {Biologically} {Inspired} {Computing} ({NaBIC} 2014)},
	publisher = {IEEE},
	author = {Kikuchi, Motoki and Osana, Yuko},
	month = jul,
	year = {2014},
	pages = {190--195},
	annote = {Rhythm and pitch are assigned to each notes after that. For the rhythm : Markov Model : calculate the probability that a note continues. Pitch assigned randomly Quite complicate and very practival},
	file = {Kikuchi et Osana - 2014 - Automatic melody generation considering chord prog.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\92\\Kikuchi et Osana - 2014 - Automatic melody generation considering chord prog.pdf:application/pdf}
}

@book{noauthor_papers_nodate,
	title = {Papers {With} {Code} : {End}-to-end music source separation: is it possible in the waveform domain?},
	shorttitle = {Papers {With} {Code}},
	url = {http://paperswithcode.com/paper/end-to-end-music-source-separation-is-it},
	abstract = {Implemented in one code library. Click to access code and evaluation tables.},
	language = {en},
	urldate = {2019-03-29},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\96\\end-to-end-music-source-separation-is-it.html:text/html}
}

@article{lluis_end--end_2018,
	title = {End-to-end music source separation: is it possible in the waveform domain?},
	shorttitle = {End-to-end music source separation},
	url = {http://arxiv.org/abs/1810.12187},
	abstract = {Most of the currently successful source separation techniques use the magnitude spectrogram as input, and are therefore by default omitting part of the signal: the phase. In order to avoid omitting potentially useful information, we study the viability of using end-to-end models for music source separation. By operating directly over the waveform, these models take into account all the information available in the raw audio signal, including the phase. Our results show that waveform-based models can outperform a recent spectrogram-based deep learning model. Namely, a novel Wavenet-based model we propose and Wave-U-Net can outperform DeepConvSep, a spectrogram-based deep learning model. This suggests that end-to-end learning has a great potential for the problem of music source separation.},
	urldate = {2019-03-29},
	journal = {arXiv:1810.12187 [cs, eess]},
	author = {Llu√≠s, Francesc and Pons, Jordi and Serra, Xavier},
	month = oct,
	year = {2018},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {arXiv: 1810.12187},
	annote = {Comment: Code: https://github.com/francesclluis/source-separation-wavenet and Demo: http://jordipons.me/apps/end-to-end-music-source-separation/},
	file = {arXiv\:1810.12187 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\99\\Llu√≠s et al. - 2018 - End-to-end music source separation is it possible.pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\100\\1810.html:text/html}
}

@article{stoller_adversarial_2017,
	title = {Adversarial {Semi}-{Supervised} {Audio} {Source} {Separation} applied to {Singing} {Voice} {Extraction}},
	url = {http://arxiv.org/abs/1711.00048},
	abstract = {The state of the art in music source separation employs neural networks trained in a supervised fashion on multi-track databases to estimate the sources from a given mixture. With only few datasets available, often extensive data augmentation is used to combat overfitting. Mixing random tracks, however, can even reduce separation performance as instruments in real music are strongly correlated. The key concept in our approach is that source estimates of an optimal separator should be indistinguishable from real source signals. Based on this idea, we drive the separator towards outputs deemed as realistic by discriminator networks that are trained to tell apart real from separator samples. This way, we can also use unpaired source and mixture recordings without the drawbacks of creating unrealistic music mixtures. Our framework is widely applicable as it does not assume a specific network architecture or number of sources. To our knowledge, this is the first adoption of adversarial training for music source separation. In a prototype experiment for singing voice separation, separation performance increases with our approach compared to purely supervised training.},
	urldate = {2019-03-29},
	journal = {arXiv:1711.00048 [cs]},
	author = {Stoller, Daniel and Ewert, Sebastian and Dixon, Simon},
	month = oct,
	year = {2017},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, H.5.5, I.2.6},
	annote = {arXiv: 1711.00048},
	annote = {Comment: 5 pages, 2 figures, 1 table. Final version of manuscript accepted for 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Implementation available at https://github.com/f90/AdversarialAudioSeparation},
	file = {arXiv\:1711.00048 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\103\\Stoller et al. - 2017 - Adversarial Semi-Supervised Audio Source Separatio.pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\104\\1711.html:text/html}
}

@article{michelashvili_semi-supervised_2018,
	title = {Semi-{Supervised} {Monaural} {Singing} {Voice} {Separation} {With} a {Masking} {Network} {Trained} on {Synthetic} {Mixtures}},
	url = {http://arxiv.org/abs/1812.06087},
	abstract = {We study the problem of semi-supervised singing voice separation, in which the training data contains a set of samples of mixed music (singing and instrumental) and an unmatched set of instrumental music. Our solution employs a single mapping function g, which, applied to a mixed sample, recovers the underlying instrumental music, and, applied to an instrumental sample, returns the same sample. The network g is trained using purely instrumental samples, as well as on synthetic mixed samples that are created by mixing reconstructed singing voices with random instrumental samples. Our results indicate that we are on a par with or better than fully supervised methods, which are also provided with training samples of unmixed singing voices, and are better than other recent semi-supervised methods.},
	urldate = {2019-03-29},
	journal = {arXiv:1812.06087 [cs, eess, stat]},
	author = {Michelashvili, Michael and Benaim, Sagie and Wolf, Lior},
	month = dec,
	year = {2018},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	annote = {arXiv: 1812.06087},
	file = {arXiv\:1812.06087 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\106\\Michelashvili et al. - 2018 - Semi-Supervised Monaural Singing Voice Separation .pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\107\\1812.html:text/html}
}

@article{briot_deep_2017,
	title = {Deep {Learning} {Techniques} for {Music} {Generation} - {A} {Survey}},
	url = {http://arxiv.org/abs/1709.01620},
	abstract = {This paper is a survey and an analysis of different ways of using deep learning to generate musical content. We propose a methodology based on five dimensions: Objective - What musical content is to be generated? Examples are: melody, polyphony, accompaniment or counterpoint. - For what destination and for what use? To be performed by a human(s) (in the case of a musical score), or by a machine (in the case of an audio file). Representation - What are the concepts to be manipulated? Examples are: waveform, spectrogram, note, chord, meter and beat. - What format is to be used? Examples are: MIDI, piano roll or text. - How will the representation be encoded? Examples are: scalar, one-hot or many-hot. Architecture - What type(s) of deep neural network is (are) to be used? Examples are: feedforward network, recurrent network, autoencoder or generative adversarial networks. Challenges - What are the limitations and open challenges? Examples are: variability, interactivity and creativity. Strategy - How do we model and control the process of generation? Examples are: single-step feedforward, iterative feedforward, sampling or input manipulation. For each dimension, we conduct a comparative analysis of various models and techniques and propose some tentative multidimensional typology which is bottom-up, based on the analysis of many existing deep-learning based systems for music generation selected from the relevant literature. These systems are described and used to exemplify the various choices of objective, representation, architecture, challenges and strategies. The last part of the paper includes some discussion and some prospects. This is a simplified version (weak DRM) of the book: Briot, J.-P., Hadjeres, G. and Pachet, F.-D. (2019) Deep Learning Techniques for Music Generation, Computational Synthesis and Creative Systems, Springer.},
	urldate = {2019-04-02},
	journal = {arXiv:1709.01620 [cs]},
	author = {Briot, Jean-Pierre and Hadjeres, Ga√´tan and Pachet, Fran√ßois-David},
	month = sep,
	year = {2017},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
	annote = {arXiv: 1709.01620},
	annote = {Comment: 199 pages},
	file = {arXiv\:1709.01620 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\111\\Briot et al. - 2017 - Deep Learning Techniques for Music Generation - A .pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\112\\1709.html:text/html}
}

@article{chu_song_2017,
	title = {{SONG} {FROM} {PI}: {A} {MUSICALLY} {PLAUSIBLE} {NETWORK} {FOR} {POP} {MUSIC} {GENERATION}},
	abstract = {We present a novel framework for generating pop music. Our model is a hierarchical Recurrent Neural Network, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed. In particular, the bottom layers generate the melody, while the higher levels produce the drums and chords. We conduct several human studies that show strong preference of our generated music over that produced by the recent method by Google. We additionally show two applications of our framework: neural dancing and karaoke, as well as neural story singing.},
	language = {en},
	author = {Chu, Hang and Urtasun, Raquel and Fidler, Sanja},
	year = {2017},
	pages = {9},
	file = {Chu et al. - 2017 - SONG FROM PI A MUSICALLY PLAUSIBLE NETWORK FOR PO.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\113\\Chu et al. - 2017 - SONG FROM PI A MUSICALLY PLAUSIBLE NETWORK FOR PO.pdf:application/pdf}
}

@article{wiggins_computer_2007,
	title = {Computer {Models} of {Musical} {Creativity}: {A} {Review} of {Computer} {Models} of {Musical} {Creativity} by {David} {Cope}},
	volume = {23},
	issn = {0268-1145, 1477-4615},
	shorttitle = {Computer {Models} of {Musical} {Creativity}},
	url = {https://academic.oup.com/dsh/article-lookup/doi/10.1093/llc/fqm025},
	doi = {10.1093/llc/fqm025},
	language = {en},
	number = {1},
	urldate = {2019-04-02},
	journal = {Literary and Linguistic Computing},
	author = {Wiggins, G. A.},
	month = dec,
	year = {2007},
	pages = {109--116},
	file = {Wiggins - 2007 - Computer Models of Musical Creativity A Review of.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\115\\Wiggins - 2007 - Computer Models of Musical Creativity A Review of.pdf:application/pdf}
}

@book{tedx_talks_tedxgeorgiatech_nodate,
	title = {{TEDxGeorgiaTech} - {Gil} {Weinberg} - {Towards} {Robotic} {Musicianship}},
	url = {https://www.youtube.com/watch?v=9Lw7ax5a7lU},
	urldate = {2019-04-05},
	author = {{TEDx Talks}},
	annote = {Summary : }
}

@book{tedx_talks_tedxgeorgiatech_nodate-1,
	title = {{TEDxGeorgiaTech} - {Gil} {Weinberg} - {Towards} {Robotic} {Musicianship}},
	url = {https://www.youtube.com/watch?v=v5eUo2R_Lrc},
	urldate = {2019-04-05},
	author = {{TEDx Talks}}
}

@article{sahoo_learning_2018,
	title = {Learning {Equations} for {Extrapolation} and {Control}},
	url = {http://arxiv.org/abs/1806.07259},
	abstract = {We present an approach to identify concise equations from data using a shallow neural network approach. In contrast to ordinary black-box regression, this approach allows understanding functional relations and generalizing them from observed data to unseen parts of the parameter space. We show how to extend the class of learnable equations for a recently proposed equation learning network to include divisions, and we improve the learning and model selection strategy to be useful for challenging real-world data. For systems governed by analytical expressions, our method can in many cases identify the true underlying equation and extrapolate to unseen domains. We demonstrate its effectiveness by experiments on a cart-pendulum system, where only 2 random rollouts are required to learn the forward dynamics and successfully achieve the swing-up task.},
	urldate = {2019-04-11},
	journal = {arXiv:1806.07259 [cs, stat]},
	author = {Sahoo, Subham S. and Lampert, Christoph H. and Martius, Georg},
	month = jun,
	year = {2018},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, I.2.6, 62J02, 62M20, 65D15, 68T05, 68T30, 68T40, 70E60, 93C40, I.2.8},
	annote = {arXiv: 1806.07259},
	annote = {Comment: 9 pages, 9 figures, ICML 2018},
	annote = {This document introduce a architecture of a shallow neural network which can extrapolate function with cosinus and sinus in it and division. The functions are put in the neural network. and the last layer is calcuting the division part which is tricky because there are a lot of problem with the division per 0 and back propagation.},
	file = {arXiv\:1806.07259 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\126\\Sahoo et al. - 2018 - Learning Equations for Extrapolation and Control.pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\127\\1806.html:text/html}
}

@book{weinberg_survey_nodate,
	title = {A {Survey} of {Robotic} {Musicianship}},
	url = {https://cacm.acm.org/magazines/2016/5/201594-a-survey-of-robotic-musicianship/fulltext},
	abstract = {Reviewing the technologies that enable robot musicians to jam.},
	language = {en},
	urldate = {2019-04-11},
	author = {Weinberg, Gil, Mason Bretan},
	annote = {Talk about the mechanical challenges to create an actual musician robot with percussive/wind/string instruments Input of the system: audio and/or spectrogram Reconize the chords with "chroma" features : It projects the spectrogram into the 12 notes of an octave (chroma) and from the energy, it can decide which chord is played. Western music : can use HHM because the same sequence of chords can be often played Robot Shimon generates notes using Markov decision processes. The pitch and duration of their notes are modeled in 2nd order Markov chains},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\131\\fulltext.html:text/html}
}

@article{nikolaidis_generative_2012,
	title = {Generative {Musical} {Tension} {Modeling} and {Its} {Application} to {Dynamic} {Sonification}},
	volume = {36},
	issn = {0148-9267, 1531-5169},
	url = {http://www.mitpressjournals.org/doi/10.1162/COMJ_a_00105},
	doi = {10.1162/COMJ_a_00105},
	language = {en},
	number = {1},
	urldate = {2019-04-11},
	journal = {Computer Music Journal},
	author = {Nikolaidis, Ryan and Walker, Bruce and Weinberg, Gil},
	month = mar,
	year = {2012},
	pages = {55--64},
	file = {Nikolaidis et al. - 2012 - Generative Musical Tension Modeling and Its Applic.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\133\\Nikolaidis et al. - 2012 - Generative Musical Tension Modeling and Its Applic.pdf:application/pdf}
}

@inproceedings{hoffman_shimon:_2010,
	address = {Atlanta, Georgia, USA},
	title = {Shimon: an interactive improvisational robotic marimba player},
	isbn = {978-1-60558-930-5},
	shorttitle = {Shimon},
	url = {http://portal.acm.org/citation.cfm?doid=1753846.1753925},
	doi = {10.1145/1753846.1753925},
	abstract = {Shimon is an autonomous marimba-playing robot designed to create interactions with human players that lead to novel musical outcomes. The robot combines music perception, interaction, and improvisation with the capacity to produce melodic and harmonic acoustic responses through choreographic gestures. We developed an anticipatory action framework, and a gesture-based behavior system, allowing the robot to play improvised Jazz with humans in synchrony, fluently, and without delay. In addition, we built an expressive non-humanoid head for musical social communication. This paper describes our system, used in a performance and demonstration at the CHI 2010 Media Showcase.},
	language = {en},
	urldate = {2019-04-12},
	booktitle = {Proceedings of the 28th of the international conference extended abstracts on {Human} factors in computing systems - {CHI} {EA} '10},
	publisher = {ACM Press},
	author = {Hoffman, Guy and Weinberg, Gil},
	year = {2010},
	pages = {3097},
	annote = {It says the visual feedback is important in music to anticipate and communicate with the others musician. It is why a robot playing a real instrument is pertinent. Works with audio : classify the chords },
	file = {Hoffman et Weinberg - 2010 - Shimon an interactive improvisational robotic mar.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\138\\Hoffman et Weinberg - 2010 - Shimon an interactive improvisational robotic mar.pdf:application/pdf}
}

@article{fernandez_ai_2013,
	title = {{AI} {Methods} in {Algorithmic} {Composition}: {A} {Comprehensive} {Survey}},
	volume = {48},
	issn = {1076-9757},
	shorttitle = {{AI} {Methods} in {Algorithmic} {Composition}},
	url = {https://jair.org/index.php/jair/article/view/10845},
	doi = {10.1613/jair.3908},
	abstract = {Algorithmic composition is the partial or total automation of the process of music composition by using computers. Since the 1950s, diÔ¨Äerent computational techniques related to ArtiÔ¨Åcial Intelligence have been used for algorithmic composition, including grammatical representations, probabilistic methods, neural networks, symbolic rule-based systems, constraint programming and evolutionary algorithms. This survey aims to be a comprehensive account of research on algorithmic composition, presenting a thorough view of the Ô¨Åeld for researchers in ArtiÔ¨Åcial Intelligence.},
	language = {en},
	urldate = {2019-04-13},
	journal = {Journal of Artificial Intelligence Research},
	author = {Fernandez, J.D. and Vico, F.},
	month = nov,
	year = {2013},
	pages = {513--582},
	file = {Fernandez et Vico - 2013 - AI Methods in Algorithmic Composition A Comprehen.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\142\\Fernandez et Vico - 2013 - AI Methods in Algorithmic Composition A Comprehen.pdf:application/pdf}
}

@book{noauthor_automatic_nodate,
	title = {Automatic {Melody} {Generation} considering {Chord} {Progression} by {Genetic} {Algorithm}},
	file = {Automatic Melody Generation considering Chord Prog.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\145\\Automatic Melody Generation considering Chord Prog.pdf:application/pdf}
}

@article{wakui_automatic_nodate,
	title = {Automatic {Melody} {Generation} considering {Chord} {Progression} using {Genetic} {Algorithm}},
	abstract = {In this paper, we propose an automatic melody generation system considering chord progression. In the proposed system, chord progression and rhythm sequence are generated randomly, and the pitch is assigned to each note using genetic algorithm. We carried out a series of computer experiments, and we conÔ¨Årmed that melodies can be generated by the proposed system.},
	language = {en},
	author = {Wakui, Yudai and Hatori, Yoshinori and Osana, Yuko},
	pages = {4},
	annote = {For each chords, they creates a table of possible notes. They create a random rhythm and the pitch decide with a genetic algorithm and some fitness functions.},
	file = {Wakui et al. - Automatic Melody Generation considering Chord Prog.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\146\\Wakui et al. - Automatic Melody Generation considering Chord Prog.pdf:application/pdf}
}

@book{noauthor_cs4347_nodate,
	title = {{CS4347} {Acoustic} {Scene} {Classifier} {Report}},
	url = {https://fr.overleaf.com/project/5cb81a5a70921e1466432a94},
	abstract = {Un √©diteur LaTeX en ligne facile √† utiliser. Pas d'installation, collaboration en temps r√©el, gestion des versions, des centaines de mod√®les de documents LaTeX, et plus encore.},
	language = {fr},
	urldate = {2019-04-20},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\151\\5cb81a5a70921e1466432a94.html:text/html}
}

@book{chablani_densenet_2017-1,
	title = {{DenseNet}},
	url = {https://towardsdatascience.com/densenet-2810936aeebb},
	abstract = {Many papers:},
	urldate = {2019-04-20},
	author = {Chablani, Manish},
	month = aug,
	year = {2017},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\153\\densenet-2810936aeebb.html:text/html}
}

@book{noauthor_acm_nodate,
	title = {{ACM} {Master} {Article} {Template}},
	url = {https://www.acm.org/publications/proceedings-template},
	abstract = {ACM Master Article Template},
	language = {en},
	urldate = {2019-04-20},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\155\\proceedings-template.html:text/html}
}

@article{surname_insert_nodate,
	title = {Insert {Your} {Title} {Here}},
	abstract = {In this sample-structured document, neither the cross-linking of float elements and bibliography nor metadata/copyright information is available. The sample document is provided in ‚ÄúDraft‚Äù mode and to view it in the final layout format, applying the required template is essential with some standard steps.},
	language = {en},
	author = {Surname, FirstName and Surname, FirstName and Surname, FirstName},
	pages = {2},
	file = {Surname et al. - Insert Your Title Here.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\156\\Surname et al. - Insert Your Title Here.pdf:application/pdf}
}

@book{noauthor_dcase2018_nodate,
	title = {{DCASE2018} {Challenge} - {DCASE}},
	url = {http://dcase.community/challenge2018/index},
	urldate = {2019-04-20},
	file = {DCASE2018 Challenge - DCASE:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\159\\index.html:text/html}
}

@article{eronen_audio-based_2006,
	title = {Audio-based context recognition},
	volume = {14},
	issn = {1558-7916},
	url = {http://ieeexplore.ieee.org/document/1561288/},
	doi = {10.1109/TSA.2005.854103},
	abstract = {The aim of this paper is to investigate the feasibility of an audio-based context recognition system. Here, context recognition refers to the automatic classiÔ¨Åcation of the context or an environment around a device. A system is developed and compared to the accuracy of human listeners in the same task. Particular emphasis is placed on the computational complexity of the methods, since the application is of particular interest in resource-constrained portable devices. Simplistic low-dimensional feature vectors are evaluated against more standard spectral features. Using discriminative training, competitive recognition accuracies are achieved with very low-order hidden Markov models (1‚Äì3 Gaussian components). Slight improvement in recognition accuracy is observed when linear data-driven feature transformations are applied to mel-cepstral features. The recognition rate of the system as a function of the test sequence length appears to converge only after about 30 to 60 s. Some degree of accuracy can be achieved even with less than 1-s test sequence lengths. The average reaction time of the human listeners was 14 s, i.e., somewhat smaller, but of the same order as that of the system. The average recognition accuracy of the system was 58\% against 69\%, obtained in the listening tests in recognizing between 24 everyday contexts. The accuracies in recognizing six high-level classes were 82\% for the system and 88\% for the subjects.},
	language = {en},
	number = {1},
	urldate = {2019-04-20},
	journal = {IEEE Transactions on Audio, Speech and Language Processing},
	author = {Eronen, A.J. and Peltonen, V.T. and Tuomi, J.T. and Klapuri, A.P. and Fagerlund, S. and Sorsa, T. and Lorho, G. and Huopaniemi, J.},
	month = jan,
	year = {2006},
	pages = {321--329},
	file = {Eronen et al. - 2006 - Audio-based context recognition.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\160\\Eronen et al. - 2006 - Audio-based context recognition.pdf:application/pdf}
}

@book{noauthor_[1411.3715]_nodate,
	title = {[1411.3715] {Acoustic} {Scene} {Classification}},
	url = {https://arxiv.org/abs/1411.3715},
	urldate = {2019-04-20},
	file = {[1411.3715] Acoustic Scene Classification:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\163\\1411.html:text/html}
}

@article{mesaros_detection_2018,
	title = {Detection and {Classification} of {Acoustic} {Scenes} and {Events}: {Outcome} of the {DCASE} 2016 {Challenge}},
	volume = {26},
	issn = {2329-9290},
	shorttitle = {Detection and {Classification} of {Acoustic} {Scenes} and {Events}},
	doi = {10.1109/TASLP.2017.2778423},
	abstract = {Public evaluation campaigns and datasets promote active development in target research areas, allowing direct comparison of algorithms. The second edition of the challenge on detection and classification of acoustic scenes and events (DCASE 2016) has offered such an opportunity for development of the state-of-the-art methods, and succeeded in drawing together a large number of participants from academic and industrial backgrounds. In this paper, we report on the tasks and outcomes of the DCASE 2016 challenge. The challenge comprised four tasks: acoustic scene classification, sound event detection in synthetic audio, sound event detection in real-life audio, and domestic audio tagging. We present each task in detail and analyze the submitted systems in terms of design and performance. We observe the emergence of deep learning as the most popular classification method, replacing the traditional approaches based on Gaussian mixture models and support vector machines. By contrast, feature representations have not changed substantially throughout the years, as mel frequency-based representations predominate in all tasks. The datasets created for and used in DCASE 2016 are publicly available and are a valuable resource for further research.},
	number = {2},
	journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
	author = {Mesaros, A. and Heittola, T. and Benetos, E. and Foster, P. and Lagrange, M. and Virtanen, T. and Plumbley, M. D.},
	month = feb,
	year = {2018},
	keywords = {acoustic scene classification, Acoustic scene classification, acoustic scenes, acoustic signal processing, Acoustics, audio datasets, audio signal processing, classification method, DCASE 2016 challenge, deep learning, domestic audio tagging, Event detection, feature extraction, Gaussian mixture models, Hidden Markov models, learning (artificial intelligence), pattern recognition, public evaluation campaigns, signal classification, sound event detection, Speech, Speech processing, support vector machines, synthetic audio, Tagging},
	pages = {379--393},
	file = {IEEE Xplore Abstract Record:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\165\\8123864.html:text/html;Version soumise:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\167\\Mesaros et al. - 2018 - Detection and Classification of Acoustic Scenes an.pdf:application/pdf}
}

@article{lagrange_bag--frames_2015,
	title = {The bag-of-frames approach: {A} not so sufficient model for urban soundscapes},
	volume = {138},
	issn = {0001-4966},
	shorttitle = {The bag-of-frames approach},
	url = {https://asa.scitation.org/doi/10.1121/1.4935350},
	doi = {10.1121/1.4935350},
	abstract = {The ‚Äúbag-of-frames‚Äù (BOF) approach, which encodes audio signals as the long-term statistical distribution of short-term spectral features, is commonly regarded as an effective and sufficient way to represent environmental sound recordings (soundscapes). The present paper describes a conceptual replication of a use of the BOF approach in a seminal article using several other soundscape datasets, with results strongly questioning the adequacy of the BOF approach for the task. As demonstrated in this paper, the good accuracy originally reported with BOF likely resulted from a particularly permissive dataset with low within-class variability. Soundscape modeling, therefore, may not be the closed case it was once thought to be.},
	number = {5},
	urldate = {2019-04-20},
	journal = {The Journal of the Acoustical Society of America},
	author = {Lagrange, Mathieu and Lafay, Gr√©goire and D√©fr√©ville, Boris and Aucouturier, Jean-Julien},
	month = nov,
	year = {2015},
	pages = {EL487--EL492},
	file = {Full Text PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\168\\Lagrange et al. - 2015 - The bag-of-frames approach A not so sufficient mo.pdf:application/pdf;Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\169\\1.html:text/html}
}

@book{ruiz_understanding_2018,
	title = {Understanding and visualizing {DenseNets}},
	url = {https://towardsdatascience.com/understanding-and-visualizing-densenets-7f688092391a},
	abstract = {This post be found in PDF here.},
	urldate = {2019-04-20},
	author = {Ruiz, Pablo Ruiz},
	month = oct,
	year = {2018},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\171\\understanding-and-visualizing-densenets-7f688092391a.html:text/html}
}

@book{tsang_review:_2018,
	title = {Review: {DenseNet} ‚Äî {Dense} {Convolutional} {Network} ({Image} {Classification})},
	shorttitle = {Review},
	url = {https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803},
	abstract = {In this story, DenseNet (Dense Convolutional Network) is reviewed. This is the paper in 2017 CVPR which got Best Paper Award with over 2000‚Ä¶},
	urldate = {2019-04-20},
	author = {Tsang, Sik-Ho},
	month = nov,
	year = {2018},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\173\\review-densenet-image-classification-b6631a8ef803.html:text/html}
}

@article{huang_densely_2016-1,
	title = {Densely {Connected} {Convolutional} {Networks}},
	url = {https://arxiv.org/abs/1608.06993v5},
	abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
	language = {en},
	urldate = {2019-04-20},
	author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
	month = aug,
	year = {2016},
	file = {Full Text PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\175\\Huang et al. - 2016 - Densely Connected Convolutional Networks.pdf:application/pdf;Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\176\\1608.html:text/html}
}

@article{huang_densely_2016-2,
	title = {Densely {Connected} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1608.06993},
	abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
	urldate = {2019-04-20},
	journal = {arXiv:1608.06993 [cs]},
	author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
	month = aug,
	year = {2016},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv: 1608.06993},
	annote = {Comment: CVPR 2017},
	file = {arXiv\:1608.06993 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\179\\Huang et al. - 2016 - Densely Connected Convolutional Networks.pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\180\\1608.html:text/html}
}

@article{jegou_one_2016,
	title = {The {One} {Hundred} {Layers} {Tiramisu}: {Fully} {Convolutional} {DenseNets} for {Semantic} {Segmentation}},
	shorttitle = {The {One} {Hundred} {Layers} {Tiramisu}},
	url = {http://arxiv.org/abs/1611.09326},
	abstract = {State-of-the-art approaches for semantic image segmentation are built on Convolutional Neural Networks (CNNs). The typical segmentation architecture is composed of (a) a downsampling path responsible for extracting coarse semantic features, followed by (b) an upsampling path trained to recover the input image resolution at the output of the model and, optionally, (c) a post-processing module (e.g. Conditional Random Fields) to refine the model predictions. Recently, a new CNN architecture, Densely Connected Convolutional Networks (DenseNets), has shown excellent results on image classification tasks. The idea of DenseNets is based on the observation that if each layer is directly connected to every other layer in a feed-forward fashion then the network will be more accurate and easier to train. In this paper, we extend DenseNets to deal with the problem of semantic segmentation. We achieve state-of-the-art results on urban scene benchmark datasets such as CamVid and Gatech, without any further post-processing module nor pretraining. Moreover, due to smart construction of the model, our approach has much less parameters than currently published best entries for these datasets. Code to reproduce the experiments is available here : https://github.com/SimJeg/FC-DenseNet/blob/master/train.py},
	urldate = {2019-04-20},
	journal = {arXiv:1611.09326 [cs]},
	author = {J√©gou, Simon and Drozdzal, Michal and Vazquez, David and Romero, Adriana and Bengio, Yoshua},
	month = nov,
	year = {2016},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {arXiv: 1611.09326},
	file = {arXiv\:1611.09326 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\184\\J√©gou et al. - 2016 - The One Hundred Layers Tiramisu Fully Convolution.pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\185\\1611.html:text/html}
}

@article{zhu_densenet_2017,
	title = {{DenseNet} for {Dense} {Flow}},
	url = {http://arxiv.org/abs/1707.06316},
	abstract = {Classical approaches for estimating optical flow have achieved rapid progress in the last decade. However, most of them are too slow to be applied in real-time video analysis. Due to the great success of deep learning, recent work has focused on using CNNs to solve such dense prediction problems. In this paper, we investigate a new deep architecture, Densely Connected Convolutional Networks (DenseNet), to learn optical flow. This specific architecture is ideal for the problem at hand as it provides shortcut connections throughout the network, which leads to implicit deep supervision. We extend current DenseNet to a fully convolutional network to learn motion estimation in an unsupervised manner. Evaluation results on three standard benchmarks demonstrate that DenseNet is a better fit than other widely adopted CNN architectures for optical flow estimation.},
	urldate = {2019-04-20},
	journal = {arXiv:1707.06316 [cs]},
	author = {Zhu, Yi and Newsam, Shawn},
	month = jul,
	year = {2017},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Multimedia},
	annote = {arXiv: 1707.06316},
	annote = {Comment: Accepted at ICIP 2017},
	file = {arXiv\:1707.06316 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\186\\Zhu et Newsam - 2017 - DenseNet for Dense Flow.pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\187\\1707.html:text/html}
}

@article{srivastava_highway_2015,
	title = {Highway {Networks}},
	url = {https://arxiv.org/abs/1505.00387v2},
	abstract = {There is plenty of theoretical and empirical evidence that depth of neural networks is a crucial ingredient for their success. However, network training becomes more difficult with increasing depth and training of very deep networks remains an open problem. In this extended abstract, we introduce a new architecture designed to ease gradient-based training of very deep networks. We refer to networks with this architecture as highway networks, since they allow unimpeded information flow across several layers on "information highways". The architecture is characterized by the use of gating units which learn to regulate the flow of information through a network. Highway networks with hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions, opening up the possibility of studying extremely deep and efficient architectures.},
	language = {en},
	urldate = {2019-04-20},
	author = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, J√ºrgen},
	month = may,
	year = {2015},
	file = {Full Text PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\189\\Srivastava et al. - 2015 - Highway Networks.pdf:application/pdf;Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\190\\1505.html:text/html}
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {https://arxiv.org/abs/1512.03385v1},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers‚Äî8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	language = {en},
	urldate = {2019-04-20},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	file = {Full Text PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\192\\He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\193\\1512.html:text/html}
}

@article{larsson_fractalnet:_2016,
	title = {{FractalNet}: {Ultra}-{Deep} {Neural} {Networks} without {Residuals}},
	shorttitle = {{FractalNet}},
	url = {https://arxiv.org/abs/1605.07648v4},
	abstract = {We introduce a design strategy for neural network macro-architecture based on self-similarity. Repeated application of a simple expansion rule generates deep networks whose structural layouts are precisely truncated fractals. These networks contain interacting subpaths of different lengths, but do not include any pass-through or residual connections; every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers. In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks. Rather, the key may be the ability to transition, during training, from effectively shallow to deep. We note similarities with student-teacher behavior and develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures. Such regularization allows extraction of high-performance fixed-depth subnetworks. Additionally, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer.},
	language = {en},
	urldate = {2019-04-20},
	author = {Larsson, Gustav and Maire, Michael and Shakhnarovich, Gregory},
	month = may,
	year = {2016},
	file = {Full Text PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\195\\Larsson et al. - 2016 - FractalNet Ultra-Deep Neural Networks without Res.pdf:application/pdf;Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\196\\1605.html:text/html}
}

@article{dubnov_system_nodate,
	title = {A {System} for {Computer} {Music} {Generation} by {Learning} and {Improvisation} in a {Particular} {Style}},
	abstract = {The paper deals with question of music modeling and generation, capturing the idiomatic style of a composer or a genre, in an attempt to provide the computer with ‚Äúcreative‚Äù, human-like capabilities. The system described in the paper performs analyses of musical sequences and computes a model according to which new interpretations / improvisations close to the original's style can be generated. Important aspects of the musical structure are captured, including rhythm, melodic contour, and polyphonic relationships. Two statistical learning methods are implemented and their performance is compared in the paper: The first method is based on ‚ÄúUniversal Prediction‚Äù, a method derived from Information Theory and previously shown to be useful for statistical modeling in the musical domain. The second method is an application to music of Prediction Suffix Trees (PST), a learning technique initially developed for statistical modeling of complex sequences with applications in Linguistics and Computational Biology. Both methods provide a stochastic representation of musical style and are able to capture musical structures of variable lengths, from short ornamentations to complete musical phrases. Operations such as improvisation or computer aided composition can be realized using the system.},
	language = {en},
	author = {Dubnov, S and Assayag, G and Bejerano, G and Lartillot, O},
	pages = {15},
	file = {Dubnov et al. - A System for Computer Music Generation by Learning.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\199\\Dubnov et al. - A System for Computer Music Generation by Learning.pdf:application/pdf}
}

@book{arxiv_listen_nodate,
	title = {Listen to this classical music composed in the style of {Bach} by a deep-learning machine},
	url = {https://www.technologyreview.com/s/603137/deep-learning-machine-listens-to-bach-then-writes-its-own-music-in-the-same-style/},
	abstract = {Can you tell the difference between music composed by Bach and by a neural network?},
	language = {en-US},
	urldate = {2019-05-06},
	author = {arXiv, Emerging Technology from the},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\204\\deep-learning-machine-listens-to-bach-then-writes-its-own-music-in-the-same-style.html:text/html}
}

@article{hadjeres_deepbach:_2016,
	title = {{DeepBach}: a {Steerable} {Model} for {Bach} {Chorales} {Generation}},
	shorttitle = {{DeepBach}},
	url = {http://arxiv.org/abs/1612.01010},
	abstract = {This paper introduces DeepBach, a graphical model aimed at modeling polyphonic music and specifically hymn-like pieces. We claim that, after being trained on the chorale harmonizations by Johann Sebastian Bach, our model is capable of generating highly convincing chorales in the style of Bach. DeepBach's strength comes from the use of pseudo-Gibbs sampling coupled with an adapted representation of musical data. This is in contrast with many automatic music composition approaches which tend to compose music sequentially. Our model is also steerable in the sense that a user can constrain the generation by imposing positional constraints such as notes, rhythms or cadences in the generated score. We also provide a plugin on top of the MuseScore music editor making the interaction with DeepBach easy to use.},
	urldate = {2019-05-06},
	journal = {arXiv:1612.01010 [cs]},
	author = {Hadjeres, Ga√´tan and Pachet, Fran√ßois and Nielsen, Frank},
	month = dec,
	year = {2016},
	keywords = {Computer Science - Sound, Computer Science - Artificial Intelligence},
	annote = {arXiv: 1612.01010},
	annote = {Comment: 10 pages, ICML2017 version},
	annote = {Use RNN forward and backward Very small (on 4 beats = 16 notes) Representation of note : just the name, simple rhythm and simple No silence, no notes shorter than ¬º beat Always 4 notes (because 4 voices)},
	file = {arXiv\:1612.01010 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\207\\Hadjeres et al. - 2016 - DeepBach a Steerable Model for Bach Chorales Gene.pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\208\\1612.html:text/html}
}

@inproceedings{karishma_cluster_2015,
	title = {Cluster {Based} {Melody} {Generation}},
	abstract = {This project describes the development of an application for generating tonal melodies. The goal of the project is to ascertain our current understanding of tonal music by means of algorithmic music generation. The method followed consists of four stages: 1) selection of music-theoretical insights, 2) translation of these insights into a set of principles, 3) conversion of the principles into a computational model having the form of an algorithm for music generation, 4) testing the ‚Äúmusic‚Äù generated by the algorithm to evaluate the adequacy of the model. As an example, the method is implemented in Melody Generator, an algorithm for generating tonal melodies. The program has a structure suited for generating, displaying, playing and storing melodies, functions which are all accessible via a dedicated interface. The actual generation of melodies, is based in part on constraints imposed by the tonal context, i.e. by meter and key, the settings of which are controlled by means of parameters on the interface. Out proposed system will add parallel processing activities to get output in very short time.},
	author = {Karishma, Malla Vaidya and Shivani, J. L. Divya and Juhi, Gandhi and Pradnya, Kale and Avhad, Kiran},
	year = {2015},
	keywords = {Algorithm, Algorithmic composition, Computation, Computational model, Parallel computing, Probabilistic automaton, Robot (device)},
	file = {Full Text PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\213\\Karishma et al. - 2015 - Cluster Based Melody Generation.pdf:application/pdf}
}

@article{herremans_functional_2017,
	title = {A {Functional} {Taxonomy} of {Music} {Generation} {Systems}},
	volume = {50},
	issn = {03600300},
	url = {http://dl.acm.org/citation.cfm?doid=3145473.3108242},
	doi = {10.1145/3108242},
	language = {en},
	number = {5},
	urldate = {2019-05-07},
	journal = {ACM Comput. Surv.},
	author = {Herremans, Dorien and Chuan, Ching-Hua and Chew, Elaine},
	month = sep,
	year = {2017},
	pages = {1--30},
	annote = {Talk a lot about approaches to integrate rules in the generation of music. But it is mostly for classical rules which is not the direction I would like to take. Present the work done on interaction algorithm : generation of music in real time with human, but the last work presented has been done in 2007},
	file = {Herremans et al. - 2017 - A Functional Taxonomy of Music Generation Systems.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\215\\Herremans et al. - 2017 - A Functional Taxonomy of Music Generation Systems.pdf:application/pdf}
}

@article{chuan_modeling_nodate,
	title = {Modeling {Temporal} {Tonal} {Relations} in {Polyphonic} {Music} through {Deep} {Networks} with a {Novel} {Image}-{Based} {Representation}},
	abstract = {We propose an end-to-end approach for modeling polyphonic music with a novel graphical representation, based on music theory, in a deep neural network. Despite the success of deep learning in various applications, it remains a challenge to incorporate existing domain knowledge in a network without affecting its training routines. In this paper we present a novel approach for predictive music modeling and music generation that incorporates domain knowledge in its representation. In this work, music is transformed into a 2D representation, inspired by tonnetz from music theory, which graphically encodes musical relationships between pitches. This representation is incorporated in a deep network structure consisting of multilayered convolutional neural networks (CNN, for learning an efÔ¨Åcient abstract encoding of the representation) and recurrent neural networks with long short-term memory cells (LSTM, for capturing temporal dependencies in music sequences). We empirically evaluate the nature and the effectiveness of the network by using a dataset of classical music from various composers. We investigate the effect of parameters including the number of convolution feature maps, pooling strategies, and three conÔ¨Ågurations of the network: LSTM without CNN, LSTM with CNN (pre-trained vs. not pre-trained). Visualizations of the feature maps and Ô¨Ålters in the CNN are explored, and a comparison is made between the proposed tonnetz-inspired representation and pianoroll, a commonly used representation of music in computational systems. Experimental results show that the tonnetz representation produces musical sequences that are more tonally stable and contain more repeated patterns than sequences generated by pianoroll-based models, a Ô¨Ånding that is directly useful for tackling current challenges in music and AI such as smart music generation.},
	language = {en},
	author = {Chuan, Ching-Hua and Herremans, Dorien},
	pages = {8},
	annote = {It says the tonnetz ‚Äì based representation has never been used before. They use 1 for a step and an encoder decoder made with CNN and an RNN (LSTM) },
	file = {Chuan et Herremans - Modeling Temporal Tonal Relations in Polyphonic Mu.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\219\\Chuan et Herremans - Modeling Temporal Tonal Relations in Polyphonic Mu.pdf:application/pdf}
}

@article{herremans_morpheus:_2017,
	title = {{MorpheuS}: generating structured music with constrained patterns and tension},
	issn = {1949-3045},
	shorttitle = {{MorpheuS}},
	url = {http://ieeexplore.ieee.org/document/8007229/},
	doi = {10.1109/TAFFC.2017.2737984},
	abstract = {Automatic music generation systems have gained in popularity and sophistication as advances in cloud computing have enabled large-scale complex computations such as deep models and optimization algorithms on personal devices. Yet, they still face an important challenge, that of long-term structure, which is key to conveying a sense of musical coherence. We present the MorpheuS music generation system designed to tackle this problem. MorpheuS‚Äô novel framework has the ability to generate polyphonic pieces with a given tension proÔ¨Åle and long- and short-term repeated pattern structures. A mathematical model for tonal tension quantiÔ¨Åes the tension proÔ¨Åle and state-of-the-art pattern detection algorithms extract repeated patterns in a template piece. An efÔ¨Åcient optimization metaheuristic, variable neighborhood search, generates music by assigning pitches that best Ô¨Åt the prescribed tension proÔ¨Åle to the template rhythm while hard constraining long-term structure through the detected patterns. This ability to generate affective music with speciÔ¨Åc tension proÔ¨Åle and long-term structure is particularly useful in a game or Ô¨Ålm music context. Music generated by the MorpheuS system has been performed live in concerts.},
	language = {en},
	urldate = {2019-05-13},
	journal = {IEEE Trans. Affective Comput.},
	author = {Herremans, Dorien and Chew, Elaine},
	year = {2017},
	pages = {1--1},
	annote = {Goal : Be able to have a long structure and manage to control the tension of the music through the composition (for movies or video games) Use some expression to calculate the tension and compute the distance between the desired tension and the one generated. Use VNS algorithm (outperform GA) (VARIABLE NEIGHBORHOOD SEARCH) The tension create some ‚Äúsuprising outsanding‚Äù moves},
	file = {Herremans et Chew - 2017 - MorpheuS generating structured music with constra.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\223\\Herremans et Chew - 2017 - MorpheuS generating structured music with constra.pdf:application/pdf}
}

@article{agres_harmonic_2017,
	title = {Harmonic {Structure} {Predicts} the {Enjoyment} of {Uplifting} {Trance} {Music}},
	volume = {7},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2016.01999/full},
	doi = {10.3389/fpsyg.2016.01999},
	abstract = {An empirical investigation of how local harmonic structures (e.g., chord progressions) contribute to the experience and enjoyment of uplifting trance (UT) music is presented. The connection between rhythmic and percussive elements and resulting trance-like states has been highlighted by musicologists, but no research, to our knowledge, has explored whether repeated harmonic elements influence affective responses in listeners of trance music. Two alternative hypotheses are discussed, the first highlighting the direct relationship between repetition/complexity and enjoyment, and the second based on the theoretical inverted-U relationship described by the Wundt curve. We investigate the connection between harmonic structure and subjective enjoyment through interdisciplinary behavioral and computational methods: First we discuss an experiment in which listeners provided enjoyment ratings for computer-generated UT anthems with varying levels of harmonic repetition and complexity. The anthems were generated using a statistical model trained on a corpus of 100 uplifting trance anthems created for this purpose, and harmonic structure was constrained by imposing particular repetition structures (semiotic patterns defining the order of chords in the sequence) on a professional UT music production template. Second, the relationship between harmonic structure and enjoyment is further explored using two computational approaches, one based on average Information Content, and another that measures average tonal tension between chords. The results of the listening experiment indicate that harmonic repetition does in fact contribute to the enjoyment of uplifting trance music. More compelling evidence was found for the second hypothesis discussed above, however some maximally repetitive structures were also preferred. Both computational models provide evidence for a Wundt-type relationship between complexity and enjoyment. By systematically manipulating the structure of chord progressions, we have discovered specific harmonic contexts in which repetitive or complex structure contribute to the enjoyment of uplifting trance music.},
	language = {English},
	urldate = {2019-05-15},
	journal = {Front. Psychol.},
	author = {Agres, Kat and Herremans, Dorien and Bigo, Louis and Conklin, Darrell},
	year = {2017},
	keywords = {Complexity, Computational Creativity, enjoyment, music cognition, repetition, tension, Uplifting Trance music, Wundt curve},
	annote = {They studied trance music and the enjoyment due to the chords (with different generated chords)
They talk about Wundt curve (= inverse U curve). Basically, it just says that the best is something between too simple and too complex.
¬†},
	file = {Full Text PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\228\\Agres et al. - 2017 - Harmonic Structure Predicts the Enjoyment of Uplif.pdf:application/pdf}
}

@article{herremans_modeling_2017,
	title = {Modeling {Musical} {Context} {Using} {Word2vec}},
	abstract = {We present a semantic vector space model for capturing complex polyphonic musical context. A word2vec model based on a skip-gram representation with negative sampling was used to model slices of music from a dataset of Beethoven‚Äôs piano sonatas. A visualization of the reduced vector space using t-distributed stochastic neighbor embedding shows that the resulting embedded vector space captures tonal relationships, even without any explicit information about the musical contents of the slices. Secondly, an excerpt of the Moonlight Sonata from Beethoven was altered by replacing slices based on context similarity. The resulting music shows that the selected slice based on similar word2vec context also has a relatively short tonal distance from the original slice.},
	language = {en},
	author = {Herremans, D},
	year = {2017},
	pages = {8},
	annote = {Use word2Vec representation for notes (chords)
Each step time is considered as a word and they tried to replace some word with another one (the nearest one to see if the representation is coherent) but they seem to say, it is okay but not perfect.},
	file = {Herremans - 2017 - Modeling Musical Context Using Word2vec.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\229\\Herremans - 2017 - Modeling Musical Context Using Word2vec.pdf:application/pdf}
}

@inproceedings{herremans_multi-modal_2017,
	address = {San Diego, CA, USA},
	title = {A {Multi}-modal {Platform} for {Semantic} {Music} {Analysis}: {Visualizing} {Audio}-and {Score}-{Based} {Tension}},
	isbn = {978-1-5090-4284-5},
	shorttitle = {A {Multi}-modal {Platform} for {Semantic} {Music} {Analysis}},
	url = {http://ieeexplore.ieee.org/document/7889573/},
	doi = {10.1109/ICSC.2017.49},
	abstract = {Musicologists, music cognition scientists and others have long studied music in all of its facets. During the last few decades, research in both score and audio technology has opened the doors for automated, or (in many cases) semi-automated analysis. There remains a big gap, however, between the Ô¨Åeld of audio (performance) and score-based systems. In this research, we propose a web-based Interactive system for Multi-modal Music Analysis (IMMA), that provides musicologists with an intuitive interface for a joint analysis of performance and score. As an initial use-case, we implemented a tension analysis module in the system. Tension is a semantic characteristic of music that directly shapes the music experience and thus forms a crucial topic for researchers in musicology and music cognition. The module includes methods for calculating tonal tension (from the score) and timbral tension (from the performance). An audio-toscore alignment algorithm based on dynamic time warping was implemented to automate the synchronization between the audio and score analysis. The resulting system was tested on three performances (violin, Ô¨Çute, and guitar) of Paganini‚Äôs Caprice No. 24 and four piano performances of Beethoven‚Äôs Moonlight Sonata. We statistically analyzed the results of tonal and timbral tension and found correlations between them. A clustering algorithm was implemented to Ô¨Ånd segments of music (both within and between performances) with similar shape in their tension curve. These similar segments are visualized in IMMA. By displaying selected audio and score characteristics together with musical score following in sync with the performance playback, IMMA offers a user-friendly intuitive interface to bridge the gap between audio and score analysis.},
	language = {en},
	urldate = {2019-05-15},
	booktitle = {2017 {IEEE} 11th {International} {Conference} on {Semantic} {Computing} ({ICSC})},
	publisher = {IEEE},
	author = {Herremans, Dorien and Chuan, Ching-Hua},
	year = {2017},
	pages = {419--426},
	annote = {Propose a software to compute the tension of a music through the time. Not AI, it is just formula applied on the music. },
	file = {Herremans et Chuan - 2017 - A Multi-modal Platform for Semantic Music Analysis.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\231\\Herremans et Chuan - 2017 - A Multi-modal Platform for Semantic Music Analysis.pdf:application/pdf}
}

@article{herremans_modeling_2017-1,
	title = {Modeling {Musical} {Context} {Using} {Word2vec}},
	abstract = {We present a semantic vector space model for capturing complex polyphonic musical context. A word2vec model based on a skip-gram representation with negative sampling was used to model slices of music from a dataset of Beethoven‚Äôs piano sonatas. A visualization of the reduced vector space using t-distributed stochastic neighbor embedding shows that the resulting embedded vector space captures tonal relationships, even without any explicit information about the musical contents of the slices. Secondly, an excerpt of the Moonlight Sonata from Beethoven was altered by replacing slices based on context similarity. The resulting music shows that the selected slice based on similar word2vec context also has a relatively short tonal distance from the original slice.},
	language = {en},
	author = {Herremans, D},
	year = {2017},
	pages = {8},
	file = {Herremans - 2017 - Modeling Musical Context Using Word2vec.pdf:C\:\\Users\\Vval\\Zotero\\storage\\3C256Q86\\Herremans - 2017 - Modeling Musical Context Using Word2vec.pdf:application/pdf}
}

@article{agres_harmonic_2017-1,
	title = {Harmonic {Structure} {Predicts} the {Enjoyment} of {Uplifting} {Trance} {Music}},
	volume = {7},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2016.01999/full},
	doi = {10.3389/fpsyg.2016.01999},
	abstract = {An empirical investigation of how local harmonic structures (e.g., chord progressions) contribute to the experience and enjoyment of uplifting trance (UT) music is presented. The connection between rhythmic and percussive elements and resulting trance-like states has been highlighted by musicologists, but no research, to our knowledge, has explored whether repeated harmonic elements influence affective responses in listeners of trance music. Two alternative hypotheses are discussed, the first highlighting the direct relationship between repetition/complexity and enjoyment, and the second based on the theoretical inverted-U relationship described by the Wundt curve. We investigate the connection between harmonic structure and subjective enjoyment through interdisciplinary behavioral and computational methods: First we discuss an experiment in which listeners provided enjoyment ratings for computer-generated UT anthems with varying levels of harmonic repetition and complexity. The anthems were generated using a statistical model trained on a corpus of 100 uplifting trance anthems created for this purpose, and harmonic structure was constrained by imposing particular repetition structures (semiotic patterns defining the order of chords in the sequence) on a professional UT music production template. Second, the relationship between harmonic structure and enjoyment is further explored using two computational approaches, one based on average Information Content, and another that measures average tonal tension between chords. The results of the listening experiment indicate that harmonic repetition does in fact contribute to the enjoyment of uplifting trance music. More compelling evidence was found for the second hypothesis discussed above, however some maximally repetitive structures were also preferred. Both computational models provide evidence for a Wundt-type relationship between complexity and enjoyment. By systematically manipulating the structure of chord progressions, we have discovered specific harmonic contexts in which repetitive or complex structure contribute to the enjoyment of uplifting trance music.},
	language = {English},
	urldate = {2019-06-12},
	journal = {Front. Psychol.},
	author = {Agres, Kat and Herremans, Dorien and Bigo, Louis and Conklin, Darrell},
	year = {2017},
	keywords = {Complexity, Computational Creativity, enjoyment, music cognition, repetition, tension, Uplifting Trance music, Wundt curve},
	file = {Full Text PDF:C\:\\Users\\Vval\\Zotero\\storage\\LZ5RR6HC\\Agres et al. - 2017 - Harmonic Structure Predicts the Enjoyment of Uplif.pdf:application/pdf}
}

@inproceedings{balliauw_tabu_2015,
	title = {A {Tabu} {Search} {Algorithm} to {Generate} {Piano} {Fingerings} for {Polyphonic} {Sheet} {Music}},
	abstract = {A piano fingering is an indication of which finger is to be used to play each note. Good piano fingerings enable pianists to study, remember and play pieces fluently. In this paper, we propose an algorithm to find a good piano fingering automatically. The tabu search algorithm is a metaheuristic that can find a good solution in a short amount of execution time. The algorithm implements an objective function that takes into account the characteristics of the pianist‚Äôs hand in complex polyphonic music.},
	author = {Balliauw, Matteo and Herremans, Dorien and Cuervo, Daniel Palhazi and S√∂rensen, Kenneth},
	year = {2015},
	keywords = {Metaheuristic, Optimization problem, Run time (program lifecycle phase), Search algorithm, Tabu search},
	file = {Full Text PDF:C\:\\Users\\Vval\\Zotero\\storage\\CCFF34MG\\Balliauw et al. - 2015 - A Tabu Search Algorithm to Generate Piano Fingerin.pdf:application/pdf}
}

@article{herremans_composing_2013,
	title = {Composing fifth species counterpoint music with a variable neighborhood search algorithm},
	volume = {40},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417413003692},
	doi = {10.1016/j.eswa.2013.05.071},
	abstract = {In this paper, a variable neighborhood search (VNS) algorithm is developed and analyzed that can generate Ô¨Åfth species counterpoint fragments. The existing species counterpoint rules are quantiÔ¨Åed and form the basis of the objective function used by the algorithm. The VNS developed in this research is a local search metaheuristic that starts from a randomly generated fragment and gradually improves this solution by changing one or two notes at a time. An in-depth statistical analysis reveals the signiÔ¨Åcance as well as the optimal settings of the parameters of the VNS. The algorithm has been implemented in a user-friendly software environment called Optimuse. Optimuse allows a user to input basic characteristics such as length, key and mode. Based on this input, a Ô¨Åfth species counterpoint fragment is generated by the system that can be edited and played back immediately.},
	language = {en},
	number = {16},
	urldate = {2019-06-13},
	journal = {Expert Systems with Applications},
	author = {Herremans, D. and S√∂rensen, K.},
	month = nov,
	year = {2013},
	pages = {6427--6437},
	annote = {Use Genetic Algorithm to generate CounterPoint},
	file = {Herremans et S√∂rensen - 2013 - Composing fifth species counterpoint music with a .pdf:C\:\\Users\\Vval\\Zotero\\storage\\ZTCT9WFG\\Herremans et S√∂rensen - 2013 - Composing fifth species counterpoint music with a .pdf:application/pdf}
}

@article{herremans_generating_2015,
	title = {Generating structured music for bagana using quality metrics based on {Markov} models},
	volume = {42},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417415003796},
	doi = {10.1016/j.eswa.2015.05.043},
	abstract = {In this research, a system is built that generates bagana music, a traditional lyre from Ethiopia, based on a Ô¨Årst order Markov model. Due to the size of many datasets it is often only possible to get rich and reliable statistics for low order models, yet these do not handle structure very well and their output is often very repetitive. A Ô¨Årst contribution of this paper is to propose a method that allows the enforcement of structure and repetition within music, thus handling long term coherence with a Ô¨Årst order model. The second goal of this research is to explain and propose different ways in which low order Markov models can be used to build quality assessment metrics for an optimization algorithm. These are then implemented in a variable neighbourhood search algorithm that generates bagana music. The results are examined and thorougly evaluated.},
	language = {en},
	number = {21},
	urldate = {2019-06-29},
	journal = {Expert Systems with Applications},
	author = {Herremans, D. and Weisser, S. and S√∂rensen, K. and Conklin, D.},
	month = nov,
	year = {2015},
	pages = {7424--7435},
	file = {Herremans et al. - 2015 - Generating structured music for bagana using quali.pdf:C\:\\Users\\Vval\\Zotero\\storage\\Z95V32XF\\Herremans et al. - 2015 - Generating structured music for bagana using quali.pdf:application/pdf}
}

@inproceedings{madhok_sentimozart:_2018,
	title = {{SentiMozart}: {Music} {Generation} based on {Emotions}},
	shorttitle = {{SentiMozart}},
	doi = {10.5220/0006597705010506},
	abstract = {Facial expressions are one of the best and the most intuitive way to determine a persons emotions. They most naturally express how a person is feeling currently. The aim of the proposed framework is to generate music corresponding to the emotion of the person predicted by our model. The proposed framework is divided into two models, the Image Classification Model and the Music Generation Model. The music would be generated by the latter model which is essentially a Doubly Stacked LSTM architecture. This is to be done after classification and identification of the facial expression into one of the seven major sentiment categories: Angry, Disgust, Fear, Happy, Sad, Surprise and Neutral, which would be done by using Convolutional Neural Networks (CNN). Finally, we evaluate the performance of our proposed framework using the emotional Mean Opinion Score (MOS) which is a popular evaluation metric for audio-visual data.},
	booktitle = {{ICAART}},
	author = {Madhok, Rishi and Goel, Shivali and Garg, Shweta},
	year = {2018},
	keywords = {British Informatics Olympiad, Computer science, Computer scientist, Convolutional neural network, Instagram, Long short-term memory, Regular expression, Snapchat, Synergy, User analysis},
	annote = {Music generation based on the Emotion on an image: (for snapchat for example)
CNN -{\textgreater} Emotion (Sad, happy, neutral)
Then use 3 datasets (sad, happy, neutral) labelled by 15 people to train 3 NN},
	file = {Full Text PDF:C\:\\Users\\Vval\\Zotero\\storage\\HXLQVBM7\\Madhok et al. - 2018 - SentiMozart Music Generation based on Emotions.pdf:application/pdf}
}

@misc{noauthor_[1905.13570]_nodate,
	title = {[1905.13570] {Factorized} {Inference} in {Deep} {Markov} {Models} for {Incomplete} {Multimodal} {Time} {Series}},
	url = {https://arxiv.org/abs/1905.13570},
	urldate = {2019-07-01}
}

@article{tan_factorized_2019,
	title = {Factorized {Inference} in {Deep} {Markov} {Models} for {Incomplete} {Multimodal} {Time} {Series}},
	url = {http://arxiv.org/abs/1905.13570},
	abstract = {Integrating deep learning with latent state space models has the potential to yield temporal models that are powerful, yet tractable and interpretable. Unfortunately, current models are not designed to handle missing data or multiple data modalities, which are both prevalent in real-world data. In this work, we introduce a factorized inference method for Multimodal Deep Markov Models (MDMMs), allowing us to filter and smooth in the presence of missing data, while also performing uncertainty-aware multimodal fusion. We derive this method by factorizing the posterior p(z{\textbar}x) for non-linear state space models, and develop a variational backward-forward algorithm for inference. Because our method handles incompleteness over both time and modalities, it is capable of interpolation, extrapolation, conditional generation, and label prediction in multimodal time series. We demonstrate these capabilities on both synthetic and real-world multimodal data under high levels of data deletion. Our method performs well even with more than 50\% missing data, and outperforms existing deep approaches to inference in latent time series.},
	urldate = {2019-07-01},
	journal = {arXiv:1905.13570 [cs, stat]},
	author = {Tan, Zhi-Xuan and Soh, Harold and Ong, Desmond C.},
	month = may,
	year = {2019},
	note = {arXiv: 1905.13570},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence},
	annote = {Comment: 8 pages (excl. references), 3 figures},
	file = {arXiv\:1905.13570 PDF:C\:\\Users\\Vval\\Zotero\\storage\\A3JW4SH8\\Tan et al. - 2019 - Factorized Inference in Deep Markov Models for Inc.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\TC334678\\1905.html:text/html}
}

@incollection{dieleman_challenge_2018,
	title = {The challenge of realistic music generation: modelling raw audio at scale},
	shorttitle = {The challenge of realistic music generation},
	url = {http://papers.nips.cc/paper/8023-the-challenge-of-realistic-music-generation-modelling-raw-audio-at-scale.pdf},
	urldate = {2019-07-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Dieleman, Sander and van den Oord, Aaron and Simonyan, Karen},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {7989--7999},
	file = {NIPS Full Text PDF:C\:\\Users\\Vval\\Zotero\\storage\\FGIV8GRA\\Dieleman et al. - 2018 - The challenge of realistic music generation model.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\33LMLL5S\\8023-the-challenge-of-realistic-music-generation-modelling-raw-audio-at-scale.html:text/html}
}

@article{yu_multi-scale_2015,
	title = {Multi-{Scale} {Context} {Aggregation} by {Dilated} {Convolutions}},
	url = {http://arxiv.org/abs/1511.07122},
	abstract = {State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.},
	urldate = {2019-07-18},
	journal = {arXiv:1511.07122 [cs]},
	author = {Yu, Fisher and Koltun, Vladlen},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.07122},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Published as a conference paper at ICLR 2016},
	file = {arXiv\:1511.07122 PDF:C\:\\Users\\Vval\\Zotero\\storage\\E5UKRC3A\\Yu et Koltun - 2015 - Multi-Scale Context Aggregation by Dilated Convolu.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\QPA8MDU8\\1511.html:text/html}
}

@incollection{wu_multimodal_2018,
	title = {Multimodal {Generative} {Models} for {Scalable} {Weakly}-{Supervised} {Learning}},
	url = {http://papers.nips.cc/paper/7801-multimodal-generative-models-for-scalable-weakly-supervised-learning.pdf},
	urldate = {2019-07-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Wu, Mike and Goodman, Noah},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {5575--5585},
	file = {NIPS Full Text PDF:C\:\\Users\\Vval\\Zotero\\storage\\IMWYZ7TC\\Wu et Goodman - 2018 - Multimodal Generative Models for Scalable Weakly-S.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\Z2A5HJLL\\7801-multimodal-generative-models-for-scalable-weakly-supervised-learning.html:text/html}
}

@incollection{wu_multimodal_2018-1,
	title = {Multimodal {Generative} {Models} for {Scalable} {Weakly}-{Supervised} {Learning}},
	url = {http://papers.nips.cc/paper/7801-multimodal-generative-models-for-scalable-weakly-supervised-learning.pdf},
	urldate = {2019-07-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Wu, Mike and Goodman, Noah},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {5575--5585},
	file = {NIPS Full Text PDF:C\:\\Users\\Vval\\Zotero\\storage\\TAPFGTR6\\Wu et Goodman - 2018 - Multimodal Generative Models for Scalable Weakly-S.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\UKNK9876\\7801-multimodal-generative-models-for-scalable-weakly-supervised-learning.html:text/html}
}

@article{krishnan_structured_2016,
	title = {Structured {Inference} {Networks} for {Nonlinear} {State} {Space} {Models}},
	url = {http://arxiv.org/abs/1609.09869},
	abstract = {Gaussian state space models have been used for decades as generative models of sequential data. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption. We introduce a unified algorithm to efficiently learn a broad class of linear and non-linear state space models, including variants where the emission and transition distributions are modeled by deep neural networks. Our learning algorithm simultaneously learns a compiled inference network and the generative model, leveraging a structured variational approximation parameterized by recurrent neural networks to mimic the posterior distribution. We apply the learning algorithm to both synthetic and real-world datasets, demonstrating its scalability and versatility. We find that using the structured approximation to the posterior results in models with significantly higher held-out likelihood.},
	urldate = {2019-08-01},
	journal = {arXiv:1609.09869 [cs, stat]},
	author = {Krishnan, Rahul G. and Shalit, Uri and Sontag, David},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.09869},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: To appear in the Thirty-First AAAI Conference on Artificial Intelligence, February 2017, 13 pages, 11 figures with supplement, changed to AAAI formatting style, added references},
	file = {arXiv\:1609.09869 PDF:C\:\\Users\\Vval\\Zotero\\storage\\UMISP9UH\\Krishnan et al. - 2016 - Structured Inference Networks for Nonlinear State .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\9KN7DMJU\\1609.html:text/html}
}

@article{schroecker_generative_2019,
	title = {Generative predecessor models for sample-efficient imitation learning},
	url = {http://arxiv.org/abs/1904.01139},
	abstract = {We propose Generative Predecessor Models for Imitation Learning (GPRIL), a novel imitation learning algorithm that matches the state-action distribution to the distribution observed in expert demonstrations, using generative models to reason probabilistically about alternative histories of demonstrated states. We show that this approach allows an agent to learn robust policies using only a small number of expert demonstrations and self-supervised interactions with the environment. We derive this approach from first principles and compare it empirically to a state-of-the-art imitation learning method, showing that it outperforms or matches its performance on two simulated robot manipulation tasks and demonstrate significantly higher sample efficiency by applying the algorithm on a real robot.},
	urldate = {2019-12-16},
	journal = {arXiv:1904.01139 [cs, stat]},
	author = {Schroecker, Yannick and Vecerik, Mel and Scholz, Jonathan},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.01139},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\6MVHDRIW\\Schroecker et al. - 2019 - Generative predecessor models for sample-efficient.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\NNPDG9HH\\1904.html:text/html}
}

@article{papamakarios_normalizing_2019,
	title = {Normalizing {Flows} for {Probabilistic} {Modeling} and {Inference}},
	url = {http://arxiv.org/abs/1912.02762},
	abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
	urldate = {2019-12-16},
	journal = {arXiv:1912.02762 [cs, stat]},
	author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.02762},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Review article. 60 pages, 4 figures},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\H5Z4NPL7\\Papamakarios et al. - 2019 - Normalizing Flows for Probabilistic Modeling and I.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\2XRAICLC\\1912.html:text/html}
}

@article{lipton_troubling_2018,
	title = {Troubling {Trends} in {Machine} {Learning} {Scholarship}},
	url = {http://arxiv.org/abs/1807.03341},
	abstract = {Collectively, machine learning (ML) researchers are engaged in the creation and dissemination of knowledge about data-driven algorithms. In a given paper, researchers might aspire to any subset of the following goals, among others: to theoretically characterize what is learnable, to obtain understanding through empirically rigorous experiments, or to build a working system that has high predictive accuracy. While determining which knowledge warrants inquiry may be subjective, once the topic is fixed, papers are most valuable to the community when they act in service of the reader, creating foundational knowledge and communicating as clearly as possible. Recent progress in machine learning comes despite frequent departures from these ideals. In this paper, we focus on the following four patterns that appear to us to be trending in ML scholarship: (i) failure to distinguish between explanation and speculation; (ii) failure to identify the sources of empirical gains, e.g., emphasizing unnecessary modifications to neural architectures when gains actually stem from hyper-parameter tuning; (iii) mathiness: the use of mathematics that obfuscates or impresses rather than clarifies, e.g., by confusing technical and non-technical concepts; and (iv) misuse of language, e.g., by choosing terms of art with colloquial connotations or by overloading established technical terms. While the causes behind these patterns are uncertain, possibilities include the rapid expansion of the community, the consequent thinness of the reviewer pool, and the often-misaligned incentives between scholarship and short-term measures of success (e.g., bibliometrics, attention, and entrepreneurial opportunity). While each pattern offers a corresponding remedy (don't do it), we also discuss some speculative suggestions for how the community might combat these trends.},
	urldate = {2019-12-16},
	journal = {arXiv:1807.03341 [cs, stat]},
	author = {Lipton, Zachary C. and Steinhardt, Jacob},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.03341},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Presented at ICML 2018: The Debates},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\38RXNU5P\\Lipton et Steinhardt - 2018 - Troubling Trends in Machine Learning Scholarship.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\38K7733Z\\1807.html:text/html}
}

@article{merity_single_2019,
	title = {Single {Headed} {Attention} {RNN}: {Stop} {Thinking} {With} {Your} {Head}},
	shorttitle = {Single {Headed} {Attention} {RNN}},
	url = {http://arxiv.org/abs/1911.11423},
	abstract = {The leading approaches in language modeling are all obsessed with TV shows of my youth - namely Transformers and Sesame Street. Transformers this, Transformers that, and over here a bonfire worth of GPU-TPU-neuromorphic wafer scale silicon. We opt for the lazy path of old and proven techniques with a fancy crypto inspired acronym: the Single Headed Attention RNN (SHA-RNN). The author's lone goal is to show that the entire field might have evolved a different direction if we had instead been obsessed with a slightly different acronym and slightly different result. We take a previously strong language model based only on boring LSTMs and get it to within a stone's throw of a stone's throw of state-of-the-art byte level language model results on enwik8. This work has undergone no intensive hyperparameter optimization and lived entirely on a commodity desktop machine that made the author's small studio apartment far too warm in the midst of a San Franciscan summer. The final results are achievable in plus or minus 24 hours on a single GPU as the author is impatient. The attention mechanism is also readily extended to large contexts with minimal computation. Take that Sesame Street.},
	urldate = {2019-12-16},
	journal = {arXiv:1911.11423 [cs]},
	author = {Merity, Stephen},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.11423},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	annote = {Comment: Addition of citations and contextual results (no attention head, single attention head, attention per layer), removal of wordpiece WikiText-103 numbers due to normalization issues, fix of SHA attention figure Q arrow, other minor fixes},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\4B6W2UHY\\Merity - 2019 - Single Headed Attention RNN Stop Thinking With Yo.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\JHTALPK6\\1911.html:text/html}
}

@article{lorraine_optimizing_2019,
	title = {Optimizing {Millions} of {Hyperparameters} by {Implicit} {Differentiation}},
	url = {http://arxiv.org/abs/1911.02590},
	abstract = {We propose an algorithm for inexpensive gradient-based hyperparameter optimization that combines the implicit function theorem (IFT) with efficient inverse Hessian approximations. We present results about the relationship between the IFT and differentiating through optimization, motivating our algorithm. We use the proposed approach to train modern network architectures with millions of weights and millions of hyper-parameters. For example, we learn a data-augmentation network - where every weight is a hyperparameter tuned for validation performance - outputting augmented training examples. Jointly tuning weights and hyperparameters with our approach is only a few times more costly in memory and compute than standard training.},
	urldate = {2019-12-16},
	journal = {arXiv:1911.02590 [cs, stat]},
	author = {Lorraine, Jonathan and Vicol, Paul and Duvenaud, David},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.02590},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Submitted to AISTATS 2020},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\RFLJUW2V\\Lorraine et al. - 2019 - Optimizing Millions of Hyperparameters by Implicit.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\98YZGY2F\\1911.html:text/html}
}

@inproceedings{deng_structure_2016,
	address = {Las Vegas, NV, USA},
	title = {Structure {Inference} {Machines}: {Recurrent} {Neural} {Networks} for {Analyzing} {Relations} in {Group} {Activity} {Recognition}},
	isbn = {978-1-4673-8851-1},
	shorttitle = {Structure {Inference} {Machines}},
	url = {http://ieeexplore.ieee.org/document/7780885/},
	doi = {10.1109/CVPR.2016.516},
	abstract = {Rich semantic relations are important in a variety of visual recognition problems. As a concrete example, group activity recognition involves the interactions and relative spatial relations of a set of people in a scene. State of the art recognition methods center on deep learning approaches for training highly effective, complex classiÔ¨Åers for interpreting images. However, bridging the relatively low-level concepts output by these methods to interpret higher-level compositional scenes remains a challenge. Graphical models are a standard tool for this task. In this paper, we propose a method to integrate graphical models and deep neural networks into a joint framework. Instead of using a traditional inference method, we use a sequential inference modeled by a recurrent neural network. Beyond this, the appropriate structure for inference can be learned by imposing gates on edges between nodes. Empirical results on group activity recognition demonstrate the potential of this model to handle highly structured learning tasks.},
	language = {en},
	urldate = {2020-01-04},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Deng, Zhiwei and Vahdat, Arash and Hu, Hexiang and Mori, Greg},
	month = jun,
	year = {2016},
	pages = {4772--4781},
	file = {Deng et al. - 2016 - Structure Inference Machines Recurrent Neural Net.pdf:C\:\\Users\\Vval\\Zotero\\storage\\QFMRZH3X\\Deng et al. - 2016 - Structure Inference Machines Recurrent Neural Net.pdf:application/pdf}
}

@article{frazier_tutorial_2018,
	title = {A {Tutorial} on {Bayesian} {Optimization}},
	url = {http://arxiv.org/abs/1807.02811},
	abstract = {Bayesian optimization is an approach to optimizing objective functions that take a long time (minutes or hours) to evaluate. It is best-suited for optimization over continuous domains of less than 20 dimensions, and tolerates stochastic noise in function evaluations. It builds a surrogate for the objective and quantifies the uncertainty in that surrogate using a Bayesian machine learning technique, Gaussian process regression, and then uses an acquisition function defined from this surrogate to decide where to sample. In this tutorial, we describe how Bayesian optimization works, including Gaussian process regression and three common acquisition functions: expected improvement, entropy search, and knowledge gradient. We then discuss more advanced techniques, including running multiple function evaluations in parallel, multi-fidelity and multi-information source optimization, expensive-to-evaluate constraints, random environmental conditions, multi-task Bayesian optimization, and the inclusion of derivative information. We conclude with a discussion of Bayesian optimization software and future research directions in the field. Within our tutorial material we provide a generalization of expected improvement to noisy evaluations, beyond the noise-free setting where it is more commonly applied. This generalization is justified by a formal decision-theoretic argument, standing in contrast to previous ad hoc modifications.},
	urldate = {2020-01-04},
	journal = {arXiv:1807.02811 [cs, math, stat]},
	author = {Frazier, Peter I.},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.02811},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\YN25QUGG\\Frazier - 2018 - A Tutorial on Bayesian Optimization.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\X2Y7AJUD\\1807.html:text/html}
}

@article{schindler_deep_2020,
	title = {Deep {Learning} for {MIR} {Tutorial}},
	url = {http://arxiv.org/abs/2001.05266},
	abstract = {Deep Learning has become state of the art in visual computing and continuously emerges into the Music Information Retrieval (MIR) and audio retrieval domain. In order to bring attention to this topic we propose an introductory tutorial on deep learning for MIR. Besides a general introduction to neural networks, the proposed tutorial covers a wide range of MIR relevant deep learning approaches. {\textbackslash}textbf\{Convolutional Neural Networks\} are currently a de-facto standard for deep learning based audio retrieval. {\textbackslash}textbf\{Recurrent Neural Networks\} have proven to be effective in onset detection tasks such as beat or audio-event detection. {\textbackslash}textbf\{Siamese Networks\} have been shown effective in learning audio representations and distance functions specific for music similarity retrieval. We will incorporate both academic and industrial points of view into the tutorial. Accompanying the tutorial, we will create a Github repository for the content presented at the tutorial as well as references to state of the art work and literature for further reading. This repository will remain public after the conference.},
	urldate = {2020-01-27},
	journal = {arXiv:2001.05266 [cs, eess]},
	author = {Schindler, Alexander and Lidy, Thomas and B√∂ck, Sebastian},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.05266},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Information Retrieval},
	annote = {Comment: This is a description of a tutorial held at the 19th International Society for Music Information Retrieval Conference, ISMIR 2018, Paris, France, September 23-27, 2018. 2018},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\52GPB5V5\\Schindler et al. - 2020 - Deep Learning for MIR Tutorial.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\UGUXGJM8\\2001.html:text/html}
}

@inproceedings{fuentes_music_2019,
	title = {A {Music} {Structure} {Informed} {Downbeat} {Tracking} {System} {Using} {Skip}-chain {Conditional} {Random} {Fields} and {Deep} {Learning}},
	doi = {10.1109/ICASSP.2019.8682870},
	abstract = {In recent years the task of downbeat tracking has received increasing attention and the state of the art has been improved with the introduction of deep learning methods. Among proposed solutions, existing systems exploit short-term musical rules as part of their language modelling. In this work we show in an oracle scenario how including longer-term musical rules, in particular music structure, can enhance downbeat estimation. We introduce a skip-chain conditional random field language model for downbeat tracking designed to include section information in an unified and flexible framework. We combine this model with a state-of-the-art convolutional-recurrent network and we contrast the system's performance to the commonly used Bar Pointer model. Our experiments on the popular Beatles dataset show that incorporating structure information in the language model leads to more consistent and more robust downbeat estimations.},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Fuentes, Magdalena and McFee, Brian and Crayencour, H√©l√®ne C. and Essid, Slim and Bello, Juan Pablo},
	month = may,
	year = {2019},
	note = {ISSN: 1520-6149},
	keywords = {Hidden Markov models, learning (artificial intelligence), Bars, convolutional-recurrent network, Convolutional-Recurrent Neural Networks, Deep learning, Deep Learning, deep learning methods, downbeat estimation, Downbeat Tracking, Estimation, estimation theory, language modelling, longer-term musical rules, music, Music, Music Structure, music structure informed downbeat tracking system, natural language processing, oracle scenario, random processes, recurrent neural nets, section information, short-term musical rules, skip-chain conditional random field language model, skip-chain conditional random fields, Skip-Chain Conditional Random Fields, structure information, Task analysis, Training},
	pages = {481--485},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Vval\\Zotero\\storage\\XZTVIEKN\\8682870.html:text/html}
}

@inproceedings{fuentes_music_2019-1,
	address = {Brighton, United Kingdom},
	title = {A {Music} {Structure} {Informed} {Downbeat} {Tracking} {System} {Using} {Skip}-chain {Conditional} {Random} {Fields} and {Deep} {Learning}},
	isbn = {978-1-4799-8131-1},
	url = {https://ieeexplore.ieee.org/document/8682870/},
	doi = {10.1109/ICASSP.2019.8682870},
	urldate = {2020-01-27},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Fuentes, Magdalena and McFee, Brian and Crayencour, Helene C. and Essid, Slim and Bello, Juan Pablo},
	month = may,
	year = {2019},
	pages = {481--485}
}

@article{dai_music_2018,
	title = {Music {Style} {Transfer}: {A} {Position} {Paper}},
	shorttitle = {Music {Style} {Transfer}},
	url = {http://arxiv.org/abs/1803.06841},
	abstract = {Led by the success of neural style transfer on visual arts, there has been a rising trend very recently in the effort of music style transfer. However, "music style" is not yet a well-defined concept from a scientific point of view. The difficulty lies in the intrinsic multi-level and multi-modal character of music representation (which is very different from image representation). As a result, depending on their interpretation of "music style", current studies under the category of "music style transfer", are actually solving completely different problems that belong to a variety of sub-fields of Computer Music. Also, a vanilla end-to-end approach, which aims at dealing with all levels of music representation at once by directly adopting the method of image style transfer, leads to poor results. Thus, we vitally propose a more scientifically-viable definition of music style transfer by breaking it down into precise concepts of timbre style transfer, performance style transfer and composition style transfer, as well as to connect different aspects of music style transfer with existing well-established sub-fields of computer music studies. In addition, we discuss the current limitations of music style modeling and its future directions by drawing spirit from some deep generative models, especially the ones using unsupervised learning and disentanglement techniques.},
	urldate = {2020-01-31},
	journal = {arXiv:1803.06841 [cs, eess]},
	author = {Dai, Shuqi and Zhang, Zheng and Xia, Gus G.},
	month = jul,
	year = {2018},
	note = {arXiv: 1803.06841},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: In Proceeding of International Workshop on Musical Metacreation (MUME), 2018, Salamanca, Spain},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\ENNTZQ3S\\Dai et al. - 2018 - Music Style Transfer A Position Paper.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\FTVYFSHL\\1803.html:text/html}
}

@inproceedings{liang_automatic_2017,
	title = {Automatic {Stylistic} {Composition} of {Bach} {Chorales} with {Deep} {LSTM}},
	abstract = {This paper presents ‚ÄúBachBot‚Äù: an end-to-end automatic composition system for composing and completing music in the style of Bach‚Äôs chorales using a deep long short-term memory (LSTM) generative model. We propose a new sequential encoding scheme for polyphonic music and a model for both composition and harmonization which can be efficiently sampled without expensive Markov Chain Monte Carlo (MCMC). Analysis of the trained model provides evidence of neurons specializing without prior knowledge or explicit supervision to detect common music-theoretic concepts such as tonics, chords, and cadences. To assess BachBot‚Äôs success, we conducted one of the largest musical discrimination tests on 2336 participants. Among the results, the proportion of responses correctly differentiating BachBot from Bach was only 1\% better than random guessing.},
	booktitle = {{ISMIR}},
	author = {Liang, Feynman T. and Gotham, Mark and Johnson, Matthew and Shotton, Jamie},
	year = {2017},
	keywords = {Long short-term memory, End-to-end principle, Generative model, Line code, Markov chain Monte Carlo, Monte Carlo method, Theory}
}

@misc{noauthor_161201010_nodate,
	title = {[1612.01010] {DeepBach}: a {Steerable} {Model} for {Bach} {Chorales} {Generation}},
	url = {https://arxiv.org/abs/1612.01010},
	urldate = {2020-02-08},
	file = {[1612.01010] DeepBach\: a Steerable Model for Bach Chorales Generation:C\:\\Users\\Vval\\Zotero\\storage\\4DYI98C9\\1612.html:text/html}
}

@article{xie_embedding_2019,
	title = {Embedding {Symbolic} {Knowledge} into {Deep} {Networks}},
	url = {http://arxiv.org/abs/1909.01161},
	abstract = {In this work, we aim to leverage prior symbolic knowledge to improve the performance of deep models. We propose a graph embedding network that projects propositional formulae (and assignments) onto a manifold via an augmented Graph Convolutional Network (GCN). To generate semantically-faithful embeddings, we develop techniques to recognize node heterogeneity, and semantic regularization that incorporate structural constraints into the embedding. Experiments show that our approach improves the performance of models trained to perform entailment checking and visual relation prediction. Interestingly, we observe a connection between the tractability of the propositional theory representation and the ease of embedding. Future exploration of this connection may elucidate the relationship between knowledge compilation and vector representation learning.},
	urldate = {2020-02-08},
	journal = {arXiv:1909.01161 [cs]},
	author = {Xie, Yaqi and Xu, Ziwei and Kankanhalli, Mohan S. and Meel, Kuldeep S. and Soh, Harold},
	month = oct,
	year = {2019},
	note = {arXiv: 1909.01161},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Multimedia, Computer Science - Artificial Intelligence},
	annote = {Comment: *Equal contribution; Accepted at conference Neural Information Processing Systems (NeurIPS), 2019},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\VGWP98V7\\Xie et al. - 2019 - Embedding Symbolic Knowledge into Deep Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\TVEXP3LB\\1909.html:text/html}
}

@article{ying_hierarchical_2019,
	title = {Hierarchical {Graph} {Representation} {Learning} with {Differentiable} {Pooling}},
	url = {http://arxiv.org/abs/1806.08804},
	abstract = {Recently, graph neural networks (GNNs) have revolutionized the field of graph representation learning through effectively learned node embeddings, and achieved state-of-the-art results in tasks such as node classification and link prediction. However, current GNN methods are inherently flat and do not learn hierarchical representations of graphs---a limitation that is especially problematic for the task of graph classification, where the goal is to predict the label associated with an entire graph. Here we propose DiffPool, a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion. DiffPool learns a differentiable soft cluster assignment for nodes at each layer of a deep GNN, mapping nodes to a set of clusters, which then form the coarsened input for the next GNN layer. Our experimental results show that combining existing GNN methods with DiffPool yields an average improvement of 5-10\% accuracy on graph classification benchmarks, compared to all existing pooling approaches, achieving a new state-of-the-art on four out of five benchmark data sets.},
	urldate = {2020-02-12},
	journal = {arXiv:1806.08804 [cs, stat]},
	author = {Ying, Rex and You, Jiaxuan and Morris, Christopher and Ren, Xiang and Hamilton, William L. and Leskovec, Jure},
	month = feb,
	year = {2019},
	note = {arXiv: 1806.08804},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Social and Information Networks},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\YLP45K4C\\Ying et al. - 2019 - Hierarchical Graph Representation Learning with Di.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\B3QWUGMP\\1806.html:text/html}
}

@inproceedings{darwiche_new_2004,
	title = {New {Advances} in {Compiling} {CNF} into {Decomposable} {Negation} {Normal} {Form}},
	abstract = {We describe a new algorithm for compiling conjunctive normal form (CNF) into Deterministic Decomposable Negation Normal (d-DNNF), which is a tractable logical form that permits model counting in polynomial time. The new implementation is based on latest techniques from both the SAT and OBDD literatures, and appears to be orders of magnitude more efficient than previous algorithms for this purpose. We compare our compiler experimentally to state of the art model counters, OBDD compilers, and previous CNF2dDNNF compilers.},
	booktitle = {{ECAI}},
	author = {Darwiche, Adnan},
	year = {2004},
	keywords = {Algorithm, Backtracking, Benchmark (computing), Binary decision diagram, Cobham's thesis, Compiler, Conjunctive normal form, Experiment, Negation normal form, Polynomial, Preferential entailment, Time complexity, Turing completeness}
}

@article{huang_counterpoint_2017,
	title = {{COUNTERPOINT} {BY} {CONVOLUTION}},
	abstract = {Machine learning models of music typically break up the task of composition into a chronological process, composing a piece of music in a single pass from beginning to end. On the contrary, human composers write music in a nonlinear fashion, scribbling motifs here and there, often revisiting choices previously made. In order to better approximate this process, we train a convolutional neural network to complete partial musical scores, and explore the use of blocked Gibbs sampling as an analogue to rewriting. Neither the model nor the generative procedure are tied to a particular causal direction of composition.},
	language = {en},
	author = {Huang, Cheng-Zhi Anna and Cooijmans, Tim and Roberts, Adam and Courville, Aaron and Eck, Douglas},
	year = {2017},
	pages = {8},
	file = {Huang et al. - 2017 - COUNTERPOINT BY CONVOLUTION.pdf:C\:\\Users\\Vval\\Zotero\\storage\\Q5VIYNGU\\Huang et al. - 2017 - COUNTERPOINT BY CONVOLUTION.pdf:application/pdf}
}

@article{uria_deep_2014,
	title = {A {Deep} and {Tractable} {Density} {Estimator}},
	url = {http://arxiv.org/abs/1310.1757},
	abstract = {The Neural Autoregressive Distribution Estimator (NADE) and its real-valued version RNADE are competitive density models of multidimensional data across a variety of domains. These models use a fixed, arbitrary ordering of the data dimensions. One can easily condition on variables at the beginning of the ordering, and marginalize out variables at the end of the ordering, however other inference tasks require approximate inference. In this work we introduce an efficient procedure to simultaneously train a NADE model for each possible ordering of the variables, by sharing parameters across all these models. We can thus use the most convenient model for each inference task at hand, and ensembles of such models with different orderings are immediately available. Moreover, unlike the original NADE, our training procedure scales to deep models. Empirically, ensembles of Deep NADE models obtain state of the art density estimation performance.},
	urldate = {2020-02-12},
	journal = {arXiv:1310.1757 [cs, stat]},
	author = {Uria, Benigno and Murray, Iain and Larochelle, Hugo},
	month = jan,
	year = {2014},
	note = {arXiv: 1310.1757},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 9 pages, 4 tables, 1 algorithm, 5 figures. To appear ICML 2014, JMLR W\&CP volume 32},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\N4E2969X\\Uria et al. - 2014 - A Deep and Tractable Density Estimator.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\RSFYNKJM\\1310.html:text/html}
}

@article{uria_neural_2016,
	title = {Neural {Autoregressive} {Distribution} {Estimation}},
	url = {http://arxiv.org/abs/1605.02226},
	abstract = {We present Neural Autoregressive Distribution Estimation (NADE) models, which are neural network architectures applied to the problem of unsupervised distribution and density estimation. They leverage the probability product rule and a weight sharing scheme inspired from restricted Boltzmann machines, to yield an estimator that is both tractable and has good generalization performance. We discuss how they achieve competitive performance in modeling both binary and real-valued observations. We also present how deep NADE models can be trained to be agnostic to the ordering of input dimensions used by the autoregressive product rule decomposition. Finally, we also show how to exploit the topological structure of pixels in images using a deep convolutional architecture for NADE.},
	urldate = {2020-02-12},
	journal = {arXiv:1605.02226 [cs]},
	author = {Uria, Benigno and C√¥t√©, Marc-Alexandre and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
	month = may,
	year = {2016},
	note = {arXiv: 1605.02226},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\4HBS62JY\\Uria et al. - 2016 - Neural Autoregressive Distribution Estimation.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\U5SEWKJN\\1605.html:text/html}
}

@misc{noauthor_linear_2019,
	title = {Linear temporal logic},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Linear_temporal_logic&oldid=930563841},
	abstract = {In logic, linear temporal logic or linear-time temporal logic (LTL) is a modal temporal logic with modalities referring to time. In LTL, one can encode formulae about the future of paths, e.g., a condition will eventually be true, a condition will be true until another fact becomes true, etc. It is a fragment of the more complex CTL*, which additionally allows branching time and quantifiers. Subsequently LTL is sometimes called propositional temporal logic, abbreviated PTL.
Linear temporal logic (LTL) is a fragment of first-order logic.LTL was first proposed for the formal verification of computer programs by Amir Pnueli in 1977.},
	language = {en},
	urldate = {2020-02-14},
	journal = {Wikipedia},
	month = dec,
	year = {2019},
	note = {Page Version ID: 930563841},
	file = {Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\YKBRQIC9\\index.html:text/html}
}

@misc{noauthor_graph_nodate,
	title = {Graph {Convolutional} {Networks} {\textbar}¬†{Thomas} {Kipf} {\textbar} {PhD} {Student} @ {University} of {Amsterdam}},
	url = {https://tkipf.github.io/graph-convolutional-networks/},
	urldate = {2020-02-14},
	file = {Graph Convolutional Networks |¬†Thomas Kipf | PhD Student @ University of Amsterdam:C\:\\Users\\Vval\\Zotero\\storage\\L3IWAUKE\\graph-convolutional-networks.html:text/html}
}

@article{kipf_semi-supervised_2017,
	title = {Semi-{Supervised} {Classification} with {Graph} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1609.02907},
	abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
	urldate = {2020-02-14},
	journal = {arXiv:1609.02907 [cs, stat]},
	author = {Kipf, Thomas N. and Welling, Max},
	month = feb,
	year = {2017},
	note = {arXiv: 1609.02907},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Published as a conference paper at ICLR 2017},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\N6UG7ITN\\Kipf et Welling - 2017 - Semi-Supervised Classification with Graph Convolut.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\KK3DI2PB\\1609.html:text/html}
}

@article{defferrard_convolutional_2017,
	title = {Convolutional {Neural} {Networks} on {Graphs} with {Fast} {Localized} {Spectral} {Filtering}},
	url = {http://arxiv.org/abs/1606.09375},
	abstract = {In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.},
	urldate = {2020-02-14},
	journal = {arXiv:1606.09375 [cs, stat]},
	author = {Defferrard, Micha√´l and Bresson, Xavier and Vandergheynst, Pierre},
	month = feb,
	year = {2017},
	note = {arXiv: 1606.09375},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: NIPS 2016 final revision},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\TFYD5RWA\\Defferrard et al. - 2017 - Convolutional Neural Networks on Graphs with Fast .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\HNNWWNJV\\1606.html:text/html}
}

@incollection{van_den_oord_neural_2017,
	title = {Neural {Discrete} {Representation} {Learning}},
	url = {http://papers.nips.cc/paper/7210-neural-discrete-representation-learning.pdf},
	urldate = {2020-03-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {van den Oord, Aaron and Vinyals, Oriol and kavukcuoglu, koray},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	pages = {6306--6315},
	file = {NIPS Full Text PDF:C\:\\Users\\Vval\\Zotero\\storage\\JCQPPZBJ\\van den Oord et al. - 2017 - Neural Discrete Representation Learning.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\6WUXYQIC\\7210-neural-discrete-representation-learning.html:text/html}
}

@incollection{le_variational_2018,
	title = {Variational {Memory} {Encoder}-{Decoder}},
	url = {http://papers.nips.cc/paper/7424-variational-memory-encoder-decoder.pdf},
	urldate = {2020-03-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Le, Hung and Tran, Truyen and Nguyen, Thin and Venkatesh, Svetha},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {1508--1518},
	file = {NIPS Full Text PDF:C\:\\Users\\Vval\\Zotero\\storage\\2C8PHCXQ\\Le et al. - 2018 - Variational Memory Encoder-Decoder.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\ICF2D4NY\\7424-variational-memory-encoder-decoder.html:text/html}
}

@article{kalchbrenner_efficient_2018,
	title = {Efficient {Neural} {Audio} {Synthesis}},
	url = {http://arxiv.org/abs/1802.08435},
	abstract = {Sequential models achieve state-of-the-art results in audio, visual and textual domains with respect to both estimating the data distribution and generating high-quality samples. Efficient sampling for this class of models has however remained an elusive problem. With a focus on text-to-speech synthesis, we describe a set of general techniques for reducing sampling time while maintaining high output quality. We first describe a single-layer recurrent neural network, the WaveRNN, with a dual softmax layer that matches the quality of the state-of-the-art WaveNet model. The compact form of the network makes it possible to generate 24kHz 16-bit audio 4x faster than real time on a GPU. Second, we apply a weight pruning technique to reduce the number of weights in the WaveRNN. We find that, for a constant number of parameters, large sparse networks perform better than small dense networks and this relationship holds for sparsity levels beyond 96\%. The small number of weights in a Sparse WaveRNN makes it possible to sample high-fidelity audio on a mobile CPU in real time. Finally, we propose a new generation scheme based on subscaling that folds a long sequence into a batch of shorter sequences and allows one to generate multiple samples at once. The Subscale WaveRNN produces 16 samples per step without loss of quality and offers an orthogonal method for increasing sampling efficiency.},
	urldate = {2020-03-20},
	journal = {arXiv:1802.08435 [cs, eess]},
	author = {Kalchbrenner, Nal and Elsen, Erich and Simonyan, Karen and Noury, Seb and Casagrande, Norman and Lockhart, Edward and Stimberg, Florian and Oord, Aaron van den and Dieleman, Sander and Kavukcuoglu, Koray},
	month = jun,
	year = {2018},
	note = {arXiv: 1802.08435},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: 10 pages},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\V6C33Q4N\\Kalchbrenner et al. - 2018 - Efficient Neural Audio Synthesis.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\A3JL4EFM\\1802.html:text/html}
}

@article{mehri_samplernn_2017,
	title = {{SampleRNN}: {An} {Unconditional} {End}-to-{End} {Neural} {Audio} {Generation} {Model}},
	shorttitle = {{SampleRNN}},
	url = {http://arxiv.org/abs/1612.07837},
	abstract = {In this paper we propose a novel model for unconditional audio generation based on generating one audio sample at a time. We show that our model, which profits from combining memory-less modules, namely autoregressive multilayer perceptrons, and stateful recurrent neural networks in a hierarchical structure is able to capture underlying sources of variations in the temporal sequences over very long time spans, on three datasets of different nature. Human evaluation on the generated samples indicate that our model is preferred over competing models. We also show how each component of the model contributes to the exhibited performance.},
	urldate = {2020-03-20},
	journal = {arXiv:1612.07837 [cs]},
	author = {Mehri, Soroush and Kumar, Kundan and Gulrajani, Ishaan and Kumar, Rithesh and Jain, Shubham and Sotelo, Jose and Courville, Aaron and Bengio, Yoshua},
	month = feb,
	year = {2017},
	note = {arXiv: 1612.07837},
	keywords = {Computer Science - Sound, Computer Science - Artificial Intelligence},
	annote = {Comment: Published as a conference paper at ICLR 2017},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\X5FKPZ5P\\Mehri et al. - 2017 - SampleRNN An Unconditional End-to-End Neural Audi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\2QFFETH9\\1612.html:text/html}
}

@article{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	urldate = {2020-03-20},
	journal = {arXiv:1406.2661 [cs, stat]},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.2661},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\9D4LM3KW\\Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\5BGI54NA\\1406.html:text/html}
}

@article{donahue_adversarial_2019,
	title = {Adversarial {Audio} {Synthesis}},
	url = {http://arxiv.org/abs/1802.04208},
	abstract = {Audio signals are sampled at high temporal resolutions, and learning to synthesize audio requires capturing structure across a range of timescales. Generative adversarial networks (GANs) have seen wide success at generating images that are both locally and globally coherent, but they have seen little application to audio generation. In this paper we introduce WaveGAN, a first attempt at applying GANs to unsupervised synthesis of raw-waveform audio. WaveGAN is capable of synthesizing one second slices of audio waveforms with global coherence, suitable for sound effect generation. Our experiments demonstrate that, without labels, WaveGAN learns to produce intelligible words when trained on a small-vocabulary speech dataset, and can also synthesize audio from other domains such as drums, bird vocalizations, and piano. We compare WaveGAN to a method which applies GANs designed for image generation on image-like audio feature representations, finding both approaches to be promising.},
	urldate = {2020-03-20},
	journal = {arXiv:1802.04208 [cs]},
	author = {Donahue, Chris and McAuley, Julian and Puckette, Miller},
	month = feb,
	year = {2019},
	note = {arXiv: 1802.04208},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
	annote = {Comment: Published as a conference paper at ICLR 2019},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\8YD3JNAU\\Donahue et al. - 2019 - Adversarial Audio Synthesis.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\NW4AWPVY\\1802.html:text/html}
}

@article{mason_essential_nodate,
	title = {Essential {Neo}-{Riemannian} {Theory} for {Today}'s {Musician}},
	language = {en},
	author = {Mason, Laura Felicity},
	pages = {98},
	file = {Mason - Essential Neo-Riemannian Theory for Today's Musici.pdf:C\:\\Users\\Vval\\Zotero\\storage\\R3JYAHIL\\Mason - Essential Neo-Riemannian Theory for Today's Musici.pdf:application/pdf}
}

@article{goldberg_word2vec_2014,
	title = {word2vec {Explained}: deriving {Mikolov} et al.'s negative-sampling word-embedding method},
	shorttitle = {word2vec {Explained}},
	url = {http://arxiv.org/abs/1402.3722},
	abstract = {The word2vec software of Tomas Mikolov and colleagues (https://code.google.com/p/word2vec/ ) has gained a lot of traction lately, and provides state-of-the-art word embeddings. The learning models behind the software are described in two research papers. We found the description of the models in these papers to be somewhat cryptic and hard to follow. While the motivations and presentation may be obvious to the neural-networks language-modeling crowd, we had to struggle quite a bit to figure out the rationale behind the equations. This note is an attempt to explain equation (4) (negative sampling) in "Distributed Representations of Words and Phrases and their Compositionality" by Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean.},
	urldate = {2020-03-20},
	journal = {arXiv:1402.3722 [cs, stat]},
	author = {Goldberg, Yoav and Levy, Omer},
	month = feb,
	year = {2014},
	note = {arXiv: 1402.3722},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\2VUVGNQM\\Goldberg et Levy - 2014 - word2vec Explained deriving Mikolov et al.'s nega.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\UQTB652Q\\1402.html:text/html}
}

@article{mikolov_efficient_2013,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {http://arxiv.org/abs/1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	urldate = {2020-03-20},
	journal = {arXiv:1301.3781 [cs]},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = sep,
	year = {2013},
	note = {arXiv: 1301.3781},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\ZHL57PMA\\Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\5M6TPJB6\\1301.html:text/html}
}

@article{sak_long_2014,
	title = {Long {Short}-{Term} {Memory} {Based} {Recurrent} {Neural} {Network} {Architectures} for {Large} {Vocabulary} {Speech} {Recognition}},
	url = {http://arxiv.org/abs/1402.1128},
	abstract = {Long Short-Term Memory (LSTM) is a recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. They have been successfully used for sequence labeling and sequence prediction tasks, such as handwriting recognition, language modeling, phonetic labeling of acoustic frames. However, in contrast to the deep neural networks, the use of RNNs in speech recognition has been limited to phone recognition in small scale tasks. In this paper, we present novel LSTM based RNN architectures which make more effective use of model parameters to train acoustic models for large vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models.},
	urldate = {2020-03-20},
	journal = {arXiv:1402.1128 [cs, stat]},
	author = {Sak, Ha≈üim and Senior, Andrew and Beaufays, Fran√ßoise},
	month = feb,
	year = {2014},
	note = {arXiv: 1402.1128},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\957PWVEW\\Sak et al. - 2014 - Long Short-Term Memory Based Recurrent Neural Netw.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\EYLG6HT3\\1402.html:text/html}
}

@incollection{mikolov_distributed_2013,
	title = {Distributed {Representations} of {Words} and {Phrases} and their {Compositionality}},
	url = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf},
	urldate = {2020-03-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 26},
	publisher = {Curran Associates, Inc.},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
	editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
	pages = {3111--3119},
	file = {NIPS Full Text PDF:C\:\\Users\\Vval\\Zotero\\storage\\IHQTKCSS\\Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\F7WKZXXS\\5021-distributed-representations-of-words-and-phrases-and-their-compositionality.html:text/html}
}

@misc{karani_introduction_2018,
	title = {Introduction to {Word} {Embedding} and {Word2Vec}},
	url = {https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa},
	abstract = {Word embedding is one of the most popular representation of document vocabulary. It is capable of capturing context of a word in a‚Ä¶},
	language = {en},
	urldate = {2020-03-20},
	journal = {Medium},
	author = {Karani, Dhruvil},
	month = sep,
	year = {2018},
	note = {Library Catalog: towardsdatascience.com},
	file = {Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\LMAZBG46\\introduction-to-word-embedding-and-word2vec-652d0c2060fa.html:text/html}
}

@article{wu_word_2018,
	title = {Word {Mover}'s {Embedding}: {From} {Word2Vec} to {Document} {Embedding}},
	shorttitle = {Word {Mover}'s {Embedding}},
	url = {http://arxiv.org/abs/1811.01713},
	abstract = {While the celebrated Word2Vec technique yields semantically rich representations for individual words, there has been relatively less success in extending to generate unsupervised sentences or documents embeddings. Recent work has demonstrated that a distance measure between documents called {\textbackslash}emph\{Word Mover's Distance\} (WMD) that aligns semantically similar words, yields unprecedented KNN classification accuracy. However, WMD is expensive to compute, and it is hard to extend its use beyond a KNN classifier. In this paper, we propose the {\textbackslash}emph\{Word Mover's Embedding \} (WME), a novel approach to building an unsupervised document (sentence) embedding from pre-trained word embeddings. In our experiments on 9 benchmark text classification datasets and 22 textual similarity tasks, the proposed technique consistently matches or outperforms state-of-the-art techniques, with significantly higher accuracy on problems of short length.},
	urldate = {2020-03-20},
	journal = {arXiv:1811.01713 [cs, stat]},
	author = {Wu, Lingfei and Yen, Ian E. H. and Xu, Kun and Xu, Fangli and Balakrishnan, Avinash and Chen, Pin-Yu and Ravikumar, Pradeep and Witbrock, Michael J.},
	month = oct,
	year = {2018},
	note = {arXiv: 1811.01713},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	annote = {Comment: EMNLP'18 Camera-Ready Version},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\DM26YA8G\\Wu et al. - 2018 - Word Mover's Embedding From Word2Vec to Document .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\DRCQJWXE\\1811.html:text/html}
}

@article{rong_word2vec_2016,
	title = {word2vec {Parameter} {Learning} {Explained}},
	url = {http://arxiv.org/abs/1411.2738},
	abstract = {The word2vec model and application by Mikolov et al. have attracted a great amount of attention in recent two years. The vector representations of words learned by word2vec models have been shown to carry semantic meanings and are useful in various NLP tasks. As an increasing number of researchers would like to experiment with word2vec or similar techniques, I notice that there lacks a material that comprehensively explains the parameter learning process of word embedding models in details, thus preventing researchers that are non-experts in neural networks from understanding the working mechanism of such models.},
	language = {en},
	urldate = {2020-03-20},
	journal = {arXiv:1411.2738 [cs]},
	author = {Rong, Xin},
	month = jun,
	year = {2016},
	note = {arXiv: 1411.2738},
	keywords = {Computer Science - Computation and Language},
	file = {Rong - 2016 - word2vec Parameter Learning Explained.pdf:C\:\\Users\\Vval\\Zotero\\storage\\D8AP2LG6\\Rong - 2016 - word2vec Parameter Learning Explained.pdf:application/pdf}
}

@misc{noauthor_beginners_nodate,
	title = {A {Beginner}'s {Guide} to {Word2Vec} and {Neural} {Word} {Embeddings}},
	url = {http://pathmind.com/wiki/word2vec},
	abstract = {Projection of word relationships in higher dimensions for language processing.},
	language = {en},
	urldate = {2020-03-20},
	journal = {Pathmind},
	note = {Library Catalog: pathmind.com},
	file = {Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\TMFYVBZV\\word2vec.html:text/html}
}

@article{adiloglu_machine_2007,
	title = {A machine learning approach to two-voice counterpoint composition},
	volume = {20},
	doi = {10.1016/j.knosys.2006.04.018},
	abstract = {Algorithmic composition of musical pieces is one of the most popular areas of computer aided music research. Various attempts have been made successfully in the area of music composition. Artificial intelligence methods have been extensively applied in this area. Representation of musical pieces in a computer-understandable form plays an important role in computer aided music research.This paper presents a neural network-based knowledge representation schema for representing notes, melodies, and time in first species counterpoint pieces. A musical note is composed of pitch and duration in this representation schema. The proposed representation technique was tested using the back-propagation algorithm to generate two-voice counterpoint pieces.},
	journal = {Knowledge-Based Systems},
	author = {Adiloglu, Kamil and Alpaslan, Ferda},
	month = apr,
	year = {2007},
	pages = {300--309},
	file = {Full Text PDF:C\:\\Users\\Vval\\Zotero\\storage\\9WELI5EH\\Adiloglu et Alpaslan - 2007 - A machine learning approach to two-voice counterpo.pdf:application/pdf}
}

@misc{noauthor_variational_nodate,
	title = {The variational auto-encoder},
	url = {https://ermongroup.github.io/cs228-notes/extras/vae/},
	urldate = {2020-03-20},
	file = {The variational auto-encoder:C\:\\Users\\Vval\\Zotero\\storage\\KTCFYPP2\\vae.html:text/html}
}

@misc{noauthor_variational_nodate-1,
	title = {The variational auto-encoder},
	url = {https://ermongroup.github.io/cs228-notes/extras/vae/},
	urldate = {2020-03-20},
	file = {The variational auto-encoder:C\:\\Users\\Vval\\Zotero\\storage\\8MIGZUM6\\vae.html:text/html}
}

@misc{noauthor_tutorial_nodate,
	title = {Tutorial - {What} is a variational autoencoder?},
	url = {/what-is-variational-autoencoder-vae-tutorial/},
	abstract = {Understanding Variational Autoencoders (VAEs) from two perspectives: deep learning and graphical models.},
	urldate = {2020-03-20},
	journal = {Jaan Altosaar},
	note = {Library Catalog: jaan.io},
	file = {Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\IZCEYT9X\\what-is-variational-autoencoder-vae-tutorial.html:text/html}
}

@article{doersch_tutorial_2016-2,
	title = {Tutorial on {Variational} {Autoencoders}},
	url = {http://arxiv.org/abs/1606.05908},
	abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
	urldate = {2020-03-20},
	journal = {arXiv:1606.05908 [cs, stat]},
	author = {Doersch, Carl},
	month = aug,
	year = {2016},
	note = {arXiv: 1606.05908},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\36GKVQ7I\\Doersch - 2016 - Tutorial on Variational Autoencoders.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\YKRPQH2T\\1606.html:text/html}
}

@article{akrami_robust_2019,
	title = {Robust {Variational} {Autoencoder}},
	url = {http://arxiv.org/abs/1905.09961},
	abstract = {Machine learning methods often need a large amount of labeled training data. Since the training data is assumed to be the ground truth, outliers can severely degrade learned representations and performance of trained models. Here we apply concepts from robust statistics to derive a novel variational autoencoder that is robust to outliers in the training data. Variational autoencoders (VAEs) extract a lower-dimensional encoded feature representation from which we can generate new data samples. Robustness of autoencoders to outliers is critical for generating a reliable representation of particular data types in the encoded space when using corrupted training data. Our robust VAE is based on beta-divergence rather than the standard Kullback-Leibler (KL) divergence. Our proposed lower bound lead to a RVAE model that has the same computational complexity as the VAE and contains a single tuning parameter to control the degree of robustness. We demonstrate the performance of our \${\textbackslash}beta\$-divergence based autoencoder for a range of image datasets, showing improved robustness to outliers both qualitatively and quantitatively. We also illustrate the use of our robust VAE for outlier detection.},
	urldate = {2020-03-20},
	journal = {arXiv:1905.09961 [cs, eess, stat]},
	author = {Akrami, Haleh and Joshi, Anand A. and Li, Jian and Aydore, Sergul and Leahy, Richard M.},
	month = dec,
	year = {2019},
	note = {arXiv: 1905.09961},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\KVEWDYZR\\Akrami et al. - 2019 - Robust Variational Autoencoder.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\SEV3IK65\\1905.html:text/html}
}

@article{liu_towards_2020,
	title = {Towards {Visually} {Explaining} {Variational} {Autoencoders}},
	url = {http://arxiv.org/abs/1911.07389},
	abstract = {Recent advances in Convolutional Neural Network (CNN) model interpretability have led to impressive progress in visualizing and understanding model predictions. In particular, gradient-based visual attention methods have driven much recent effort in using visual attention maps as a means for visual explanations. A key problem, however, is these methods are designed for classification and categorization tasks, and their extension to explaining generative models, e.g. variational autoencoders (VAE) is not trivial. In this work, we take a step towards bridging this crucial gap, proposing the first technique to visually explain VAEs by means of gradient-based attention. We present methods to generate visual attention from the learned latent space, and also demonstrate such attention explanations serve more than just explaining VAE predictions. We show how these attention maps can be used to localize anomalies in images, demonstrating state-of-the-art performance on the MVTec-AD dataset. We also show how they can be infused into model training, helping bootstrap the VAE into learning improved latent space disentanglement, demonstrated on the Dsprites dataset.},
	urldate = {2020-03-20},
	journal = {arXiv:1911.07389 [cs]},
	author = {Liu, Wenqian and Li, Runze and Zheng, Meng and Karanam, Srikrishna and Wu, Ziyan and Bhanu, Bir and Radke, Richard J. and Camps, Octavia},
	month = mar,
	year = {2020},
	note = {arXiv: 1911.07389},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 10 pages, 8 figures, 3 tables, CVPR 2020},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\ZV7XHCS8\\Liu et al. - 2020 - Towards Visually Explaining Variational Autoencode.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\R37KT6F2\\1911.html:text/html}
}

@article{boulanger-lewandowski_modeling_2012,
	title = {Modeling {Temporal} {Dependencies} in {High}-{Dimensional} {Sequences}: {Application} to {Polyphonic} {Music} {Generation} and {Transcription}},
	shorttitle = {Modeling {Temporal} {Dependencies} in {High}-{Dimensional} {Sequences}},
	url = {http://arxiv.org/abs/1206.6392},
	abstract = {We investigate the problem of modeling symbolic sequences of polyphonic music in a completely general piano-roll representation. We introduce a probabilistic model based on distribution estimators conditioned on a recurrent neural network that is able to discover temporal dependencies in high-dimensional sequences. Our approach outperforms many traditional models of polyphonic music on a variety of realistic datasets. We show how our musical language model can serve as a symbolic prior to improve the accuracy of polyphonic transcription.},
	urldate = {2020-03-20},
	journal = {arXiv:1206.6392 [cs, stat]},
	author = {Boulanger-Lewandowski, Nicolas and Bengio, Yoshua and Vincent, Pascal},
	month = jun,
	year = {2012},
	note = {arXiv: 1206.6392},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Statistics - Machine Learning},
	annote = {Comment: Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\BQVAVI52\\Boulanger-Lewandowski et al. - 2012 - Modeling Temporal Dependencies in High-Dimensional.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\7PH74QDY\\1206.html:text/html}
}

@article{uria_neural_2016-1,
	title = {Neural {Autoregressive} {Distribution} {Estimation}},
	url = {http://arxiv.org/abs/1605.02226},
	abstract = {We present Neural Autoregressive Distribution Estimation (NADE) models, which are neural network architectures applied to the problem of unsupervised distribution and density estimation. They leverage the probability product rule and a weight sharing scheme inspired from restricted Boltzmann machines, to yield an estimator that is both tractable and has good generalization performance. We discuss how they achieve competitive performance in modeling both binary and real-valued observations. We also present how deep NADE models can be trained to be agnostic to the ordering of input dimensions used by the autoregressive product rule decomposition. Finally, we also show how to exploit the topological structure of pixels in images using a deep convolutional architecture for NADE.},
	urldate = {2020-03-20},
	journal = {arXiv:1605.02226 [cs]},
	author = {Uria, Benigno and C√¥t√©, Marc-Alexandre and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
	month = may,
	year = {2016},
	note = {arXiv: 1605.02226},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\I48JZCRN\\Uria et al. - 2016 - Neural Autoregressive Distribution Estimation.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\HKWDYIZX\\1605.html:text/html}
}

@article{rudolph_structuring_2019,
	title = {Structuring {Autoencoders}},
	url = {http://arxiv.org/abs/1908.02626},
	abstract = {In this paper we propose Structuring AutoEncoders (SAE). SAEs are neural networks which learn a low dimensional representation of data which are additionally enriched with a desired structure in this low dimensional space. While traditional Autoencoders have proven to structure data naturally they fail to discover semantic structure that is hard to recognize in the raw data. The SAE solves the problem by enhancing a traditional Autoencoder using weak supervision to form a structured latent space. In the experiments we demonstrate, that the structured latent space allows for a much more efficient data representation for further tasks such as classification for sparsely labeled data, an efficient choice of data to label, and morphing between classes. To demonstrate the general applicability of our method, we show experiments on the benchmark image datasets MNIST, Fashion-MNIST, DeepFashion2 and on a dataset of 3D human shapes.},
	urldate = {2020-03-20},
	journal = {arXiv:1908.02626 [cs, stat]},
	author = {Rudolph, Marco and Wandt, Bastian and Rosenhahn, Bodo},
	month = aug,
	year = {2019},
	note = {arXiv: 1908.02626},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\7RBM8BV3\\Rudolph et al. - 2019 - Structuring Autoencoders.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\ZZGBPIUD\\1908.html:text/html}
}

@article{moor_topological_2020,
	title = {Topological {Autoencoders}},
	url = {http://arxiv.org/abs/1906.00722},
	abstract = {We propose a novel approach for preserving topological structures of the input space in latent representations of autoencoders. Using persistent homology, a technique from topological data analysis, we calculate topological signatures of both the input and latent space to derive a topological loss term. Under weak theoretical assumptions, we construct this loss in a differentiable manner, such that the encoding learns to retain multi-scale connectivity information. We show that our approach is theoretically well-founded and that it exhibits favourable latent representations on a synthetic manifold as well as on real-world image data sets, while preserving low reconstruction errors.},
	urldate = {2020-03-20},
	journal = {arXiv:1906.00722 [cs, math, stat]},
	author = {Moor, Michael and Horn, Max and Rieck, Bastian and Borgwardt, Karsten},
	month = feb,
	year = {2020},
	note = {arXiv: 1906.00722},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Algebraic Topology},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\IJP9EEVW\\Moor et al. - 2020 - Topological Autoencoders.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\FMWXP3CA\\1906.html:text/html}
}

@article{tschannen_recent_2018,
	title = {Recent {Advances} in {Autoencoder}-{Based} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1812.05069},
	abstract = {Learning useful representations with little or no supervision is a key challenge in artificial intelligence. We provide an in-depth review of recent advances in representation learning with a focus on autoencoder-based models. To organize these results we make use of meta-priors believed useful for downstream tasks, such as disentanglement and hierarchical organization of features. In particular, we uncover three main mechanisms to enforce such properties, namely (i) regularizing the (approximate or aggregate) posterior distribution, (ii) factorizing the encoding and decoding distribution, or (iii) introducing a structured prior distribution. While there are some promising results, implicit or explicit supervision remains a key enabler and all current methods use strong inductive biases and modeling assumptions. Finally, we provide an analysis of autoencoder-based representation learning through the lens of rate-distortion theory and identify a clear tradeoff between the amount of prior knowledge available about the downstream tasks, and how useful the representation is for this task.},
	urldate = {2020-03-20},
	journal = {arXiv:1812.05069 [cs, stat]},
	author = {Tschannen, Michael and Bachem, Olivier and Lucic, Mario},
	month = dec,
	year = {2018},
	note = {arXiv: 1812.05069},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Presented at the third workshop on Bayesian Deep Learning (NeurIPS 2018)},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\LWSA9W2E\\Tschannen et al. - 2018 - Recent Advances in Autoencoder-Based Representatio.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\YEQWA3XH\\1812.html:text/html}
}

@inproceedings{conklin_music_2003,
	title = {Music {Generation} from {Statistical} {Models}},
	abstract = {This paper discusses the use of statistical models for the problem of musical style imitation. Statistical models  are created from extant pieces in a stylistic corpus, and have an objective goal which is to accurately classify  new pieces. The process of music generation is equated with the problem of sampling from a statistical  model. In principle there is no need to make the classical distinction between analytic and synthetic models  of music. This paper presents several methods for sampling from an analytic statistical model, and proposes  a new approach that maintains the intra opus pattern repetition within an extant piece. A major component of  creativity is the adaptation of extant art works, and this is also an efficient way to sample pieces from complex  statistical models.},
	booktitle = {Proceedings of the {Aisb} 2003 {Symposium} on {Artificial} {In}‚Ñ°ligence and {Creativity} in the {Arts} and {Sciences}},
	author = {Conklin, Darrell},
	year = {2003},
	pages = {30--35},
	file = {Citeseer - Full Text PDF:C\:\\Users\\Vval\\Zotero\\storage\\DMRE97RJ\\Conklin - 2003 - Music Generation from Statistical Models.pdf:application/pdf;Citeseer - Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\D5MWH9WK\\summary.html:text/html}
}

@article{lattner_imposing_2018,
	title = {Imposing higher-level {Structure} in {Polyphonic} {Music} {Generation} using {Convolutional} {Restricted} {Boltzmann} {Machines} and {Constraints}},
	volume = {2},
	issn = {2399-7656},
	url = {http://arxiv.org/abs/1612.04742},
	doi = {10.5920/jcms.2018.01},
	abstract = {We introduce a method for imposing higher-level structure on generated, polyphonic music. A Convolutional Restricted Boltzmann Machine (C-RBM) as a generative model is combined with gradient descent constraint optimisation to provide further control over the generation process. Among other things, this allows for the use of a "template" piece, from which some structural properties can be extracted, and transferred as constraints to the newly generated material. The sampling process is guided with Simulated Annealing to avoid local optima, and to find solutions that both satisfy the constraints, and are relatively stable with respect to the C-RBM. Results show that with this approach it is possible to control the higher-level self-similarity structure, the meter, and the tonal properties of the resulting musical piece, while preserving its local musical coherence.},
	number = {2},
	urldate = {2020-03-20},
	journal = {Journal of Creative Music Systems},
	author = {Lattner, Stefan and Grachten, Maarten and Widmer, Gerhard},
	month = mar,
	year = {2018},
	note = {arXiv: 1612.04742},
	keywords = {Computer Science - Sound, Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence},
	annote = {Comment: 31 pages, 11 figures},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\TBWQWTLS\\Lattner et al. - 2018 - Imposing higher-level Structure in Polyphonic Musi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\XZRP664A\\1612.html:text/html}
}

@misc{sharma_restricted_2018,
	title = {Restricted {Boltzmann} {Machines} ‚Äî {Simplified}},
	url = {https://towardsdatascience.com/restricted-boltzmann-machines-simplified-eab1e5878976},
	abstract = {In this post, I will try to shed some light on the intuition about Restricted Boltzmann Machines and the way they work. This is supposed‚Ä¶},
	language = {en},
	urldate = {2020-03-20},
	journal = {Medium},
	author = {Sharma, Aditya},
	month = dec,
	year = {2018},
	note = {Library Catalog: towardsdatascience.com},
	file = {Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\JP4DR329\\restricted-boltzmann-machines-simplified-eab1e5878976.html:text/html}
}

@misc{noauthor_restricted_2018,
	title = {Restricted {Boltzmann} {Machine} {Tutorial} {\textbar} {Deep} {Learning} {Concepts}},
	url = {https://www.edureka.co/blog/restricted-boltzmann-machine-tutorial/},
	abstract = {This Restricted Boltzmann Machine Tutorial will provide you with a detailed insight to the different layers of RBM and their working with examples.},
	language = {en-US},
	urldate = {2020-03-20},
	journal = {Edureka},
	month = nov,
	year = {2018},
	note = {Library Catalog: www.edureka.co
Section: Artificial Intelligence},
	file = {Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\686B63A7\\restricted-boltzmann-machine-tutorial.html:text/html}
}

@inproceedings{salakhutdinov_restricted_2007,
	address = {Corvalis, Oregon},
	title = {Restricted {Boltzmann} machines for collaborative filtering},
	isbn = {978-1-59593-793-3},
	url = {http://portal.acm.org/citation.cfm?doid=1273496.1273596},
	doi = {10.1145/1273496.1273596},
	abstract = {Most of the existing approaches to collaborative Ô¨Åltering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical models, called Restricted Boltzmann Machines (RBM‚Äôs), can be used to model tabular data, such as user‚Äôs ratings of movies. We present eÔ¨Écient learning and inference procedures for this class of models and demonstrate that RBM‚Äôs can be successfully applied to the NetÔ¨Çix data set, containing over 100 million user/movie ratings. We also show that RBM‚Äôs slightly outperform carefully-tuned SVD models. When the predictions of multiple RBM models and multiple SVD models are linearly combined, we achieve an error rate that is well over 6\% better than the score of NetÔ¨Çix‚Äôs own system.},
	language = {en},
	urldate = {2020-03-20},
	booktitle = {Proceedings of the 24th international conference on {Machine} learning - {ICML} '07},
	publisher = {ACM Press},
	author = {Salakhutdinov, Ruslan and Mnih, Andriy and Hinton, Geoffrey},
	year = {2007},
	pages = {791--798},
	file = {Salakhutdinov et al. - 2007 - Restricted Boltzmann machines for collaborative fi.pdf:C\:\\Users\\Vval\\Zotero\\storage\\NUEPF5GG\\Salakhutdinov et al. - 2007 - Restricted Boltzmann machines for collaborative fi.pdf:application/pdf}
}

@article{montufar_restricted_2018,
	title = {Restricted {Boltzmann} {Machines}: {Introduction} and {Review}},
	shorttitle = {Restricted {Boltzmann} {Machines}},
	url = {http://arxiv.org/abs/1806.07066},
	abstract = {The restricted Boltzmann machine is a network of stochastic units with undirected interactions between pairs of visible and hidden units. This model was popularized as a building block of deep learning architectures and has continued to play an important role in applied and theoretical machine learning. Restricted Boltzmann machines carry a rich structure, with connections to geometry, applied algebra, probability, statistics, machine learning, and other areas. The analysis of these models is attractive in its own right and also as a platform to combine and generalize mathematical tools for graphical models with hidden variables. This article gives an introduction to the mathematical analysis of restricted Boltzmann machines, reviews recent results on the geometry of the sets of probability distributions representable by these models, and suggests a few directions for further investigation.},
	urldate = {2020-03-20},
	journal = {arXiv:1806.07066 [cs, math, stat]},
	author = {Montufar, Guido},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.07066},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Theory, Mathematics - Probability, Mathematics - Statistics Theory},
	annote = {Comment: 40 pages, 8 figures},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\2B2IEITU\\Montufar - 2018 - Restricted Boltzmann Machines Introduction and Re.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\8LXICJ2G\\1806.html:text/html}
}

@misc{noauthor_pdf_nodate,
	title = {({PDF}) {An} {Introduction} to {Restricted} {Boltzmann} {Machines}},
	url = {https://www.researchgate.net/publication/243463621_An_Introduction_to_Restricted_Boltzmann_Machines},
	urldate = {2020-03-20},
	file = {(PDF) An Introduction to Restricted Boltzmann Machines:C\:\\Users\\Vval\\Zotero\\storage\\UL2QD9LQ\\243463621_An_Introduction_to_Restricted_Boltzmann_Machines.html:text/html}
}

@inproceedings{fischer_introduction_2012,
	title = {An {Introduction} to {Restricted} {Boltzmann} {Machines}},
	doi = {10.1007/978-3-642-33275-3_2},
	abstract = {Restricted Boltzmann machines (RBMs) are probabilistic
graphical models that can be interpreted as stochastic neural networks.
The increase in computational power and the development of faster learning algorithms have made them applicable to relevant machine learning
problems. They attracted much attention recently after being proposed
as building blocks of multi-layer learning systems called deep belief networks. This tutorial introduces RBMs as undirected graphical models.
The basic concepts of graphical models are introduced first, however,
basic knowledge in statistics is presumed. Different learning algorithms
for RBMs are discussed. As most of them are based on Markov chain
Monte Carlo (MCMC) methods, an introduction to Markov chains and
the required MCMC techniques is provided.},
	author = {Fischer, Asja and Igel, Christian},
	month = jan,
	year = {2012},
	pages = {14--36},
	file = {Full Text PDF:C\:\\Users\\Vval\\Zotero\\storage\\W3S7FCJ7\\Fischer et Igel - 2012 - An Introduction to Restricted Boltzmann Machines.pdf:application/pdf}
}

@article{sutskever_recurrent_nodate,
	title = {The {Recurrent} {Temporal} {Restricted} {Boltzmann} {Machine}},
	abstract = {The Temporal Restricted Boltzmann Machine (TRBM) is a probabilistic model for sequences that is able to successfully model (i.e., generate nice-looking samples of) several very high dimensional sequences, such as motion capture data and the pixels of low resolution videos of balls bouncing in a box. The major disadvantage of the TRBM is that exact inference is extremely hard, since even computing a Gibbs update for a single variable of the posterior is exponentially expensive. This difÔ¨Åculty has necessitated the use of a heuristic inference procedure, that nonetheless was accurate enough for successful learning. In this paper we introduce the Recurrent TRBM, which is a very slight modiÔ¨Åcation of the TRBM for which exact inference is very easy and exact gradient learning is almost tractable. We demonstrate that the RTRBM is better than an analogous TRBM at generating motion capture and videos of bouncing balls.},
	language = {en},
	author = {Sutskever, Ilya and Hinton, Geoffrey and Taylor, Graham},
	pages = {8},
	file = {Sutskever et al. - The Recurrent Temporal Restricted Boltzmann Machin.pdf:C\:\\Users\\Vval\\Zotero\\storage\\KZCVJ5HI\\Sutskever et al. - The Recurrent Temporal Restricted Boltzmann Machin.pdf:application/pdf}
}

@article{mittelman_structured_nodate,
	title = {Structured {Recurrent} {Temporal} {Restricted} {Boltzmann} {Machines}},
	abstract = {The recurrent temporal restricted Boltzmann machine (RTRBM) is a probabilistic time-series model. The topology of the RTRBM graphical model, however, assumes full connectivity between all the pairs of visible units and hidden units, thereby ignoring the dependency structure within the observations. Learning this structure has the potential for not only improving the prediction performance, but also revealing important dependency patterns in the data. For example, given a meteorological dataset, we could identify regional weather patterns. In this work, we propose a new class of RTRBM, which we refer to as the structured RTRBM (SRTRBM), which explicitly uses a graph to model the dependency structure. Our technique is related to methods such as graphical lasso, which are used to learn the topology of Gaussian graphical models. We also develop a spike-and-slab version of the RTRBM, and combine it with the SRTRBM to learn dependency structures in datasets with real-valued observations. Our experimental results using synthetic and real datasets demonstrate that the SRTRBM can signiÔ¨Åcantly improve the prediction performance of the RTRBM, particularly when the number of visible units is large and the size of the training set is small. It also reveals the dependency structures underlying our benchmark datasets.},
	language = {en},
	author = {Mittelman, Roni and Kuipers, Benjamin and Savarese, Silvio and Lee, Honglak},
	pages = {9},
	file = {Mittelman et al. - Structured Recurrent Temporal Restricted Boltzmann.pdf:C\:\\Users\\Vval\\Zotero\\storage\\VTRW377F\\Mittelman et al. - Structured Recurrent Temporal Restricted Boltzmann.pdf:application/pdf}
}

@misc{noauthor_papers_nodate-1,
	title = {Papers with {Code} - {Convolutional} {Restricted} {Boltzmann} {Machine} {Based}-{Radiomics} for {Prediction} of {Pathological} {Complete} {Response} to {Neoadjuvant} {Chemotherapy} in {Breast} {Cancer}},
	url = {https://www.paperswithcode.com/paper/190513312},
	abstract = {No code available yet.},
	language = {en},
	urldate = {2020-03-20},
	note = {Library Catalog: www.paperswithcode.com},
	file = {Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\B9DRCWBR\\190513312.html:text/html}
}

@inproceedings{lee_convolutional_2009,
	address = {Montreal, Quebec, Canada},
	title = {Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations},
	isbn = {978-1-60558-516-1},
	url = {http://portal.acm.org/citation.cfm?doid=1553374.1553453},
	doi = {10.1145/1553374.1553453},
	abstract = {There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks. Scaling such models to full-sized, high-dimensional images remains a diÔ¨Écult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model which scales to realistic image sizes. This model is translation-invariant and supports eÔ¨Écient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique which shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images.},
	language = {en},
	urldate = {2020-03-20},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning} - {ICML} '09},
	publisher = {ACM Press},
	author = {Lee, Honglak and Grosse, Roger and Ranganath, Rajesh and Ng, Andrew Y.},
	year = {2009},
	pages = {1--8},
	file = {Lee et al. - 2009 - Convolutional deep belief networks for scalable un.pdf:C\:\\Users\\Vval\\Zotero\\storage\\ZYBL4PLT\\Lee et al. - 2009 - Convolutional deep belief networks for scalable un.pdf:application/pdf}
}

@article{wang_convolutional_nodate,
	title = {Convolutional {Restricted} {Boltzmann} {Machine} {Based}-{Radiomics} for {Prediction} of {Pathological} {Complete} {Response} to {Neoadjuvant} {Chemotherapy} in {Breast} {Cancer}},
	abstract = {We proposed a novel convolutional restricted Boltzmann machine (CRBM)-based radiomic method for predicting pathologic complete response (pCR) to neoadjuvant chemotherapy treatment (NACT) in breast cancer. The method consists of extracting semantic features from CRBM network, and pCR prediction ‚Ä¶ It was evaluated on the dynamic contrastenhanced magnetic resonance imaging (DCE-MRI) data of 57 patients and using the area under the receiver operating characteristic curve (AUC). Traditional radiomics features and the semantic features learned from CRBM network were extracted from the images acquired before and after the administration of NACT. After the feature selection, the support vector machine (SVM), logistic regression (LR) and random forest (RF) were trained to predict the pCR status. Compared to traditional radiomic methods, the proposed CRBM-based radiomic method yielded an AUC of 0.92 for the prediction with the images acquired before and after NACT, and an AUC of 0.87 for the pretreatment prediction, which was increased by about 38\%. The results showed that the CRBM-based radiomic method provided a potential means for accurately predicting the pCR to NACT in breast cancer before the treatment, which is very useful for making more appropriate and personalized treatment regimens.},
	language = {en},
	author = {Wang, Li and Wang, Lihui and Chen, Qijian and Sun, Caixia and Cheng, Xinyu and Zhu, Yuemin},
	pages = {8},
	file = {Wang et al. - Convolutional Restricted Boltzmann Machine Based-R.pdf:C\:\\Users\\Vval\\Zotero\\storage\\NKKIUQMX\\Wang et al. - Convolutional Restricted Boltzmann Machine Based-R.pdf:application/pdf}
}

@article{norouzi_stacks_nodate,
	title = {Stacks of {Convolutional} {Restricted} {Boltzmann} {Machines} for {Shift}-{Invariant} {Feature} {Learning}},
	abstract = {In this paper we present a method for learning classspeciÔ¨Åc features for recognition. Recently a greedy layerwise procedure was proposed to initialize weights of deep belief networks, by viewing each layer as a separate Restricted Boltzmann Machine (RBM). We develop the Convolutional RBM (C-RBM), a variant of the RBM model in which weights are shared to respect the spatial structure of images. This framework learns a set of features that can generate the images of a speciÔ¨Åc object class. Our feature extraction model is a four layer hierarchy of alternating Ô¨Åltering and maximum subsampling. We learn feature parameters of the Ô¨Årst and third layers viewing them as separate C-RBMs. The outputs of our feature extraction hierarchy are then fed as input to a discriminative classiÔ¨Åer. It is experimentally demonstrated that the extracted features are effective for object detection, using them to obtain performance comparable to the state-of-the-art on handwritten digit recognition and pedestrian detection.},
	language = {en},
	author = {Norouzi, Mohammad and Ranjbar, Mani and Mori, Greg},
	pages = {8},
	file = {Norouzi et al. - Stacks of Convolutional Restricted Boltzmann Machi.pdf:C\:\\Users\\Vval\\Zotero\\storage\\VQPLF5LX\\Norouzi et al. - Stacks of Convolutional Restricted Boltzmann Machi.pdf:application/pdf}
}

@article{norouzi_convolutional_nodate,
	title = {{CONVOLUTIONAL} {RESTRICTED} {BOLTZMANN} {MACHINES} {FOR} {FEATURE} {LEARNING}},
	language = {en},
	author = {Norouzi, Mohammad},
	pages = {61},
	file = {Norouzi - CONVOLUTIONAL RESTRICTED BOLTZMANN MACHINES FOR FE.pdf:C\:\\Users\\Vval\\Zotero\\storage\\JEKGQ57Q\\Norouzi - CONVOLUTIONAL RESTRICTED BOLTZMANN MACHINES FOR FE.pdf:application/pdf}
}

@misc{shetty_neural_2019,
	title = {Neural {Style} {Transfer} {Tutorial} -{Part} 1},
	url = {https://towardsdatascience.com/neural-style-transfer-tutorial-part-1-f5cd3315fa7f},
	abstract = {Theory of Neural Style Transfer},
	language = {en},
	urldate = {2020-03-21},
	journal = {Medium},
	author = {Shetty, Vamshik},
	month = mar,
	year = {2019},
	note = {Library Catalog: towardsdatascience.com},
	file = {Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\W4Z3NKWM\\neural-style-transfer-tutorial-part-1-f5cd3315fa7f.html:text/html}
}

@article{gatys_neural_2015,
	title = {A {Neural} {Algorithm} of {Artistic} {Style}},
	url = {http://arxiv.org/abs/1508.06576},
	abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.},
	urldate = {2020-03-21},
	journal = {arXiv:1508.06576 [cs, q-bio]},
	author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
	month = sep,
	year = {2015},
	note = {arXiv: 1508.06576},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition, Quantitative Biology - Neurons and Cognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\3PVFLBDD\\Gatys et al. - 2015 - A Neural Algorithm of Artistic Style.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\B8PNSSYZ\\1508.html:text/html}
}

@incollection{li_universal_2017,
	title = {Universal {Style} {Transfer} via {Feature} {Transforms}},
	url = {http://papers.nips.cc/paper/6642-universal-style-transfer-via-feature-transforms.pdf},
	urldate = {2020-03-21},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Li, Yijun and Fang, Chen and Yang, Jimei and Wang, Zhaowen and Lu, Xin and Yang, Ming-Hsuan},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	pages = {386--396},
	file = {NIPS Full Text PDF:C\:\\Users\\Vval\\Zotero\\storage\\WFCU9YI2\\Li et al. - 2017 - Universal Style Transfer via Feature Transforms.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\S83P2WK4\\6642-universal-style-transfer-via-feature-transforms.html:text/html}
}

@misc{noauthor_neural_nodate,
	title = {Neural {Style} {Transfer} for {Musical} {Melodies}},
	url = {https://ashispati.github.io//style-transfer/},
	abstract = {Blog-style article with preliminary experiments and findings towards developing deep learning paradigms for style transfer in symbolic music data},
	urldate = {2020-03-21},
	journal = {Ashis Pati},
	note = {Library Catalog: ashispati.github.io},
	file = {Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\2SCXTUH3\\style-transfer.html:text/html}
}

@article{kaliakatsos-papakostas_conceptual_2017,
	title = {Conceptual {Blending} of {Harmonic} {Spaces} for {Creative} {Melodic} {Harmonisation}},
	volume = {46},
	issn = {0929-8215},
	url = {https://doi.org/10.1080/09298215.2017.1355393},
	doi = {10.1080/09298215.2017.1355393},
	abstract = {In computational creativity, new concepts can be invented through conceptual blending of two independent conceptual spaces. In music, conceptual blending has been primarily used for analysing relations between musical and extra-musical elements in composed music rather than generating new music. This paper presents a probabilistic melodic harmonisation assistant that employs conceptual blending to combine learned, potentially diverse, harmonic idioms and generate new harmonic spaces that can be used to harmonise melodies given by the user. The key feature of this system is the application of creative conceptual blending to the most common chord transitions (pairs of consecutive chords) of two initial harmonic idioms. The proposed methodology integrates newly created blended chords and transitions in a compound probabilistic harmonic space, that preserves combined characteristics from both initial idioms along with those new chords and transitions within a unified setting. This methodology enables various interesting music applications, ranging from problem-solving, e.g. harmonising melodies that include key transpositions, to generative harmonic exploration, e.g. combining major‚Äìminor harmonic progressions or more extreme idiosyncratic harmonies.},
	number = {4},
	urldate = {2020-03-21},
	journal = {Journal of New Music Research},
	author = {Kaliakatsos-Papakostas, Maximos and Queiroz, Marcelo and Tsougras, Costas and Cambouropoulos, Emilios},
	month = oct,
	year = {2017},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/09298215.2017.1355393},
	keywords = {conceptual blending, harmonic blending, Markov models, melodic harmonisation},
	pages = {305--328},
	file = {Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\CGWBP7BH\\09298215.2017.html:text/html}
}

@article{hung_musical_2019,
	title = {Musical {Composition} {Style} {Transfer} via {Disentangled} {Timbre} {Representations}},
	url = {http://arxiv.org/abs/1905.13567},
	abstract = {Music creation involves not only composing the different parts (e.g., melody, chords) of a musical work but also arranging/selecting the instruments to play the different parts. While the former has received increasing attention, the latter has not been much investigated. This paper presents, to the best of our knowledge, the first deep learning models for rearranging music of arbitrary genres. Specifically, we build encoders and decoders that take a piece of polyphonic musical audio as input and predict as output its musical score. We investigate disentanglement techniques such as adversarial training to separate latent factors that are related to the musical content (pitch) of different parts of the piece, and that are related to the instrumentation (timbre) of the parts per short-time segment. By disentangling pitch and timbre, our models have an idea of how each piece was composed and arranged. Moreover, the models can realize "composition style transfer" by rearranging a musical piece without much affecting its pitch content. We validate the effectiveness of the models by experiments on instrument activity detection and composition style transfer. To facilitate follow-up research, we open source our code at https://github.com/biboamy/instrument-disentangle.},
	urldate = {2020-03-21},
	journal = {arXiv:1905.13567 [cs, eess]},
	author = {Hung, Yun-Ning and Chiang, I.-Tung and Chen, Yi-An and Yang, Yi-Hsuan},
	month = may,
	year = {2019},
	note = {arXiv: 1905.13567},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	annote = {Comment: Accepted by the 28th International Joint Conference on Artificial Intelligence. arXiv admin note: text overlap with arXiv:1811.03271},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\GMAX9IWG\\Hung et al. - 2019 - Musical Composition Style Transfer via Disentangle.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\RR2J77RG\\1905.html:text/html}
}

@article{brunner_symbolic_2018,
	title = {Symbolic {Music} {Genre} {Transfer} with {CycleGAN}},
	url = {http://arxiv.org/abs/1809.07575},
	abstract = {Deep generative models such as Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) have recently been applied to style and domain transfer for images, and in the case of VAEs, music. GAN-based models employing several generators and some form of cycle consistency loss have been among the most successful for image domain transfer. In this paper we apply such a model to symbolic music and show the feasibility of our approach for music genre transfer. Evaluations using separate genre classifiers show that the style transfer works well. In order to improve the fidelity of the transformed music, we add additional discriminators that cause the generators to keep the structure of the original music mostly intact, while still achieving strong genre transfer. Visual and audible results further show the potential of our approach. To the best of our knowledge, this paper represents the first application of GANs to symbolic music domain transfer.},
	urldate = {2020-03-21},
	journal = {arXiv:1809.07575 [cs, eess, stat]},
	author = {Brunner, Gino and Wang, Yuyi and Wattenhofer, Roger and Zhao, Sumu},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.07575},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning, H.5.5, I.2.6, I.2.1, I.2.4},
	annote = {Comment: Paper accepted at the 30th International Conference on Tools with Artificial Intelligence, ICTAI 2018, Volos, Greece},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\BFNUCSPG\\Brunner et al. - 2018 - Symbolic Music Genre Transfer with CycleGAN.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\73ESQY5V\\1809.html:text/html}
}

@article{lu_play_2018,
	title = {Play as {You} {Like}: {Timbre}-enhanced {Multi}-modal {Music} {Style} {Transfer}},
	shorttitle = {Play as {You} {Like}},
	url = {http://arxiv.org/abs/1811.12214},
	abstract = {Style transfer of polyphonic music recordings is a challenging task when considering the modeling of diverse, imaginative, and reasonable music pieces in the style different from their original one. To achieve this, learning stable multi-modal representations for both domain-variant (i.e., style) and domain-invariant (i.e., content) information of music in an unsupervised manner is critical. In this paper, we propose an unsupervised music style transfer method without the need for parallel data. Besides, to characterize the multi-modal distribution of music pieces, we employ the Multi-modal Unsupervised Image-to-Image Translation (MUNIT) framework in the proposed system. This allows one to generate diverse outputs from the learned latent distributions representing contents and styles. Moreover, to better capture the granularity of sound, such as the perceptual dimensions of timbre and the nuance in instrument-specific performance, cognitively plausible features including mel-frequency cepstral coefficients (MFCC), spectral difference, and spectral envelope, are combined with the widely-used mel-spectrogram into a timber-enhanced multi-channel input representation. The Relativistic average Generative Adversarial Networks (RaGAN) is also utilized to achieve fast convergence and high stability. We conduct experiments on bilateral style transfer tasks among three different genres, namely piano solo, guitar solo, and string quartet. Results demonstrate the advantages of the proposed method in music style transfer with improved sound quality and in allowing users to manipulate the output.},
	urldate = {2020-03-21},
	journal = {arXiv:1811.12214 [cs, eess]},
	author = {Lu, Chien-Yu and Xue, Min-Xin and Chang, Chia-Che and Lee, Che-Rung and Su, Li},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.12214},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\9AG8KQVC\\Lu et al. - 2018 - Play as You Like Timbre-enhanced Multi-modal Musi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\2XSSSNLM\\1811.html:text/html}
}

@article{colombo_learning_2019,
	title = {Learning to {Generate} {Music} with {BachProp}},
	url = {http://arxiv.org/abs/1812.06669},
	abstract = {As deep learning advances, algorithms of music composition increase in performance. However, most of the successful models are designed for specific musical structures. Here, we present BachProp, an algorithmic composer that can generate music scores in many styles given sufficient training data. To adapt BachProp to a broad range of musical styles, we propose a novel representation of music and train a deep network to predict the note transition probabilities of a given music corpus. In this paper, new music scores generated by BachProp are compared with the original corpora as well as with different network architectures and other related models. We show that BachProp captures important features of the original datasets better than other models and invite the reader to a qualitative comparison on a large collection of generated songs.},
	urldate = {2020-03-21},
	journal = {arXiv:1812.06669 [cs, eess, stat]},
	author = {Colombo, Florian and Brea, Johanni and Gerstner, Wulfram},
	month = jun,
	year = {2019},
	note = {arXiv: 1812.06669},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\B7NMC83P\\Colombo et al. - 2019 - Learning to Generate Music with BachProp.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\MDNPR7L6\\1812.html:text/html}
}

@article{zhu_unpaired_2018,
	title = {Unpaired {Image}-to-{Image} {Translation} using {Cycle}-{Consistent} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1703.10593},
	abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain \$X\$ to a target domain \$Y\$ in the absence of paired examples. Our goal is to learn a mapping \$G: X {\textbackslash}rightarrow Y\$ such that the distribution of images from \$G(X)\$ is indistinguishable from the distribution \$Y\$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping \$F: Y {\textbackslash}rightarrow X\$ and introduce a cycle consistency loss to push \$F(G(X)) {\textbackslash}approx X\$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
	urldate = {2020-03-21},
	journal = {arXiv:1703.10593 [cs]},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	month = nov,
	year = {2018},
	note = {arXiv: 1703.10593},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: An extended version of our ICCV 2017 paper, v6 updated the implementation details in the appendix. Code and data: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\MVHT4TT7\\Zhu et al. - 2018 - Unpaired Image-to-Image Translation using Cycle-Co.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\AL6ET2PQ\\1703.html:text/html}
}

@article{jolicoeur-martineau_relativistic_2018,
	title = {The relativistic discriminator: a key element missing from standard {GAN}},
	shorttitle = {The relativistic discriminator},
	url = {http://arxiv.org/abs/1807.00734},
	abstract = {In standard generative adversarial network (SGAN), the discriminator estimates the probability that the input data is real. The generator is trained to increase the probability that fake data is real. We argue that it should also simultaneously decrease the probability that real data is real because 1) this would account for a priori knowledge that half of the data in the mini-batch is fake, 2) this would be observed with divergence minimization, and 3) in optimal settings, SGAN would be equivalent to integral probability metric (IPM) GANs. We show that this property can be induced by using a relativistic discriminator which estimate the probability that the given real data is more realistic than a randomly sampled fake data. We also present a variant in which the discriminator estimate the probability that the given real data is more realistic than fake data, on average. We generalize both approaches to non-standard GAN loss functions and we refer to them respectively as Relativistic GANs (RGANs) and Relativistic average GANs (RaGANs). We show that IPM-based GANs are a subset of RGANs which use the identity function. Empirically, we observe that 1) RGANs and RaGANs are significantly more stable and generate higher quality data samples than their non-relativistic counterparts, 2) Standard RaGAN with gradient penalty generate data of better quality than WGAN-GP while only requiring a single discriminator update per generator update (reducing the time taken for reaching the state-of-the-art by 400\%), and 3) RaGANs are able to generate plausible high resolutions images (256x256) from a very small sample (N=2011), while GAN and LSGAN cannot; these images are of significantly better quality than the ones generated by WGAN-GP and SGAN with spectral normalization.},
	urldate = {2020-03-21},
	journal = {arXiv:1807.00734 [cs, stat]},
	author = {Jolicoeur-Martineau, Alexia},
	month = sep,
	year = {2018},
	note = {arXiv: 1807.00734},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
	annote = {Comment: https://github.com/AlexiaJM/RelativisticGAN},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\WCHKJZZV\\Jolicoeur-Martineau - 2018 - The relativistic discriminator a key element miss.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\LZL3AAEZ\\1807.html:text/html}
}

@article{huang_multimodal_2018,
	title = {Multimodal {Unsupervised} {Image}-to-{Image} {Translation}},
	url = {http://arxiv.org/abs/1804.04732},
	abstract = {Unsupervised image-to-image translation is an important and challenging problem in computer vision. Given an image in the source domain, the goal is to learn the conditional distribution of corresponding images in the target domain, without seeing any pairs of corresponding images. While this conditional distribution is inherently multimodal, existing approaches make an overly simplified assumption, modeling it as a deterministic one-to-one mapping. As a result, they fail to generate diverse outputs from a given source domain image. To address this limitation, we propose a Multimodal Unsupervised Image-to-image Translation (MUNIT) framework. We assume that the image representation can be decomposed into a content code that is domain-invariant, and a style code that captures domain-specific properties. To translate an image to another domain, we recombine its content code with a random style code sampled from the style space of the target domain. We analyze the proposed framework and establish several theoretical results. Extensive experiments with comparisons to the state-of-the-art approaches further demonstrates the advantage of the proposed framework. Moreover, our framework allows users to control the style of translation outputs by providing an example style image. Code and pretrained models are available at https://github.com/nvlabs/MUNIT},
	urldate = {2020-03-21},
	journal = {arXiv:1804.04732 [cs, stat]},
	author = {Huang, Xun and Liu, Ming-Yu and Belongie, Serge and Kautz, Jan},
	month = aug,
	year = {2018},
	note = {arXiv: 1804.04732},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by ECCV 2018},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\HZQ5N343\\Huang et al. - 2018 - Multimodal Unsupervised Image-to-Image Translation.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\8VE7D2V4\\1804.html:text/html}
}

@misc{noauthor_music21corpuschorales_nodate,
	title = {music21.corpus.chorales ‚Äî music21 {Documentation}},
	url = {https://web.mit.edu/music21/doc/moduleReference/moduleCorpusChorales.html},
	urldate = {2020-03-22},
	file = {music21.corpus.chorales ‚Äî music21 Documentation:C\:\\Users\\Vval\\Zotero\\storage\\5W3HEJPS\\moduleCorpusChorales.html:text/html}
}

@misc{noauthor_tensorflow_nodate,
	title = {{TensorFlow}},
	url = {https://www.tensorflow.org/?hl=fr},
	abstract = {An end-to-end open source machine learning platform for everyone. Discover TensorFlow's flexible ecosystem of tools, libraries and community resources.},
	language = {en},
	urldate = {2020-03-22},
	journal = {TensorFlow},
	note = {Library Catalog: www.tensorflow.org},
	file = {Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\93CXG2VI\\www.tensorflow.org.html:text/html}
}

@misc{noauthor_keras_nodate,
	title = {Keras {\textbar} {TensorFlow} {Core}},
	url = {https://www.tensorflow.org/guide/keras?hl=fr},
	language = {en},
	urldate = {2020-03-22},
	journal = {TensorFlow},
	note = {Library Catalog: www.tensorflow.org},
	file = {Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\6I6VLIUK\\keras.html:text/html}
}

@misc{noauthor_music21_nodate,
	title = {music21: a {Toolkit} for {Computer}-{Aided} {Musicology}},
	url = {https://web.mit.edu/music21/},
	urldate = {2020-03-22},
	file = {music21\: a Toolkit for Computer-Aided Musicology:C\:\\Users\\Vval\\Zotero\\storage\\BD3XS9YK\\music21.html:text/html}
}

@misc{noauthor_fl_nodate,
	title = {{FL} {Studio}},
	url = {https://www.image-line.com/flstudio/},
	urldate = {2020-03-22},
	file = {FL Studio:C\:\\Users\\Vval\\Zotero\\storage\\CCNRTJEK\\flstudio.html:text/html}
}

@article{vaswani_attention_2017-1,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2020-03-23},
	journal = {arXiv:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv: 1706.03762},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: 15 pages, 5 figures},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\YEF6NVNX\\Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\WRT5UVV2\\1706.html:text/html}
}

@misc{alammar_illustrated_nodate,
	title = {The {Illustrated} {Transformer}},
	url = {http://jalammar.github.io/illustrated-transformer/},
	abstract = {Discussions:
Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)


Translations: Chinese (Simplified), Korean

Watch: MIT‚Äôs Deep Learning State of the Art lecture referencing this post

In the previous post, we looked at Attention ‚Äì a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer ‚Äì a model that uses attention to boost the speed with which these models can be trained. The Transformers outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud‚Äôs recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let‚Äôs try to break the model apart and look at how it functions.

The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard‚Äôs NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.

A High-Level Look
Let‚Äôs begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.},
	urldate = {2020-03-23},
	author = {Alammar, Jay},
	note = {Library Catalog: jalammar.github.io},
	file = {Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\XRIGG5QG\\illustrated-transformer.html:text/html}
}

@misc{noauthor_transformer_nodate,
	title = {Transformer model for language understanding {\textbar} {TensorFlow} {Core}},
	url = {https://www.tensorflow.org/tutorials/text/transformer?hl=fr},
	language = {en},
	urldate = {2020-03-23},
	journal = {TensorFlow},
	note = {Library Catalog: www.tensorflow.org},
	file = {Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\UMJNKSIY\\transformer.html:text/html}
}

@misc{giacaglia_transformers_2019,
	title = {Transformers},
	url = {https://towardsdatascience.com/transformers-141e32e69591},
	abstract = {Transformers are a type of neural network architecture that have been gaining popularity. Transformers were recently used by OpenAI in‚Ä¶},
	language = {en},
	urldate = {2020-03-23},
	journal = {Medium},
	author = {Giacaglia, Giuliano},
	month = dec,
	year = {2019},
	note = {Library Catalog: towardsdatascience.com},
	file = {Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\XK579I6T\\transformers-141e32e69591.html:text/html}
}

@misc{allard_what_2020,
	title = {What is a {Transformer}?},
	url = {https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04},
	abstract = {An Introduction to Transformers and Sequence-to-Sequence Learning for Machine Learning},
	language = {en},
	urldate = {2020-03-23},
	journal = {Medium},
	author = {Allard, Maxime},
	month = mar,
	year = {2020},
	note = {Library Catalog: medium.com},
	file = {Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\MJETZL53\\what-is-a-transformer-d07dd1fbec04.html:text/html}
}

@article{bahdanau_neural_2016,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	urldate = {2020-03-23},
	journal = {arXiv:1409.0473 [cs, stat]},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = may,
	year = {2016},
	note = {arXiv: 1409.0473},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	annote = {Comment: Accepted at ICLR 2015 as oral presentation},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\XEBRJW9M\\Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\8PEJE8ZJ\\1409.html:text/html}
}

@article{briot_music_2020,
	title = {Music {Generation} by {Deep} {Learning} - {Challenges} and {Directions}},
	volume = {32},
	issn = {0941-0643, 1433-3058},
	url = {http://arxiv.org/abs/1712.04371},
	doi = {10.1007/s00521-018-3813-6},
	abstract = {In addition to traditional tasks such as prediction, classification and translation, deep learning is receiving growing attention as an approach for music generation, as witnessed by recent research groups such as Magenta at Google and CTRL (Creator Technology Research Lab) at Spotify. The motivation is in using the capacity of deep learning architectures and training techniques to automatically learn musical styles from arbitrary musical corpora and then to generate samples from the estimated distribution. However, a direct application of deep learning to generate content rapidly reaches limits as the generated content tends to mimic the training set without exhibiting true creativity. Moreover, deep learning architectures do not offer direct ways for controlling generation (e.g., imposing some tonality or other arbitrary constraints). Furthermore, deep learning architectures alone are autistic automata which generate music autonomously without human user interaction, far from the objective of interactively assisting musicians to compose and refine music. Issues such as: control, structure, creativity and interactivity are the focus of our analysis. In this paper, we select some limitations of a direct application of deep learning to music generation, analyze why the issues are not fulfilled and how to address them by possible approaches. Various examples of recent systems are cited as examples of promising directions.},
	number = {4},
	urldate = {2020-03-23},
	journal = {Neural Comput \& Applic},
	author = {Briot, Jean-Pierre and Pachet, Fran√ßois},
	month = feb,
	year = {2020},
	note = {arXiv: 1712.04371},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {981--993},
	annote = {Comment: 17 pages. arXiv admin note: substantial text overlap with arXiv:1709.01620. Accepted for publication in Special Issue on Deep learning for music and audio, Neural Computing \& Applications, Springer Nature, 2018},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\EECC9TU4\\Briot et Pachet - 2020 - Music Generation by Deep Learning - Challenges and.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\3F4DPAN5\\1712.html:text/html}
}

@article{choi_encoding_2019,
	title = {Encoding {Musical} {Style} with {Transformer} {Autoencoders}},
	url = {http://arxiv.org/abs/1912.05537},
	abstract = {We consider the problem of learning high-level controls over the global structure of sequence generation, particularly in the context of symbolic music generation with complex language models. In this work, we present the Transformer autoencoder, which aggregates encodings of the input data across time to obtain a global representation of style from a given performance. We show it is possible to combine this global embedding with other temporally distributed embeddings, enabling improved control over the separate aspects of performance style and and melody. Empirically, we demonstrate the effectiveness of our method on a variety of music generation tasks on the MAESTRO dataset and a YouTube dataset with 10,000+ hours of piano performances, where we achieve improvements in terms of log-likelihood and mean listening scores as compared to relevant baselines.},
	urldate = {2020-03-23},
	journal = {arXiv:1912.05537 [cs, eess, stat]},
	author = {Choi, Kristy and Hawthorne, Curtis and Simon, Ian and Dinculescu, Monica and Engel, Jesse},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.05537},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\4MVPUSDE\\Choi et al. - 2019 - Encoding Musical Style with Transformer Autoencode.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\THBN6JUQ\\1912.html:text/html}
}

@article{huang_bach_2019,
	title = {The {Bach} {Doodle}: {Approachable} music composition with machine learning at scale},
	shorttitle = {The {Bach} {Doodle}},
	url = {http://arxiv.org/abs/1907.06637},
	abstract = {To make music composition more approachable, we designed the first AI-powered Google Doodle, the Bach Doodle, where users can create their own melody and have it harmonized by a machine learning model Coconet (Huang et al., 2017) in the style of Bach. For users to input melodies, we designed a simplified sheet-music based interface. To support an interactive experience at scale, we re-implemented Coconet in TensorFlow.js (Smilkov et al., 2019) to run in the browser and reduced its runtime from 40s to 2s by adopting dilated depth-wise separable convolutions and fusing operations. We also reduced the model download size to approximately 400KB through post-training weight quantization. We calibrated a speed test based on partial model evaluation time to determine if the harmonization request should be performed locally or sent to remote TPU servers. In three days, people spent 350 years worth of time playing with the Bach Doodle, and Coconet received more than 55 million queries. Users could choose to rate their compositions and contribute them to a public dataset, which we are releasing with this paper. We hope that the community finds this dataset useful for applications ranging from ethnomusicological studies, to music education, to improving machine learning models.},
	urldate = {2020-03-23},
	journal = {arXiv:1907.06637 [cs, eess, stat]},
	author = {Huang, Cheng-Zhi Anna and Hawthorne, Curtis and Roberts, Adam and Dinculescu, Monica and Wexler, James and Hong, Leon and Howcroft, Jacob},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.06637},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning, Computer Science - Human-Computer Interaction},
	annote = {Comment: Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2019},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\M3L6J6U2\\Huang et al. - 2019 - The Bach Doodle Approachable music composition wi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\RSJ27WF5\\1907.html:text/html}
}

@article{huang_music_2018-1,
	title = {Music {Transformer}},
	url = {http://arxiv.org/abs/1809.04281},
	abstract = {Music relies heavily on repetition to build structure and meaning. Self-reference occurs on multiple timescales, from motifs to phrases to reusing of entire sections of music, such as in pieces with ABA structure. The Transformer (Vaswani et al., 2017), a sequence model based on self-attention, has achieved compelling results in many generation tasks that require maintaining long-range coherence. This suggests that self-attention might also be well-suited to modeling music. In musical composition and performance, however, relative timing is critically important. Existing approaches for representing relative positional information in the Transformer modulate attention based on pairwise distance (Shaw et al., 2018). This is impractical for long sequences such as musical compositions since their memory complexity for intermediate relative information is quadratic in the sequence length. We propose an algorithm that reduces their intermediate memory requirement to linear in the sequence length. This enables us to demonstrate that a Transformer with our modified relative attention mechanism can generate minute-long compositions (thousands of steps, four times the length modeled in Oore et al., 2018) with compelling structure, generate continuations that coherently elaborate on a given motif, and in a seq2seq setup generate accompaniments conditioned on melodies. We evaluate the Transformer with our relative attention mechanism on two datasets, JSB Chorales and Piano-e-Competition, and obtain state-of-the-art results on the latter.},
	urldate = {2020-03-23},
	journal = {arXiv:1809.04281 [cs, eess, stat]},
	author = {Huang, Cheng-Zhi Anna and Vaswani, Ashish and Uszkoreit, Jakob and Shazeer, Noam and Simon, Ian and Hawthorne, Curtis and Dai, Andrew M. and Hoffman, Matthew D. and Dinculescu, Monica and Eck, Douglas},
	month = dec,
	year = {2018},
	note = {arXiv: 1809.04281},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	annote = {Comment: Improved skewing section and accompanying figures. Previous titles are "An Improved Relative Self-Attention Mechanism for Transformer with Application to Music Generation" and "Music Transformer"},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\P2CD5IGQ\\Huang et al. - 2018 - Music Transformer.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\IMAUYZZQ\\1809.html:text/html}
}

@article{wu_hierarchical_2018,
	title = {A {Hierarchical} {Recurrent} {Neural} {Network} for {Symbolic} {Melody} {Generation}},
	url = {http://arxiv.org/abs/1712.05274},
	abstract = {In recent years, neural networks have been used to generate symbolic melodies. However, the long-term structure in the melody has posed great difficulty for designing a good model. In this paper, we present a hierarchical recurrent neural network for melody generation, which consists of three Long-Short-Term-Memory (LSTM) subnetworks working in a coarse-to-fine manner along time. Specifically, the three subnetworks generate bar profiles, beat profiles and notes in turn, and the output of the high-level subnetworks are fed into the low-level subnetworks, serving as guidance for generating the finer time-scale melody components in low-level subnetworks. Two human behavior experiments demonstrate the advantage of this structure over the single-layer LSTM which attempts to learn all hidden structures in melodies. Compared with the state-of-the-art models MidiNet and MusicVAE, the hierarchical recurrent neural network produces better melodies evaluated by humans.},
	urldate = {2020-03-23},
	journal = {arXiv:1712.05274 [cs]},
	author = {Wu, Jian and Hu, Changran and Wang, Yulong and Hu, Xiaolin and Zhu, Jun},
	month = sep,
	year = {2018},
	note = {arXiv: 1712.05274},
	keywords = {Computer Science - Sound, Computer Science - Multimedia},
	annote = {Comment: 9 pages},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\QJZLLNV8\\Wu et al. - 2018 - A Hierarchical Recurrent Neural Network for Symbol.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\A9XKRF7B\\1712.html:text/html}
}

@misc{noauthor_tensorflowjs_nodate,
	title = {{TensorFlow}.js {\textbar} {Machine} {Learning} for {Javascript} {Developers}},
	url = {https://www.tensorflow.org/js?hl=fr},
	abstract = {Train and deploy models in the browser, Node.js, or Google Cloud Platform. TensorFlow.js is an open source ML platform for Javascript and web development.},
	language = {en},
	urldate = {2020-03-23},
	journal = {TensorFlow},
	note = {Library Catalog: www.tensorflow.org},
	file = {Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\8AJSFVRS\\js.html:text/html}
}

@misc{noauthor_180702811_nodate,
	title = {[1807.02811] {A} {Tutorial} on {Bayesian} {Optimization}},
	url = {https://arxiv.org/abs/1807.02811},
	urldate = {2020-03-25},
	file = {[1807.02811] A Tutorial on Bayesian Optimization:C\:\\Users\\Vval\\Zotero\\storage\\V8DFZWPM\\1807.html:text/html}
}

@misc{noauthor_190512280_nodate,
	title = {[1905.12280] {Lifelong} {Bayesian} {Optimization}},
	url = {https://arxiv.org/abs/1905.12280},
	urldate = {2020-03-25},
	file = {[1905.12280] Lifelong Bayesian Optimization:C\:\\Users\\Vval\\Zotero\\storage\\U9ZDP4V4\\1905.html:text/html}
}

@article{adams_tutorial_nodate,
	title = {A {Tutorial} on! {Bayesian} {Optimization}! for {Machine} {Learning}},
	language = {en},
	author = {Adams, Ryan P},
	pages = {45},
	file = {Adams - A Tutorial on! Bayesian Optimization! for Machine .pdf:C\:\\Users\\Vval\\Zotero\\storage\\ZWCSEU5H\\Adams - A Tutorial on! Bayesian Optimization! for Machine .pdf:application/pdf}
}

@misc{noauthor_conceptual_nodate,
	title = {A {Conceptual} {Explanation} of {Bayesian} {Hyperparameter} {Optimization} for {Machine} {Learning}},
	url = {https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f},
	urldate = {2020-03-25},
	file = {A Conceptual Explanation of Bayesian Hyperparameter Optimization for Machine Learning:C\:\\Users\\Vval\\Zotero\\storage\\HLJWVWKG\\a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learni.html:text/html}
}

@misc{noauthor_scikit-optimize_nodate,
	title = {scikit-optimize: sequential model-based optimization in {Python} ‚Äî scikit-optimize 0.7.4 documentation},
	url = {https://scikit-optimize.github.io/stable/},
	urldate = {2020-03-25},
	file = {scikit-optimize\: sequential model-based optimization in Python ‚Äî scikit-optimize 0.7.4 documentation:C\:\\Users\\Vval\\Zotero\\storage\\7F4IAN2W\\stable.html:text/html}
}

@misc{brownlee_gentle_2017,
	title = {Gentle {Introduction} to the {Adam} {Optimization} {Algorithm} for {Deep} {Learning}},
	url = {https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/},
	abstract = {The choice of optimization algorithm for your deep learning model can mean the difference between good results in minutes, hours, and days. The Adam optimization algorithm is an extension to stochastic gradient descent that has recently seen broader adoption for deep learning applications in computer vision and natural language processing. In this post, you will ‚Ä¶},
	language = {en-US},
	urldate = {2020-04-06},
	journal = {Machine Learning Mastery},
	author = {Brownlee, Jason},
	month = jul,
	year = {2017},
	note = {Library Catalog: machinelearningmastery.com
Section: Deep Learning Performance},
	file = {Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\FLNBDQHF\\adam-optimization-algorithm-for-deep-learning.html:text/html}
}

@misc{bushaev_adam_2018,
	title = {Adam ‚Äî latest trends in deep learning optimization.},
	url = {https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c},
	abstract = {Adam is an adaptive learning rate optimization algorithm that‚Äôs been designed specifically for training deep neural networks. First‚Ä¶},
	language = {en},
	urldate = {2020-04-06},
	journal = {Medium},
	author = {Bushaev, Vitaly},
	month = oct,
	year = {2018},
	note = {Library Catalog: towardsdatascience.com},
	file = {Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\6RMG6MJI\\adam-latest-trends-in-deep-learning-optimization-6be9a291375c.html:text/html}
}

@article{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2020-04-06},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\HIDVUGR5\\Kingma et Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\AUQSC4VJ\\1412.html:text/html}
}

@incollection{huang_improving_2018,
	title = {Improving {Explorability} in {Variational} {Inference} with {Annealed} {Variational} {Objectives}},
	url = {http://papers.nips.cc/paper/8178-improving-explorability-in-variational-inference-with-annealed-variational-objectives.pdf},
	urldate = {2020-04-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Huang, Chin-Wei and Tan, Shawn and Lacoste, Alexandre and Courville, Aaron C},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {9701--9711},
	file = {NIPS Full Text PDF:C\:\\Users\\Vval\\Zotero\\storage\\4DKFJLV4\\Huang et al. - 2018 - Improving Explorability in Variational Inference w.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\X3XE6CKM\\8178-improving-explorability-in-variational-inference-with-annealed-variational-objectives.html:text/html}
}