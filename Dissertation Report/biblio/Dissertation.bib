
@book{noauthor_gansynth:_nodate,
	title = {{GANSynth}: Making music with {GANs}},
	url = {https://magenta.tensorflow.org/gansynth},
	shorttitle = {{GANSynth}},
	abstract = {In this post, we introduce {GANSynth}, a method for generating high-fidelity audio with Generative Adversarial Networks ({GANs}). Colab Notebook üéµAudio E...},
	urldate = {2019-03-12},
	langid = {english},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\3\\gansynth.html:text/html}
}

@book{noauthor_music_nodate,
	title = {Music Transformer: Generating Music with Long-Term Structure},
	url = {https://magenta.tensorflow.org/music-transformer},
	shorttitle = {Music Transformer},
	abstract = {Generating long pieces of music is a challenging problem, as music containsstructure at multiple timescales, from milisecond timings to motifs to phrases t...},
	urldate = {2019-03-12},
	langid = {english},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\5\\music-transformer.html:text/html}
}

@article{engel_gansynth:_2019,
	title = {{GANSYNTH}: {ADVERSARIAL} {NEURAL} {AUDIO} {SYNTHESIS}},
	pages = {17},
	author = {Engel, Jesse and Agrawal, Kumar Krishna and Chen, Shuo and Gulrajani, Ishaan and Donahue, Chris and Roberts, Adam},
	date = {2019},
	langid = {english},
	file = {Engel et al. - 2019 - GANSYNTH ADVERSARIAL NEURAL AUDIO SYNTHESIS.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\6\\Engel et al. - 2019 - GANSYNTH ADVERSARIAL NEURAL AUDIO SYNTHESIS.pdf:application/pdf}
}

@book{noauthor_musicvae:_nodate,
	title = {{MusicVAE}: Creating a palette for musical scores with machine learning.},
	url = {https://magenta.tensorflow.org/music-vae},
	shorttitle = {{MusicVAE}},
	abstract = {When a painter creates a work of art, she first blends and explores color options on an artist‚Äôs palette before applying them to the canvas. This process is ...},
	urldate = {2019-03-12},
	langid = {english},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\9\\music-vae.html:text/html}
}

@article{huang_music_2018,
	title = {Music Transformer},
	url = {http://arxiv.org/abs/1809.04281},
	abstract = {Music relies heavily on repetition to build structure and meaning. Self-reference occurs on multiple timescales, from motifs to phrases to reusing of entire sections of music, such as in pieces with {ABA} structure. The Transformer (Vaswani et al., 2017), a sequence model based on self-attention, has achieved compelling results in many generation tasks that require maintaining long-range coherence. This suggests that self-attention might also be well-suited to modeling music. In musical composition and performance, however, relative timing is critically important. Existing approaches for representing relative positional information in the Transformer modulate attention based on pairwise distance (Shaw et al., 2018). This is impractical for long sequences such as musical compositions since their memory complexity for intermediate relative information is quadratic in the sequence length. We propose an algorithm that reduces their intermediate memory requirement to linear in the sequence length. This enables us to demonstrate that a Transformer with our modified relative attention mechanism can generate minute-long compositions (thousands of steps, four times the length modeled in Oore et al., 2018) with compelling structure, generate continuations that coherently elaborate on a given motif, and in a seq2seq setup generate accompaniments conditioned on melodies. We evaluate the Transformer with our relative attention mechanism on two datasets, {JSB} Chorales and Piano-e-Competition, and obtain state-of-the-art results on the latter.},
	journaltitle = {{arXiv}:1809.04281 [cs, eess, stat]},
	author = {Huang, Cheng-Zhi Anna and Vaswani, Ashish and Uszkoreit, Jakob and Shazeer, Noam and Simon, Ian and Hawthorne, Curtis and Dai, Andrew M. and Hoffman, Matthew D. and Dinculescu, Monica and Eck, Douglas},
	urldate = {2019-03-12},
	date = {2018-09},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	file = {arXiv\:1809.04281 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\13\\Huang et al. - 2018 - Music Transformer.pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\15\\1809.html:text/html}
}

@book{noauthor_gan_2017,
	title = {{GAN} Deep Learning Architectures - review},
	url = {https://sigmoidal.io/beginners-review-of-gan-architectures/},
	abstract = {{GAN} Deep Learning Architectures overview aims to give a comprehensive introduction to general ideas behind Generative Adversarial Networks, show you the main architectures that would be good starting points and provide you with an armory of tricks that would significantly improve your results.},
	urldate = {2019-03-12},
	date = {2017-09},
	langid = {american},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\14\\beginners-review-of-gan-architectures.html:text/html}
}

@article{nayebi_gruv:_nodate,
	title = {{GRUV}: Algorithmic Music Generation using Recurrent Neural Networks},
	abstract = {We compare the performance of two different types of recurrent neural networks ({RNNs}) for the task of algorithmic music generation, with audio waveforms as input. In particular, we focus on {RNNs} that have a sophisticated gating mechanism, namely, the Long Short-Term Memory ({LSTM}) network and the recently introduced Gated Recurrent Unit ({GRU}). Our results indicate that the generated outputs of the {LSTM} network were signiÔ¨Åcantly more musically plausible than those of the {GRU}.},
	pages = {6},
	author = {Nayebi, Aran and Vitelli, Matt},
	langid = {english},
	file = {Nayebi et Vitelli - GRUV Algorithmic Music Generation using Recurrent.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\16\\Nayebi et Vitelli - GRUV Algorithmic Music Generation using Recurrent.pdf:application/pdf}
}

@book{noauthor_what_2018,
	title = {What the heck is time-series data (and why do I need a time-series database)?},
	url = {https://blog.timescale.com/what-the-heck-is-time-series-data-and-why-do-i-need-a-time-series-database-dcf3b1b18563/},
	abstract = {This article is a primer on time-series data and why you may not want to use a ‚Äúnormal‚Äù database to store it.},
	urldate = {2019-03-12},
	date = {2018-11},
	langid = {english},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\19\\what-the-heck-is-time-series-data-and-why-do-i-need-a-time-series-database-dcf3b1b18563.html:text/html}
}

@article{karras_progressive_2017,
	title = {Progressive Growing of {GANs} for Improved Quality, Stability, and Variation},
	url = {http://arxiv.org/abs/1710.10196},
	abstract = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., {CelebA} images at 1024ÀÜ2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised {CIFAR}10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating {GAN} results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the {CelebA} dataset.},
	journaltitle = {{arXiv}:1710.10196 [cs, stat]},
	author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
	urldate = {2019-03-12},
	date = {2017-10},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1710.10196 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\22\\Karras et al. - 2017 - Progressive Growing of GANs for Improved Quality, .pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\23\\1710.html:text/html}
}

@article{goodfellow_nips_2016,
	title = {{NIPS} 2016 Tutorial: Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1701.00160},
	shorttitle = {{NIPS} 2016 Tutorial},
	abstract = {This report summarizes the tutorial presented by the author at {NIPS} 2016 on generative adversarial networks ({GANs}). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how {GANs} compare to other generative models, (3) the details of how {GANs} work, (4) research frontiers in {GANs}, and (5) state-of-the-art image models that combine {GANs} with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
	journaltitle = {{arXiv}:1701.00160 [cs]},
	author = {Goodfellow, Ian},
	urldate = {2019-03-12},
	date = {2016-12},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1701.00160 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\29\\Goodfellow - 2016 - NIPS 2016 Tutorial Generative Adversarial Network.pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\30\\1701.html:text/html}
}

@article{goodfellow_generative_2016,
	title = {Generative Adversarial Networks ({GANs})},
	pages = {86},
	author = {Goodfellow, Ian},
	date = {2016},
	langid = {english},
	file = {Goodfellow - 2016 - Generative Adversarial Networks (GANs).pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\32\\Goodfellow - 2016 - Generative Adversarial Networks (GANs).pdf:application/pdf}
}

@article{doersch_tutorial_2016,
	title = {Tutorial on Variational Autoencoders},
	url = {http://arxiv.org/abs/1606.05908},
	abstract = {In just three years, Variational Autoencoders ({VAEs}) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. {VAEs} are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. {VAEs} have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, {CIFAR} images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind {VAEs}, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
	journaltitle = {{arXiv}:1606.05908 [cs, stat]},
	author = {Doersch, Carl},
	urldate = {2019-03-19},
	date = {2016-06},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1606.05908 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\37\\Doersch - 2016 - Tutorial on Variational Autoencoders.pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\38\\1606.html:text/html}
}

@article{oord_conditional_2016,
	title = {Conditional Image Generation with {PixelCNN} Decoders},
	url = {http://arxiv.org/abs/1606.05328},
	abstract = {This work explores conditional image generation with a new image density model based on the {PixelCNN} architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the {ImageNet} database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional {PixelCNN} can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of {PixelCNN} to match the state-of-the-art performance of {PixelRNN} on {ImageNet}, with greatly reduced computational cost.},
	journaltitle = {{arXiv}:1606.05328 [cs]},
	author = {Oord, Aaron van den and Kalchbrenner, Nal and Vinyals, Oriol and Espeholt, Lasse and Graves, Alex and Kavukcuoglu, Koray},
	urldate = {2019-03-19},
	date = {2016-06},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1606.05328 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\40\\Oord et al. - 2016 - Conditional Image Generation with PixelCNN Decoder.pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\41\\1606.html:text/html}
}

@article{yang_convolutional_2018,
	title = {Convolutional Self-Attention Network},
	url = {http://arxiv.org/abs/1810.13320},
	abstract = {Self-attention network ({SAN}) has recently attracted increasing interest due to its fully parallelized computation and flexibility in modeling dependencies. It can be further enhanced with multi-headed attention mechanism by allowing the model to jointly attend to information from different representation subspaces at different positions (Vaswani et al., 2017). In this work, we propose a novel convolutional self-attention network ({CSAN}), which offers {SAN} the abilities to 1) capture neighboring dependencies, and 2) model the interaction between multiple attention heads. Experimental results on {WMT}14 English-to-German translation task demonstrate that the proposed approach outperforms both the strong Transformer baseline and other existing works on enhancing the locality of {SAN}. Comparing with previous work, our model does not introduce any new parameters.},
	journaltitle = {{arXiv}:1810.13320 [cs]},
	author = {Yang, Baosong and Wang, Longyue and Wong, Derek F. and Chao, Lidia S. and Tu, Zhaopeng},
	urldate = {2019-03-19},
	date = {2018-10},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1810.13320 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\44\\Yang et al. - 2018 - Convolutional Self-Attention Network.pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\45\\1810.html:text/html}
}

@book{noauthor_self-attention_nodate,
	title = {Self-Attention Mechanisms in Natural Language Processing},
	url = {https://www.alibabacloud.com/blog/self-attention-mechanisms-in-natural-language-processing_593968},
	abstract = {This article explores various forms of Attention Mechanisms in Natural Language Processing ({NLP}) and their applications in multiple areas such as machine translation tasks.},
	urldate = {2019-03-19},
	langid = {english},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\47\\self-attention-mechanisms-in-natural-language-processing_593968.html:text/html}
}

@book{jay_understanding_2018,
	title = {Understanding and Implementing Architectures of {ResNet} and {ResNeXt} for state-of-the-art Image‚Ä¶},
	url = {https://medium.com/@14prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-cf51669e1624},
	abstract = {In this two part blog post we will explore Residual networks. More specifically we will discuss three papers released by Microsoft Research‚Ä¶},
	author = {Jay, Prakash},
	urldate = {2019-03-20},
	date = {2018-02},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\50\\understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-c.html:text/html}
}

@incollection{vaswani_attention_2017,
	title = {Attention is All you Need},
	url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf},
	pages = {5998--6008},
	booktitle = {Advances in Neural Information Processing Systems 30},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, ≈Åukasz and Polosukhin, Illia},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	urldate = {2019-03-20},
	date = {2017},
	file = {NIPS Full Text PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\52\\Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf;NIPS Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\53\\7181-attention-is-all-you-need.html:text/html}
}

@book{noauthor_attention_nodate,
	title = {Attention in Neural Networks and How to Use It},
	url = {http://akosiorek.github.io/ml/2017/10/14/visual-attention.html},
	urldate = {2019-03-20},
	file = {Attention in Neural Networks and How to Use It:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\56\\visual-attention.html:text/html}
}

@book{noauthor_attention?_nodate,
	title = {Attention? Attention!},
	url = {https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html},
	urldate = {2019-03-20},
	file = {Attention? Attention!:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\58\\attention-attention.html:text/html}
}

@book{synced_brief_2017,
	title = {A Brief Overview of Attention Mechanism},
	url = {https://medium.com/syncedreview/a-brief-overview-of-attention-mechanism-13c578ba9129},
	abstract = {What is Attention?},
	author = {{Synced}},
	urldate = {2019-03-20},
	date = {2017-09},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\60\\a-brief-overview-of-attention-mechanism-13c578ba9129.html:text/html}
}

@book{britz_attention_2016,
	title = {Attention and Memory in Deep Learning and {NLP}},
	url = {http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/},
	abstract = {A recent trend in Deep Learning are Attention Mechanisms. In an interview, Ilya Sutskever, now the research director of {OpenAI}, mentioned that Attention Mechanisms are one of the most exciting adva‚Ä¶},
	author = {Britz, Denny},
	urldate = {2019-03-21},
	date = {2016-01},
	langid = {american},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\63\\attention-and-memory-in-deep-learning-and-nlp.html:text/html}
}

@article{huang_visualizing_nodate,
	title = {Visualizing Music Self-Attention},
	abstract = {Like language, music can be represented as a sequence of discrete symbols that form a hierarchical syntax, with notes being roughly like characters and motifs of notes like words. Unlike text however, music relies heavily on repetition on multiple timescales to build structure and meaning. The Music Transformer has shown compelling results in generating music with structure [3]. In this paper, we introduce a tool for visualizing self-attention on polyphonic music with an interactive pianoroll. We use music transformer as both a descriptive tool and a generative model. For the former, we use it to analyze existing music to see if the resulting self-attention structure corroborates with the musical structure known from music theory. For the latter, we inspect the model‚Äôs self-attention during generation, in order to understand how past notes affect future ones. We also compare and contrast the attention structure of regular attention to that of relative attention [6, 3], and examine its impact on the resulting generated music. For example, for the {JSB} Chorales dataset, a model trained with relative attention is more consistent in attending to all the voices in the preceding timestep and the chords before, and at cadences to the beginning of a phrase, allowing it to create an arc. We hope that our analyses will offer more evidence for relative self-attention as a powerful inductive bias for modeling music. We invite the reader to view our video animations of music attention and to interact with the visualizations at https:// storage.googleapis.com/nips-workshop-visualization/index.html.},
	pages = {5},
	author = {Huang, Anna and Dinculescu, Monica and Eck, Douglas and Vaswani, Ashish},
	langid = {english},
	file = {Huang et al. - Visualizing Music Self-Attention.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\64\\Huang et al. - Visualizing Music Self-Attention.pdf:application/pdf}
}

@article{devlin_bert:_2018,
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {http://arxiv.org/abs/1810.04805},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, {BERT} is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. {BERT} is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the {GLUE} benchmark to 80.4\% (7.6\% absolute improvement), {MultiNLI} accuracy to 86.7 (5.6\% absolute improvement) and the {SQuAD} v1.1 question answering Test F1 to 93.2 (1.5\% absolute improvement), outperforming human performance by 2.0\%.},
	journaltitle = {{arXiv}:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	urldate = {2019-03-22},
	date = {2018-10},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1810.04805 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\72\\Devlin et al. - 2018 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\73\\1810.html:text/html}
}

@article{huang_densely_2016,
	title = {Densely Connected Convolutional Networks},
	url = {http://arxiv.org/abs/1608.06993},
	abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network ({DenseNet}), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. {DenseNets} have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks ({CIFAR}-10, {CIFAR}-100, {SVHN}, and {ImageNet}). {DenseNets} obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/{DenseNet} .},
	journaltitle = {{arXiv}:1608.06993 [cs]},
	author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
	urldate = {2019-03-24},
	date = {2016-08},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1608.06993 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\77\\Huang et al. - 2016 - Densely Connected Convolutional Networks.pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\78\\1608.html:text/html}
}

@book{chablani_densenet_2017,
	title = {{DenseNet}},
	url = {https://towardsdatascience.com/densenet-2810936aeebb},
	abstract = {Many papers:},
	author = {Chablani, Manish},
	urldate = {2019-03-24},
	date = {2017-08},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\80\\densenet-2810936aeebb.html:text/html}
}

@book{noauthor_wavenet:_nodate,
	title = {{WaveNet}: A Generative Model for Raw Audio},
	url = {https://deepmind.com/blog/wavenet-generative-model-raw-audio/},
	shorttitle = {{WaveNet}},
	abstract = {This post presents {WaveNet}, a deep generative model of raw audio waveforms. We show that {WaveNets} are able to generate speech which mimics any human voice and which sounds more natural than the best existing Text-to-Speech systems, reducing the gap with human performance by over 50\%. We also demonstrate that the same network can be used to synthesize other audio signals such as music, and present some striking samples of automatically generated piano pieces.},
	urldate = {2019-03-28},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\83\\wavenet-generative-model-raw-audio.html:text/html}
}

@article{oord_wavenet:_2016,
	title = {{WaveNet}: A Generative Model for Raw Audio},
	url = {http://arxiv.org/abs/1609.03499},
	shorttitle = {{WaveNet}},
	abstract = {This paper introduces {WaveNet}, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single {WaveNet} can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	journaltitle = {{arXiv}:1609.03499 [cs]},
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	urldate = {2019-03-28},
	date = {2016-09},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
	file = {arXiv\:1609.03499 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\85\\Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\86\\1609.html:text/html}
}

@article{heusel_gans_2017,
	title = {{GANs} Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},
	url = {http://arxiv.org/abs/1706.08500},
	abstract = {Generative Adversarial Networks ({GANs}) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of {GAN} training has still not been proved. We propose a two time-scale update rule ({TTUR}) for training {GANs} with stochastic gradient descent on arbitrary {GAN} loss functions. {TTUR} has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the {TTUR} converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of {GANs} at image generation, we introduce the "Fr{\textbackslash}textbackslash'echet Inception Distance" ({FID}) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, {TTUR} improves learning for {DCGANs} and Improved Wasserstein {GANs} ({WGAN}-{GP}) outperforming conventional {GAN} training on {CelebA}, {CIFAR}-10, {SVHN}, {LSUN} Bedrooms, and the One Billion Word Benchmark.},
	journaltitle = {{arXiv}:1706.08500 [cs, stat]},
	author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
	urldate = {2019-03-28},
	date = {2017-06},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1706.08500 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\89\\Heusel et al. - 2017 - GANs Trained by a Two Time-Scale Update Rule Conve.pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\90\\1706.html:text/html}
}

@inproceedings{kikuchi_automatic_2014,
	location = {Porto, Portugal},
	title = {Automatic melody generation considering chord progression by genetic algorithm},
	isbn = {978-1-4799-5937-2 978-1-4799-5936-5},
	url = {http://ieeexplore.ieee.org/document/6921876/},
	doi = {10.1109/NaBIC.2014.6921876},
	abstract = {In this research, an automatic melody generation system considering chord progression by genetic algorithm is proposed. In the proposed automatic melody generation system, initial population are generated based on features on rhythm, pitch and chord progression of trained melody. In this system, the trained sample melody is divided into some melody blocks. Here, melody blocks mean verse, bridge, chorus and so on. And some new melodies are generated considering melody features in each block. The features on rhythm and pitch in each melody block of the sample melody are trained in some N-gram models, and they are used in order to calculate fitness in the melody generation by genetic algorithm. Some melodies are generated using the proposed system and confirmed that the proposed system can generate melodies considering features in each melody block of the trained sample melody.},
	pages = {190--195},
	booktitle = {2014 Sixth World Congress on Nature and Biologically Inspired Computing ({NaBIC} 2014)},
	publisher = {{IEEE}},
	author = {Kikuchi, Motoki and Osana, Yuko},
	urldate = {2019-03-28},
	date = {2014-07},
	langid = {english},
	file = {Kikuchi et Osana - 2014 - Automatic melody generation considering chord prog.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\92\\Kikuchi et Osana - 2014 - Automatic melody generation considering chord prog.pdf:application/pdf}
}

@book{noauthor_papers_nodate,
	title = {Papers With Code : End-to-end music source separation: is it possible in the waveform domain?},
	url = {http://paperswithcode.com/paper/end-to-end-music-source-separation-is-it},
	shorttitle = {Papers With Code},
	abstract = {Implemented in one code library. Click to access code and evaluation tables.},
	urldate = {2019-03-29},
	langid = {english},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\96\\end-to-end-music-source-separation-is-it.html:text/html}
}

@article{lluis_end--end_2018,
	title = {End-to-end music source separation: is it possible in the waveform domain?},
	url = {http://arxiv.org/abs/1810.12187},
	shorttitle = {End-to-end music source separation},
	abstract = {Most of the currently successful source separation techniques use the magnitude spectrogram as input, and are therefore by default omitting part of the signal: the phase. In order to avoid omitting potentially useful information, we study the viability of using end-to-end models for music source separation. By operating directly over the waveform, these models take into account all the information available in the raw audio signal, including the phase. Our results show that waveform-based models can outperform a recent spectrogram-based deep learning model. Namely, a novel Wavenet-based model we propose and Wave-U-Net can outperform {DeepConvSep}, a spectrogram-based deep learning model. This suggests that end-to-end learning has a great potential for the problem of music source separation.},
	journaltitle = {{arXiv}:1810.12187 [cs, eess]},
	author = {Llu√≠s, Francesc and Pons, Jordi and Serra, Xavier},
	urldate = {2019-03-29},
	date = {2018-10},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv\:1810.12187 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\99\\Llu√≠s et al. - 2018 - End-to-end music source separation is it possible.pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\100\\1810.html:text/html}
}

@article{stoller_adversarial_2017,
	title = {Adversarial Semi-Supervised Audio Source Separation applied to Singing Voice Extraction},
	url = {http://arxiv.org/abs/1711.00048},
	abstract = {The state of the art in music source separation employs neural networks trained in a supervised fashion on multi-track databases to estimate the sources from a given mixture. With only few datasets available, often extensive data augmentation is used to combat overfitting. Mixing random tracks, however, can even reduce separation performance as instruments in real music are strongly correlated. The key concept in our approach is that source estimates of an optimal separator should be indistinguishable from real source signals. Based on this idea, we drive the separator towards outputs deemed as realistic by discriminator networks that are trained to tell apart real from separator samples. This way, we can also use unpaired source and mixture recordings without the drawbacks of creating unrealistic music mixtures. Our framework is widely applicable as it does not assume a specific network architecture or number of sources. To our knowledge, this is the first adoption of adversarial training for music source separation. In a prototype experiment for singing voice separation, separation performance increases with our approach compared to purely supervised training.},
	journaltitle = {{arXiv}:1711.00048 [cs]},
	author = {Stoller, Daniel and Ewert, Sebastian and Dixon, Simon},
	urldate = {2019-03-29},
	date = {2017-10},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, H.5.5, I.2.6},
	file = {arXiv\:1711.00048 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\103\\Stoller et al. - 2017 - Adversarial Semi-Supervised Audio Source Separatio.pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\104\\1711.html:text/html}
}

@article{michelashvili_semi-supervised_2018,
	title = {Semi-Supervised Monaural Singing Voice Separation With a Masking Network Trained on Synthetic Mixtures},
	url = {http://arxiv.org/abs/1812.06087},
	abstract = {We study the problem of semi-supervised singing voice separation, in which the training data contains a set of samples of mixed music (singing and instrumental) and an unmatched set of instrumental music. Our solution employs a single mapping function g, which, applied to a mixed sample, recovers the underlying instrumental music, and, applied to an instrumental sample, returns the same sample. The network g is trained using purely instrumental samples, as well as on synthetic mixed samples that are created by mixing reconstructed singing voices with random instrumental samples. Our results indicate that we are on a par with or better than fully supervised methods, which are also provided with training samples of unmixed singing voices, and are better than other recent semi-supervised methods.},
	journaltitle = {{arXiv}:1812.06087 [cs, eess, stat]},
	author = {Michelashvili, Michael and Benaim, Sagie and Wolf, Lior},
	urldate = {2019-03-29},
	date = {2018-12},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	file = {arXiv\:1812.06087 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\106\\Michelashvili et al. - 2018 - Semi-Supervised Monaural Singing Voice Separation .pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\107\\1812.html:text/html}
}

@article{briot_deep_2017,
	title = {Deep Learning Techniques for Music Generation - A Survey},
	url = {http://arxiv.org/abs/1709.01620},
	abstract = {This paper is a survey and an analysis of different ways of using deep learning to generate musical content. We propose a methodology based on five dimensions: Objective - What musical content is to be generated? Examples are: melody, polyphony, accompaniment or counterpoint. - For what destination and for what use? To be performed by a human(s) (in the case of a musical score), or by a machine (in the case of an audio file). Representation - What are the concepts to be manipulated? Examples are: waveform, spectrogram, note, chord, meter and beat. - What format is to be used? Examples are: {MIDI}, piano roll or text. - How will the representation be encoded? Examples are: scalar, one-hot or many-hot. Architecture - What type(s) of deep neural network is (are) to be used? Examples are: feedforward network, recurrent network, autoencoder or generative adversarial networks. Challenges - What are the limitations and open challenges? Examples are: variability, interactivity and creativity. Strategy - How do we model and control the process of generation? Examples are: single-step feedforward, iterative feedforward, sampling or input manipulation. For each dimension, we conduct a comparative analysis of various models and techniques and propose some tentative multidimensional typology which is bottom-up, based on the analysis of many existing deep-learning based systems for music generation selected from the relevant literature. These systems are described and used to exemplify the various choices of objective, representation, architecture, challenges and strategies. The last part of the paper includes some discussion and some prospects. This is a simplified version (weak {DRM}) of the book: Briot, J.-P., Hadjeres, G. and Pachet, F.-D. (2019) Deep Learning Techniques for Music Generation, Computational Synthesis and Creative Systems, Springer.},
	journaltitle = {{arXiv}:1709.01620 [cs]},
	author = {Briot, Jean-Pierre and Hadjeres, Ga√´tan and Pachet, Fran√ßois-David},
	urldate = {2019-04-02},
	date = {2017-09},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound},
	file = {arXiv\:1709.01620 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\111\\Briot et al. - 2017 - Deep Learning Techniques for Music Generation - A .pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\112\\1709.html:text/html}
}

@article{chu_song_2017,
	title = {{SONG} {FROM} {PI}: A {MUSICALLY} {PLAUSIBLE} {NETWORK} {FOR} {POP} {MUSIC} {GENERATION}},
	abstract = {We present a novel framework for generating pop music. Our model is a hierarchical Recurrent Neural Network, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed. In particular, the bottom layers generate the melody, while the higher levels produce the drums and chords. We conduct several human studies that show strong preference of our generated music over that produced by the recent method by Google. We additionally show two applications of our framework: neural dancing and karaoke, as well as neural story singing.},
	pages = {9},
	author = {Chu, Hang and Urtasun, Raquel and Fidler, Sanja},
	date = {2017},
	langid = {english},
	file = {Chu et al. - 2017 - SONG FROM PI A MUSICALLY PLAUSIBLE NETWORK FOR PO.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\113\\Chu et al. - 2017 - SONG FROM PI A MUSICALLY PLAUSIBLE NETWORK FOR PO.pdf:application/pdf}
}

@article{wiggins_computer_2007,
	title = {Computer Models of Musical Creativity: A Review of Computer Models of Musical Creativity by David Cope},
	volume = {23},
	issn = {0268-1145, 1477-4615},
	url = {https://academic.oup.com/dsh/article-lookup/doi/10.1093/llc/fqm025},
	doi = {10.1093/llc/fqm025},
	shorttitle = {Computer Models of Musical Creativity},
	pages = {109--116},
	number = {1},
	journaltitle = {Literary and Linguistic Computing},
	author = {Wiggins, G. A.},
	urldate = {2019-04-02},
	date = {2007-12},
	langid = {english},
	file = {Wiggins - 2007 - Computer Models of Musical Creativity A Review of.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\115\\Wiggins - 2007 - Computer Models of Musical Creativity A Review of.pdf:application/pdf}
}

@book{tedx_talks_tedxgeorgiatech_nodate,
	title = {{TEDxGeorgiaTech} - Gil Weinberg - Towards Robotic Musicianship},
	url = {https://www.youtube.com/watch?v=9Lw7ax5a7lU},
	author = {{TEDx Talks}},
	urldate = {2019-04-05}
}

@book{tedx_talks_tedxgeorgiatech_nodate-1,
	title = {{TEDxGeorgiaTech} - Gil Weinberg - Towards Robotic Musicianship},
	url = {https://www.youtube.com/watch?v=v5eUo2R_Lrc},
	author = {{TEDx Talks}},
	urldate = {2019-04-05}
}

@article{sahoo_learning_2018,
	title = {Learning Equations for Extrapolation and Control},
	url = {http://arxiv.org/abs/1806.07259},
	abstract = {We present an approach to identify concise equations from data using a shallow neural network approach. In contrast to ordinary black-box regression, this approach allows understanding functional relations and generalizing them from observed data to unseen parts of the parameter space. We show how to extend the class of learnable equations for a recently proposed equation learning network to include divisions, and we improve the learning and model selection strategy to be useful for challenging real-world data. For systems governed by analytical expressions, our method can in many cases identify the true underlying equation and extrapolate to unseen domains. We demonstrate its effectiveness by experiments on a cart-pendulum system, where only 2 random rollouts are required to learn the forward dynamics and successfully achieve the swing-up task.},
	journaltitle = {{arXiv}:1806.07259 [cs, stat]},
	author = {Sahoo, Subham S. and Lampert, Christoph H. and Martius, Georg},
	urldate = {2019-04-11},
	date = {2018-06},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, I.2.6, 62J02, 62M20, 65D15, 68T05, 68T30, 68T40, 70E60, 93C40, I.2.8},
	file = {arXiv\:1806.07259 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\126\\Sahoo et al. - 2018 - Learning Equations for Extrapolation and Control.pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\127\\1806.html:text/html}
}

@book{weinberg_survey_nodate,
	title = {A Survey of Robotic Musicianship},
	url = {https://cacm.acm.org/magazines/2016/5/201594-a-survey-of-robotic-musicianship/fulltext},
	abstract = {Reviewing the technologies that enable robot musicians to jam.},
	author = {Weinberg, Gil, Mason Bretan},
	urldate = {2019-04-11},
	langid = {english},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\131\\fulltext.html:text/html}
}

@article{nikolaidis_generative_2012,
	title = {Generative Musical Tension Modeling and Its Application to Dynamic Sonification},
	volume = {36},
	issn = {0148-9267, 1531-5169},
	url = {http://www.mitpressjournals.org/doi/10.1162/COMJ_a_00105},
	doi = {10.1162/COMJ_a_00105},
	pages = {55--64},
	number = {1},
	journaltitle = {Computer Music Journal},
	author = {Nikolaidis, Ryan and Walker, Bruce and Weinberg, Gil},
	urldate = {2019-04-11},
	date = {2012-03},
	langid = {english},
	file = {Nikolaidis et al. - 2012 - Generative Musical Tension Modeling and Its Applic.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\133\\Nikolaidis et al. - 2012 - Generative Musical Tension Modeling and Its Applic.pdf:application/pdf}
}

@inproceedings{hoffman_shimon:_2010,
	location = {Atlanta, Georgia, {USA}},
	title = {Shimon: an interactive improvisational robotic marimba player},
	isbn = {978-1-60558-930-5},
	url = {http://portal.acm.org/citation.cfm?doid=1753846.1753925},
	doi = {10.1145/1753846.1753925},
	shorttitle = {Shimon},
	abstract = {Shimon is an autonomous marimba-playing robot designed to create interactions with human players that lead to novel musical outcomes. The robot combines music perception, interaction, and improvisation with the capacity to produce melodic and harmonic acoustic responses through choreographic gestures. We developed an anticipatory action framework, and a gesture-based behavior system, allowing the robot to play improvised Jazz with humans in synchrony, fluently, and without delay. In addition, we built an expressive non-humanoid head for musical social communication. This paper describes our system, used in a performance and demonstration at the {CHI} 2010 Media Showcase.},
	pages = {3097},
	booktitle = {Proceedings of the 28th of the international conference extended abstracts on Human factors in computing systems - {CHI} {EA} '10},
	publisher = {{ACM} Press},
	author = {Hoffman, Guy and Weinberg, Gil},
	urldate = {2019-04-12},
	date = {2010},
	langid = {english},
	file = {Hoffman et Weinberg - 2010 - Shimon an interactive improvisational robotic mar.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\138\\Hoffman et Weinberg - 2010 - Shimon an interactive improvisational robotic mar.pdf:application/pdf}
}

@article{fernandez_ai_2013,
	title = {{AI} Methods in Algorithmic Composition: A Comprehensive Survey},
	volume = {48},
	issn = {1076-9757},
	url = {https://jair.org/index.php/jair/article/view/10845},
	doi = {10.1613/jair.3908},
	shorttitle = {{AI} Methods in Algorithmic Composition},
	abstract = {Algorithmic composition is the partial or total automation of the process of music composition by using computers. Since the 1950s, diÔ¨Äerent computational techniques related to ArtiÔ¨Åcial Intelligence have been used for algorithmic composition, including grammatical representations, probabilistic methods, neural networks, symbolic rule-based systems, constraint programming and evolutionary algorithms. This survey aims to be a comprehensive account of research on algorithmic composition, presenting a thorough view of the Ô¨Åeld for researchers in ArtiÔ¨Åcial Intelligence.},
	pages = {513--582},
	journaltitle = {Journal of Artificial Intelligence Research},
	author = {Fernandez, J.D. and Vico, F.},
	urldate = {2019-04-13},
	date = {2013-11},
	langid = {english},
	file = {Fernandez et Vico - 2013 - AI Methods in Algorithmic Composition A Comprehen.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\142\\Fernandez et Vico - 2013 - AI Methods in Algorithmic Composition A Comprehen.pdf:application/pdf}
}

@book{noauthor_automatic_nodate,
	title = {Automatic Melody Generation considering Chord Progression by Genetic Algorithm},
	file = {Automatic Melody Generation considering Chord Prog.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\145\\Automatic Melody Generation considering Chord Prog.pdf:application/pdf}
}

@article{wakui_automatic_nodate,
	title = {Automatic Melody Generation considering Chord Progression using Genetic Algorithm},
	abstract = {In this paper, we propose an automatic melody generation system considering chord progression. In the proposed system, chord progression and rhythm sequence are generated randomly, and the pitch is assigned to each note using genetic algorithm. We carried out a series of computer experiments, and we conÔ¨Årmed that melodies can be generated by the proposed system.},
	pages = {4},
	author = {Wakui, Yudai and Hatori, Yoshinori and Osana, Yuko},
	langid = {english},
	file = {Wakui et al. - Automatic Melody Generation considering Chord Prog.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\146\\Wakui et al. - Automatic Melody Generation considering Chord Prog.pdf:application/pdf}
}

@book{noauthor_cs4347_nodate,
	title = {{CS}4347 Acoustic Scene Classifier Report},
	url = {https://fr.overleaf.com/project/5cb81a5a70921e1466432a94},
	abstract = {Un √©diteur {LaTeX} en ligne facile √† utiliser. Pas d'installation, collaboration en temps r√©el, gestion des versions, des centaines de mod√®les de documents {LaTeX}, et plus encore.},
	urldate = {2019-04-20},
	langid = {french},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\151\\5cb81a5a70921e1466432a94.html:text/html}
}

@book{noauthor_acm_nodate,
	title = {{ACM} Master Article Template},
	url = {https://www.acm.org/publications/proceedings-template},
	abstract = {{ACM} Master Article Template},
	urldate = {2019-04-20},
	langid = {english},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\155\\proceedings-template.html:text/html}
}

@article{surname_insert_nodate,
	title = {Insert Your Title Here},
	abstract = {In this sample-structured document, neither the cross-linking of float elements and bibliography nor metadata/copyright information is available. The sample document is provided in ‚ÄúDraft‚Äù mode and to view it in the final layout format, applying the required template is essential with some standard steps.},
	pages = {2},
	author = {Surname, {FirstName} and Surname, {FirstName} and Surname, {FirstName}},
	langid = {english},
	file = {Surname et al. - Insert Your Title Here.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\156\\Surname et al. - Insert Your Title Here.pdf:application/pdf}
}

@book{noauthor_dcase2018_nodate,
	title = {{DCASE}2018 Challenge - {DCASE}},
	url = {http://dcase.community/challenge2018/index},
	urldate = {2019-04-20},
	file = {DCASE2018 Challenge - DCASE:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\159\\index.html:text/html}
}

@article{eronen_audio-based_2006,
	title = {Audio-based context recognition},
	volume = {14},
	issn = {1558-7916},
	url = {http://ieeexplore.ieee.org/document/1561288/},
	doi = {10.1109/TSA.2005.854103},
	abstract = {The aim of this paper is to investigate the feasibility of an audio-based context recognition system. Here, context recognition refers to the automatic classiÔ¨Åcation of the context or an environment around a device. A system is developed and compared to the accuracy of human listeners in the same task. Particular emphasis is placed on the computational complexity of the methods, since the application is of particular interest in resource-constrained portable devices. Simplistic low-dimensional feature vectors are evaluated against more standard spectral features. Using discriminative training, competitive recognition accuracies are achieved with very low-order hidden Markov models (1‚Äì3 Gaussian components). Slight improvement in recognition accuracy is observed when linear data-driven feature transformations are applied to mel-cepstral features. The recognition rate of the system as a function of the test sequence length appears to converge only after about 30 to 60 s. Some degree of accuracy can be achieved even with less than 1-s test sequence lengths. The average reaction time of the human listeners was 14 s, i.e., somewhat smaller, but of the same order as that of the system. The average recognition accuracy of the system was 58\% against 69\%, obtained in the listening tests in recognizing between 24 everyday contexts. The accuracies in recognizing six high-level classes were 82\% for the system and 88\% for the subjects.},
	pages = {321--329},
	number = {1},
	journaltitle = {{IEEE} Transactions on Audio, Speech and Language Processing},
	author = {Eronen, A.J. and Peltonen, V.T. and Tuomi, J.T. and Klapuri, A.P. and Fagerlund, S. and Sorsa, T. and Lorho, G. and Huopaniemi, J.},
	urldate = {2019-04-20},
	date = {2006-01},
	langid = {english},
	file = {Eronen et al. - 2006 - Audio-based context recognition.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\160\\Eronen et al. - 2006 - Audio-based context recognition.pdf:application/pdf}
}

@book{noauthor_[1411.3715]_nodate,
	title = {[1411.3715] Acoustic Scene Classification},
	url = {https://arxiv.org/abs/1411.3715},
	urldate = {2019-04-20},
	file = {[1411.3715] Acoustic Scene Classification:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\163\\1411.html:text/html}
}

@article{mesaros_detection_2018,
	title = {Detection and Classification of Acoustic Scenes and Events: Outcome of the {DCASE} 2016 Challenge},
	volume = {26},
	issn = {2329-9290},
	doi = {10.1109/TASLP.2017.2778423},
	shorttitle = {Detection and Classification of Acoustic Scenes and Events},
	abstract = {Public evaluation campaigns and datasets promote active development in target research areas, allowing direct comparison of algorithms. The second edition of the challenge on detection and classification of acoustic scenes and events ({DCASE} 2016) has offered such an opportunity for development of the state-of-the-art methods, and succeeded in drawing together a large number of participants from academic and industrial backgrounds. In this paper, we report on the tasks and outcomes of the {DCASE} 2016 challenge. The challenge comprised four tasks: acoustic scene classification, sound event detection in synthetic audio, sound event detection in real-life audio, and domestic audio tagging. We present each task in detail and analyze the submitted systems in terms of design and performance. We observe the emergence of deep learning as the most popular classification method, replacing the traditional approaches based on Gaussian mixture models and support vector machines. By contrast, feature representations have not changed substantially throughout the years, as mel frequency-based representations predominate in all tasks. The datasets created for and used in {DCASE} 2016 are publicly available and are a valuable resource for further research.},
	pages = {379--393},
	number = {2},
	journaltitle = {{IEEE}/{ACM} Transactions on Audio, Speech, and Language Processing},
	author = {Mesaros, A. and Heittola, T. and Benetos, E. and Foster, P. and Lagrange, M. and Virtanen, T. and Plumbley, M. D.},
	date = {2018-02},
	keywords = {acoustic scene classification, Acoustic scene classification, acoustic scenes, acoustic signal processing, Acoustics, audio datasets, audio signal processing, classification method, {DCASE} 2016 challenge, deep learning, domestic audio tagging, Event detection, feature extraction, Gaussian mixture models, Hidden Markov models, learning (artificial intelligence), pattern recognition, public evaluation campaigns, signal classification, sound event detection, Speech, Speech processing, support vector machines, synthetic audio, Tagging},
	file = {IEEE Xplore Abstract Record:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\165\\8123864.html:text/html;Version soumise:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\167\\Mesaros et al. - 2018 - Detection and Classification of Acoustic Scenes an.pdf:application/pdf}
}

@article{lagrange_bag--frames_2015,
	title = {The bag-of-frames approach: A not so sufficient model for urban soundscapes},
	volume = {138},
	issn = {0001-4966},
	url = {https://asa.scitation.org/doi/10.1121/1.4935350},
	doi = {10.1121/1.4935350},
	shorttitle = {The bag-of-frames approach},
	abstract = {The ‚Äúbag-of-frames‚Äù ({BOF}) approach, which encodes audio signals as the long-term statistical distribution of short-term spectral features, is commonly regarded as an effective and sufficient way to represent environmental sound recordings (soundscapes). The present paper describes a conceptual replication of a use of the {BOF} approach in a seminal article using several other soundscape datasets, with results strongly questioning the adequacy of the {BOF} approach for the task. As demonstrated in this paper, the good accuracy originally reported with {BOF} likely resulted from a particularly permissive dataset with low within-class variability. Soundscape modeling, therefore, may not be the closed case it was once thought to be.},
	pages = {EL487--EL492},
	number = {5},
	journaltitle = {The Journal of the Acoustical Society of America},
	author = {Lagrange, Mathieu and Lafay, Gr√©goire and D√©fr√©ville, Boris and Aucouturier, Jean-Julien},
	urldate = {2019-04-20},
	date = {2015-11},
	file = {Full Text PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\168\\Lagrange et al. - 2015 - The bag-of-frames approach A not so sufficient mo.pdf:application/pdf;Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\169\\1.html:text/html}
}

@book{ruiz_understanding_2018,
	title = {Understanding and visualizing {DenseNets}},
	url = {https://towardsdatascience.com/understanding-and-visualizing-densenets-7f688092391a},
	abstract = {This post be found in {PDF} here.},
	author = {Ruiz, Pablo Ruiz},
	urldate = {2019-04-20},
	date = {2018-10},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\171\\understanding-and-visualizing-densenets-7f688092391a.html:text/html}
}

@book{tsang_review:_2018,
	title = {Review: {DenseNet} ‚Äî Dense Convolutional Network (Image Classification)},
	url = {https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803},
	shorttitle = {Review},
	abstract = {In this story, {DenseNet} (Dense Convolutional Network) is reviewed. This is the paper in 2017 {CVPR} which got Best Paper Award with over 2000‚Ä¶},
	author = {Tsang, Sik-Ho},
	urldate = {2019-04-20},
	date = {2018-11},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\173\\review-densenet-image-classification-b6631a8ef803.html:text/html}
}

@article{jegou_one_2016,
	title = {The One Hundred Layers Tiramisu: Fully Convolutional {DenseNets} for Semantic Segmentation},
	url = {http://arxiv.org/abs/1611.09326},
	shorttitle = {The One Hundred Layers Tiramisu},
	abstract = {State-of-the-art approaches for semantic image segmentation are built on Convolutional Neural Networks ({CNNs}). The typical segmentation architecture is composed of (a) a downsampling path responsible for extracting coarse semantic features, followed by (b) an upsampling path trained to recover the input image resolution at the output of the model and, optionally, (c) a post-processing module (e.g. Conditional Random Fields) to refine the model predictions. Recently, a new {CNN} architecture, Densely Connected Convolutional Networks ({DenseNets}), has shown excellent results on image classification tasks. The idea of {DenseNets} is based on the observation that if each layer is directly connected to every other layer in a feed-forward fashion then the network will be more accurate and easier to train. In this paper, we extend {DenseNets} to deal with the problem of semantic segmentation. We achieve state-of-the-art results on urban scene benchmark datasets such as {CamVid} and Gatech, without any further post-processing module nor pretraining. Moreover, due to smart construction of the model, our approach has much less parameters than currently published best entries for these datasets. Code to reproduce the experiments is available here : https://github.com/{SimJeg}/{FC}-{DenseNet}/blob/master/train.py},
	journaltitle = {{arXiv}:1611.09326 [cs]},
	author = {J√©gou, Simon and Drozdzal, Michal and Vazquez, David and Romero, Adriana and Bengio, Yoshua},
	urldate = {2019-04-20},
	date = {2016-11},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1611.09326 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\184\\J√©gou et al. - 2016 - The One Hundred Layers Tiramisu Fully Convolution.pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\185\\1611.html:text/html}
}

@article{zhu_densenet_2017,
	title = {{DenseNet} for Dense Flow},
	url = {http://arxiv.org/abs/1707.06316},
	abstract = {Classical approaches for estimating optical flow have achieved rapid progress in the last decade. However, most of them are too slow to be applied in real-time video analysis. Due to the great success of deep learning, recent work has focused on using {CNNs} to solve such dense prediction problems. In this paper, we investigate a new deep architecture, Densely Connected Convolutional Networks ({DenseNet}), to learn optical flow. This specific architecture is ideal for the problem at hand as it provides shortcut connections throughout the network, which leads to implicit deep supervision. We extend current {DenseNet} to a fully convolutional network to learn motion estimation in an unsupervised manner. Evaluation results on three standard benchmarks demonstrate that {DenseNet} is a better fit than other widely adopted {CNN} architectures for optical flow estimation.},
	journaltitle = {{arXiv}:1707.06316 [cs]},
	author = {Zhu, Yi and Newsam, Shawn},
	urldate = {2019-04-20},
	date = {2017-07},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Multimedia},
	file = {arXiv\:1707.06316 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\186\\Zhu et Newsam - 2017 - DenseNet for Dense Flow.pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\187\\1707.html:text/html}
}

@article{srivastava_highway_2015,
	title = {Highway Networks},
	url = {https://arxiv.org/abs/1505.00387v2},
	abstract = {There is plenty of theoretical and empirical evidence that depth of neural networks is a crucial ingredient for their success. However, network training becomes more difficult with increasing depth and training of very deep networks remains an open problem. In this extended abstract, we introduce a new architecture designed to ease gradient-based training of very deep networks. We refer to networks with this architecture as highway networks, since they allow unimpeded information flow across several layers on "information highways". The architecture is characterized by the use of gating units which learn to regulate the flow of information through a network. Highway networks with hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions, opening up the possibility of studying extremely deep and efficient architectures.},
	author = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, J√ºrgen},
	urldate = {2019-04-20},
	date = {2015-05},
	langid = {english},
	file = {Full Text PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\189\\Srivastava et al. - 2015 - Highway Networks.pdf:application/pdf;Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\190\\1505.html:text/html}
}

@article{he_deep_2015,
	title = {Deep Residual Learning for Image Recognition},
	url = {https://arxiv.org/abs/1512.03385v1},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the {ImageNet} dataset we evaluate residual nets with a depth of up to 152 layers‚Äî8x deeper than {VGG} nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the {ImageNet} test set. This result won the 1st place on the {ILSVRC} 2015 classification task. We also present analysis on {CIFAR}-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the {COCO} object detection dataset. Deep residual nets are foundations of our submissions to {ILSVRC} \& {COCO} 2015 competitions, where we also won the 1st places on the tasks of {ImageNet} detection, {ImageNet} localization, {COCO} detection, and {COCO} segmentation.},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	urldate = {2019-04-20},
	date = {2015-12},
	langid = {english},
	file = {Full Text PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\192\\He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\193\\1512.html:text/html}
}

@article{larsson_fractalnet:_2016,
	title = {{FractalNet}: Ultra-Deep Neural Networks without Residuals},
	url = {https://arxiv.org/abs/1605.07648v4},
	shorttitle = {{FractalNet}},
	abstract = {We introduce a design strategy for neural network macro-architecture based on self-similarity. Repeated application of a simple expansion rule generates deep networks whose structural layouts are precisely truncated fractals. These networks contain interacting subpaths of different lengths, but do not include any pass-through or residual connections; every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers. In experiments, fractal networks match the excellent performance of standard residual networks on both {CIFAR} and {ImageNet} classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks. Rather, the key may be the ability to transition, during training, from effectively shallow to deep. We note similarities with student-teacher behavior and develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures. Such regularization allows extraction of high-performance fixed-depth subnetworks. Additionally, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer.},
	author = {Larsson, Gustav and Maire, Michael and Shakhnarovich, Gregory},
	urldate = {2019-04-20},
	date = {2016-05},
	langid = {english},
	file = {Full Text PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\195\\Larsson et al. - 2016 - FractalNet Ultra-Deep Neural Networks without Res.pdf:application/pdf;Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\196\\1605.html:text/html}
}

@article{dubnov_system_nodate,
	title = {A System for Computer Music Generation by Learning and Improvisation in a Particular Style},
	abstract = {The paper deals with question of music modeling and generation, capturing the idiomatic style of a composer or a genre, in an attempt to provide the computer with ‚Äúcreative‚Äù, human-like capabilities. The system described in the paper performs analyses of musical sequences and computes a model according to which new interpretations / improvisations close to the original's style can be generated. Important aspects of the musical structure are captured, including rhythm, melodic contour, and polyphonic relationships. Two statistical learning methods are implemented and their performance is compared in the paper: The first method is based on ‚ÄúUniversal Prediction‚Äù, a method derived from Information Theory and previously shown to be useful for statistical modeling in the musical domain. The second method is an application to music of Prediction Suffix Trees ({PST}), a learning technique initially developed for statistical modeling of complex sequences with applications in Linguistics and Computational Biology. Both methods provide a stochastic representation of musical style and are able to capture musical structures of variable lengths, from short ornamentations to complete musical phrases. Operations such as improvisation or computer aided composition can be realized using the system.},
	pages = {15},
	author = {Dubnov, S and Assayag, G and Bejerano, G and Lartillot, O},
	langid = {english},
	file = {Dubnov et al. - A System for Computer Music Generation by Learning.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\199\\Dubnov et al. - A System for Computer Music Generation by Learning.pdf:application/pdf}
}

@book{arxiv_listen_nodate,
	title = {Listen to this classical music composed in the style of Bach by a deep-learning machine},
	url = {https://www.technologyreview.com/s/603137/deep-learning-machine-listens-to-bach-then-writes-its-own-music-in-the-same-style/},
	abstract = {Can you tell the difference between music composed by Bach and by a neural network?},
	author = {{arXiv}, Emerging Technology from the},
	urldate = {2019-05-06},
	langid = {american},
	file = {Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\204\\deep-learning-machine-listens-to-bach-then-writes-its-own-music-in-the-same-style.html:text/html}
}

@article{hadjeres_deepbach:_2016,
	title = {{DeepBach}: a Steerable Model for Bach Chorales Generation},
	url = {http://arxiv.org/abs/1612.01010},
	shorttitle = {{DeepBach}},
	abstract = {This paper introduces {DeepBach}, a graphical model aimed at modeling polyphonic music and specifically hymn-like pieces. We claim that, after being trained on the chorale harmonizations by Johann Sebastian Bach, our model is capable of generating highly convincing chorales in the style of Bach. {DeepBach}'s strength comes from the use of pseudo-Gibbs sampling coupled with an adapted representation of musical data. This is in contrast with many automatic music composition approaches which tend to compose music sequentially. Our model is also steerable in the sense that a user can constrain the generation by imposing positional constraints such as notes, rhythms or cadences in the generated score. We also provide a plugin on top of the {MuseScore} music editor making the interaction with {DeepBach} easy to use.},
	journaltitle = {{arXiv}:1612.01010 [cs]},
	author = {Hadjeres, Ga√´tan and Pachet, Fran√ßois and Nielsen, Frank},
	urldate = {2019-05-06},
	date = {2016-12},
	keywords = {Computer Science - Sound, Computer Science - Artificial Intelligence},
	file = {arXiv\:1612.01010 PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\207\\Hadjeres et al. - 2016 - DeepBach a Steerable Model for Bach Chorales Gene.pdf:application/pdf;arXiv.org Snapshot:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\208\\1612.html:text/html}
}

@inproceedings{karishma_cluster_2015,
	title = {Cluster Based Melody Generation},
	abstract = {This project describes the development of an application for generating tonal melodies. The goal of the project is to ascertain our current understanding of tonal music by means of algorithmic music generation. The method followed consists of four stages: 1) selection of music-theoretical insights, 2) translation of these insights into a set of principles, 3) conversion of the principles into a computational model having the form of an algorithm for music generation, 4) testing the ‚Äúmusic‚Äù generated by the algorithm to evaluate the adequacy of the model. As an example, the method is implemented in Melody Generator, an algorithm for generating tonal melodies. The program has a structure suited for generating, displaying, playing and storing melodies, functions which are all accessible via a dedicated interface. The actual generation of melodies, is based in part on constraints imposed by the tonal context, i.e. by meter and key, the settings of which are controlled by means of parameters on the interface. Out proposed system will add parallel processing activities to get output in very short time.},
	author = {Karishma, Malla Vaidya and Shivani, J. L. Divya and Juhi, Gandhi and Pradnya, Kale and Avhad, Kiran},
	date = {2015},
	keywords = {Algorithm, Algorithmic composition, Computation, Computational model, Parallel computing, Probabilistic automaton, Robot (device)},
	file = {Full Text PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\213\\Karishma et al. - 2015 - Cluster Based Melody Generation.pdf:application/pdf}
}

@article{herremans_functional_2017,
	title = {A Functional Taxonomy of Music Generation Systems},
	volume = {50},
	issn = {03600300},
	url = {http://dl.acm.org/citation.cfm?doid=3145473.3108242},
	doi = {10.1145/3108242},
	pages = {1--30},
	number = {5},
	journaltitle = {{ACM} Comput. Surv.},
	author = {Herremans, Dorien and Chuan, Ching-Hua and Chew, Elaine},
	urldate = {2019-05-07},
	date = {2017-09},
	langid = {english},
	file = {Herremans et al. - 2017 - A Functional Taxonomy of Music Generation Systems.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\215\\Herremans et al. - 2017 - A Functional Taxonomy of Music Generation Systems.pdf:application/pdf}
}

@article{chuan_modeling_nodate,
	title = {Modeling Temporal Tonal Relations in Polyphonic Music through Deep Networks with a Novel Image-Based Representation},
	abstract = {We propose an end-to-end approach for modeling polyphonic music with a novel graphical representation, based on music theory, in a deep neural network. Despite the success of deep learning in various applications, it remains a challenge to incorporate existing domain knowledge in a network without affecting its training routines. In this paper we present a novel approach for predictive music modeling and music generation that incorporates domain knowledge in its representation. In this work, music is transformed into a 2D representation, inspired by tonnetz from music theory, which graphically encodes musical relationships between pitches. This representation is incorporated in a deep network structure consisting of multilayered convolutional neural networks ({CNN}, for learning an efÔ¨Åcient abstract encoding of the representation) and recurrent neural networks with long short-term memory cells ({LSTM}, for capturing temporal dependencies in music sequences). We empirically evaluate the nature and the effectiveness of the network by using a dataset of classical music from various composers. We investigate the effect of parameters including the number of convolution feature maps, pooling strategies, and three conÔ¨Ågurations of the network: {LSTM} without {CNN}, {LSTM} with {CNN} (pre-trained vs. not pre-trained). Visualizations of the feature maps and Ô¨Ålters in the {CNN} are explored, and a comparison is made between the proposed tonnetz-inspired representation and pianoroll, a commonly used representation of music in computational systems. Experimental results show that the tonnetz representation produces musical sequences that are more tonally stable and contain more repeated patterns than sequences generated by pianoroll-based models, a Ô¨Ånding that is directly useful for tackling current challenges in music and {AI} such as smart music generation.},
	pages = {8},
	author = {Chuan, Ching-Hua and Herremans, Dorien},
	langid = {english},
	file = {Chuan et Herremans - Modeling Temporal Tonal Relations in Polyphonic Mu.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\219\\Chuan et Herremans - Modeling Temporal Tonal Relations in Polyphonic Mu.pdf:application/pdf}
}

@article{herremans_morpheus:_2017,
	title = {{MorpheuS}: generating structured music with constrained patterns and tension},
	issn = {1949-3045},
	url = {http://ieeexplore.ieee.org/document/8007229/},
	doi = {10.1109/TAFFC.2017.2737984},
	shorttitle = {{MorpheuS}},
	abstract = {Automatic music generation systems have gained in popularity and sophistication as advances in cloud computing have enabled large-scale complex computations such as deep models and optimization algorithms on personal devices. Yet, they still face an important challenge, that of long-term structure, which is key to conveying a sense of musical coherence. We present the {MorpheuS} music generation system designed to tackle this problem. {MorpheuS}‚Äô novel framework has the ability to generate polyphonic pieces with a given tension proÔ¨Åle and long- and short-term repeated pattern structures. A mathematical model for tonal tension quantiÔ¨Åes the tension proÔ¨Åle and state-of-the-art pattern detection algorithms extract repeated patterns in a template piece. An efÔ¨Åcient optimization metaheuristic, variable neighborhood search, generates music by assigning pitches that best Ô¨Åt the prescribed tension proÔ¨Åle to the template rhythm while hard constraining long-term structure through the detected patterns. This ability to generate affective music with speciÔ¨Åc tension proÔ¨Åle and long-term structure is particularly useful in a game or Ô¨Ålm music context. Music generated by the {MorpheuS} system has been performed live in concerts.},
	pages = {1--1},
	journaltitle = {{IEEE} Trans. Affective Comput.},
	author = {Herremans, Dorien and Chew, Elaine},
	urldate = {2019-05-13},
	date = {2017},
	langid = {english},
	file = {Herremans et Chew - 2017 - MorpheuS generating structured music with constra.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\223\\Herremans et Chew - 2017 - MorpheuS generating structured music with constra.pdf:application/pdf}
}

@article{agres_harmonic_2017,
	title = {Harmonic Structure Predicts the Enjoyment of Uplifting Trance Music},
	volume = {7},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2016.01999/full},
	doi = {10.3389/fpsyg.2016.01999},
	abstract = {An empirical investigation of how local harmonic structures (e.g., chord progressions) contribute to the experience and enjoyment of uplifting trance ({UT}) music is presented. The connection between rhythmic and percussive elements and resulting trance-like states has been highlighted by musicologists, but no research, to our knowledge, has explored whether repeated harmonic elements influence affective responses in listeners of trance music. Two alternative hypotheses are discussed, the first highlighting the direct relationship between repetition/complexity and enjoyment, and the second based on the theoretical inverted-U relationship described by the Wundt curve. We investigate the connection between harmonic structure and subjective enjoyment through interdisciplinary behavioral and computational methods: First we discuss an experiment in which listeners provided enjoyment ratings for computer-generated {UT} anthems with varying levels of harmonic repetition and complexity. The anthems were generated using a statistical model trained on a corpus of 100 uplifting trance anthems created for this purpose, and harmonic structure was constrained by imposing particular repetition structures (semiotic patterns defining the order of chords in the sequence) on a professional {UT} music production template. Second, the relationship between harmonic structure and enjoyment is further explored using two computational approaches, one based on average Information Content, and another that measures average tonal tension between chords. The results of the listening experiment indicate that harmonic repetition does in fact contribute to the enjoyment of uplifting trance music. More compelling evidence was found for the second hypothesis discussed above, however some maximally repetitive structures were also preferred. Both computational models provide evidence for a Wundt-type relationship between complexity and enjoyment. By systematically manipulating the structure of chord progressions, we have discovered specific harmonic contexts in which repetitive or complex structure contribute to the enjoyment of uplifting trance music.},
	journaltitle = {Front. Psychol.},
	author = {Agres, Kat and Herremans, Dorien and Bigo, Louis and Conklin, Darrell},
	urldate = {2019-05-15},
	date = {2017},
	keywords = {Complexity, Computational Creativity, enjoyment, music cognition, repetition, tension, Uplifting Trance music, Wundt curve},
	file = {Full Text PDF:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\228\\Agres et al. - 2017 - Harmonic Structure Predicts the Enjoyment of Uplif.pdf:application/pdf}
}

@article{herremans_modeling_2017,
	title = {Modeling Musical Context Using Word2vec},
	abstract = {We present a semantic vector space model for capturing complex polyphonic musical context. A word2vec model based on a skip-gram representation with negative sampling was used to model slices of music from a dataset of Beethoven‚Äôs piano sonatas. A visualization of the reduced vector space using t-distributed stochastic neighbor embedding shows that the resulting embedded vector space captures tonal relationships, even without any explicit information about the musical contents of the slices. Secondly, an excerpt of the Moonlight Sonata from Beethoven was altered by replacing slices based on context similarity. The resulting music shows that the selected slice based on similar word2vec context also has a relatively short tonal distance from the original slice.},
	pages = {8},
	author = {Herremans, D},
	date = {2017},
	langid = {english},
	file = {Herremans - 2017 - Modeling Musical Context Using Word2vec.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\229\\Herremans - 2017 - Modeling Musical Context Using Word2vec.pdf:application/pdf}
}

@inproceedings{herremans_multi-modal_2017,
	location = {San Diego, {CA}, {USA}},
	title = {A Multi-modal Platform for Semantic Music Analysis: Visualizing Audio-and Score-Based Tension},
	isbn = {978-1-5090-4284-5},
	url = {http://ieeexplore.ieee.org/document/7889573/},
	doi = {10.1109/ICSC.2017.49},
	shorttitle = {A Multi-modal Platform for Semantic Music Analysis},
	abstract = {Musicologists, music cognition scientists and others have long studied music in all of its facets. During the last few decades, research in both score and audio technology has opened the doors for automated, or (in many cases) semi-automated analysis. There remains a big gap, however, between the Ô¨Åeld of audio (performance) and score-based systems. In this research, we propose a web-based Interactive system for Multi-modal Music Analysis ({IMMA}), that provides musicologists with an intuitive interface for a joint analysis of performance and score. As an initial use-case, we implemented a tension analysis module in the system. Tension is a semantic characteristic of music that directly shapes the music experience and thus forms a crucial topic for researchers in musicology and music cognition. The module includes methods for calculating tonal tension (from the score) and timbral tension (from the performance). An audio-toscore alignment algorithm based on dynamic time warping was implemented to automate the synchronization between the audio and score analysis. The resulting system was tested on three performances (violin, Ô¨Çute, and guitar) of Paganini‚Äôs Caprice No. 24 and four piano performances of Beethoven‚Äôs Moonlight Sonata. We statistically analyzed the results of tonal and timbral tension and found correlations between them. A clustering algorithm was implemented to Ô¨Ånd segments of music (both within and between performances) with similar shape in their tension curve. These similar segments are visualized in {IMMA}. By displaying selected audio and score characteristics together with musical score following in sync with the performance playback, {IMMA} offers a user-friendly intuitive interface to bridge the gap between audio and score analysis.},
	pages = {419--426},
	booktitle = {2017 {IEEE} 11th International Conference on Semantic Computing ({ICSC})},
	publisher = {{IEEE}},
	author = {Herremans, Dorien and Chuan, Ching-Hua},
	urldate = {2019-05-15},
	date = {2017},
	langid = {english},
	file = {Herremans et Chuan - 2017 - A Multi-modal Platform for Semantic Music Analysis.pdf:D\:\\Valentin\\Etudes\\NUS\\Dissertation\\Zotero\\Ma biblioth√®que\\files\\231\\Herremans et Chuan - 2017 - A Multi-modal Platform for Semantic Music Analysis.pdf:application/pdf}
}

@inproceedings{balliauw_tabu_2015,
	title = {A Tabu Search Algorithm to Generate Piano Fingerings for Polyphonic Sheet Music},
	abstract = {A piano fingering is an indication of which finger is to be used to play each note. Good piano fingerings enable pianists to study, remember and play pieces fluently. In this paper, we propose an algorithm to find a good piano fingering automatically. The tabu search algorithm is a metaheuristic that can find a good solution in a short amount of execution time. The algorithm implements an objective function that takes into account the characteristics of the pianist‚Äôs hand in complex polyphonic music.},
	author = {Balliauw, Matteo and Herremans, Dorien and Cuervo, Daniel Palhazi and S√∂rensen, Kenneth},
	date = {2015},
	keywords = {Metaheuristic, Optimization problem, Run time (program lifecycle phase), Search algorithm, Tabu search},
	file = {Full Text PDF:C\:\\Users\\Vval\\Zotero\\storage\\CCFF34MG\\Balliauw et al. - 2015 - A Tabu Search Algorithm to Generate Piano Fingerin.pdf:application/pdf}
}

@article{herremans_composing_2013,
	title = {Composing fifth species counterpoint music with a variable neighborhood search algorithm},
	volume = {40},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417413003692},
	doi = {10.1016/j.eswa.2013.05.071},
	abstract = {In this paper, a variable neighborhood search ({VNS}) algorithm is developed and analyzed that can generate Ô¨Åfth species counterpoint fragments. The existing species counterpoint rules are quantiÔ¨Åed and form the basis of the objective function used by the algorithm. The {VNS} developed in this research is a local search metaheuristic that starts from a randomly generated fragment and gradually improves this solution by changing one or two notes at a time. An in-depth statistical analysis reveals the signiÔ¨Åcance as well as the optimal settings of the parameters of the {VNS}. The algorithm has been implemented in a user-friendly software environment called Optimuse. Optimuse allows a user to input basic characteristics such as length, key and mode. Based on this input, a Ô¨Åfth species counterpoint fragment is generated by the system that can be edited and played back immediately.},
	pages = {6427--6437},
	number = {16},
	journaltitle = {Expert Systems with Applications},
	author = {Herremans, D. and S√∂rensen, K.},
	urldate = {2019-06-13},
	date = {2013-11},
	langid = {english},
	file = {Herremans et S√∂rensen - 2013 - Composing fifth species counterpoint music with a .pdf:C\:\\Users\\Vval\\Zotero\\storage\\ZTCT9WFG\\Herremans et S√∂rensen - 2013 - Composing fifth species counterpoint music with a .pdf:application/pdf}
}

@article{herremans_generating_2015,
	title = {Generating structured music for bagana using quality metrics based on Markov models},
	volume = {42},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417415003796},
	doi = {10.1016/j.eswa.2015.05.043},
	abstract = {In this research, a system is built that generates bagana music, a traditional lyre from Ethiopia, based on a Ô¨Årst order Markov model. Due to the size of many datasets it is often only possible to get rich and reliable statistics for low order models, yet these do not handle structure very well and their output is often very repetitive. A Ô¨Årst contribution of this paper is to propose a method that allows the enforcement of structure and repetition within music, thus handling long term coherence with a Ô¨Årst order model. The second goal of this research is to explain and propose different ways in which low order Markov models can be used to build quality assessment metrics for an optimization algorithm. These are then implemented in a variable neighbourhood search algorithm that generates bagana music. The results are examined and thorougly evaluated.},
	pages = {7424--7435},
	number = {21},
	journaltitle = {Expert Systems with Applications},
	author = {Herremans, D. and Weisser, S. and S√∂rensen, K. and Conklin, D.},
	urldate = {2019-06-29},
	date = {2015-11},
	langid = {english},
	file = {Herremans et al. - 2015 - Generating structured music for bagana using quali.pdf:C\:\\Users\\Vval\\Zotero\\storage\\Z95V32XF\\Herremans et al. - 2015 - Generating structured music for bagana using quali.pdf:application/pdf}
}

@inproceedings{madhok_sentimozart:_2018,
	title = {{SentiMozart}: Music Generation based on Emotions},
	doi = {10.5220/0006597705010506},
	shorttitle = {{SentiMozart}},
	abstract = {Facial expressions are one of the best and the most intuitive way to determine a persons emotions. They most naturally express how a person is feeling currently. The aim of the proposed framework is to generate music corresponding to the emotion of the person predicted by our model. The proposed framework is divided into two models, the Image Classification Model and the Music Generation Model. The music would be generated by the latter model which is essentially a Doubly Stacked {LSTM} architecture. This is to be done after classification and identification of the facial expression into one of the seven major sentiment categories: Angry, Disgust, Fear, Happy, Sad, Surprise and Neutral, which would be done by using Convolutional Neural Networks ({CNN}). Finally, we evaluate the performance of our proposed framework using the emotional Mean Opinion Score ({MOS}) which is a popular evaluation metric for audio-visual data.},
	booktitle = {{ICAART}},
	author = {Madhok, Rishi and Goel, Shivali and Garg, Shweta},
	date = {2018},
	keywords = {British Informatics Olympiad, Computer science, Computer scientist, Convolutional neural network, Instagram, Long short-term memory, Regular expression, Snapchat, Synergy, User analysis},
	file = {Full Text PDF:C\:\\Users\\Vval\\Zotero\\storage\\HXLQVBM7\\Madhok et al. - 2018 - SentiMozart Music Generation based on Emotions.pdf:application/pdf}
}

@online{noauthor_[1905.13570]_nodate,
	title = {[1905.13570] Factorized Inference in Deep Markov Models for Incomplete Multimodal Time Series},
	url = {https://arxiv.org/abs/1905.13570},
	urldate = {2019-07-01}
}

@article{tan_factorized_2019,
	title = {Factorized Inference in Deep Markov Models for Incomplete Multimodal Time Series},
	url = {http://arxiv.org/abs/1905.13570},
	abstract = {Integrating deep learning with latent state space models has the potential to yield temporal models that are powerful, yet tractable and interpretable. Unfortunately, current models are not designed to handle missing data or multiple data modalities, which are both prevalent in real-world data. In this work, we introduce a factorized inference method for Multimodal Deep Markov Models ({MDMMs}), allowing us to filter and smooth in the presence of missing data, while also performing uncertainty-aware multimodal fusion. We derive this method by factorizing the posterior p(z{\textbar}x) for non-linear state space models, and develop a variational backward-forward algorithm for inference. Because our method handles incompleteness over both time and modalities, it is capable of interpolation, extrapolation, conditional generation, and label prediction in multimodal time series. We demonstrate these capabilities on both synthetic and real-world multimodal data under high levels of data deletion. Our method performs well even with more than 50\% missing data, and outperforms existing deep approaches to inference in latent time series.},
	journaltitle = {{arXiv}:1905.13570 [cs, stat]},
	author = {Tan, Zhi-Xuan and Soh, Harold and Ong, Desmond C.},
	urldate = {2019-07-01},
	date = {2019-05-30},
	eprinttype = {arxiv},
	eprint = {1905.13570},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence},
	file = {arXiv\:1905.13570 PDF:C\:\\Users\\Vval\\Zotero\\storage\\A3JW4SH8\\Tan et al. - 2019 - Factorized Inference in Deep Markov Models for Inc.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\TC334678\\1905.html:text/html}
}

@incollection{dieleman_challenge_2018,
	title = {The challenge of realistic music generation: modelling raw audio at scale},
	url = {http://papers.nips.cc/paper/8023-the-challenge-of-realistic-music-generation-modelling-raw-audio-at-scale.pdf},
	shorttitle = {The challenge of realistic music generation},
	pages = {7989--7999},
	booktitle = {Advances in Neural Information Processing Systems 31},
	publisher = {Curran Associates, Inc.},
	author = {Dieleman, Sander and van den Oord, Aaron and Simonyan, Karen},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	urldate = {2019-07-18},
	date = {2018},
	file = {NIPS Full Text PDF:C\:\\Users\\Vval\\Zotero\\storage\\FGIV8GRA\\Dieleman et al. - 2018 - The challenge of realistic music generation model.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\33LMLL5S\\8023-the-challenge-of-realistic-music-generation-modelling-raw-audio-at-scale.html:text/html}
}

@article{yu_multi-scale_2015,
	title = {Multi-Scale Context Aggregation by Dilated Convolutions},
	url = {http://arxiv.org/abs/1511.07122},
	abstract = {State-of-the-art models for semantic segmentation are based on adaptations of convolutional networks that had originally been designed for image classification. However, dense prediction and image classification are structurally different. In this work, we develop a new convolutional network module that is specifically designed for dense prediction. The presented module uses dilated convolutions to systematically aggregate multi-scale contextual information without losing resolution. The architecture is based on the fact that dilated convolutions support exponential expansion of the receptive field without loss of resolution or coverage. We show that the presented context module increases the accuracy of state-of-the-art semantic segmentation systems. In addition, we examine the adaptation of image classification networks to dense prediction and show that simplifying the adapted network can increase accuracy.},
	journaltitle = {{arXiv}:1511.07122 [cs]},
	author = {Yu, Fisher and Koltun, Vladlen},
	urldate = {2019-07-18},
	date = {2015-11-23},
	eprinttype = {arxiv},
	eprint = {1511.07122},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1511.07122 PDF:C\:\\Users\\Vval\\Zotero\\storage\\E5UKRC3A\\Yu et Koltun - 2015 - Multi-Scale Context Aggregation by Dilated Convolu.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\QPA8MDU8\\1511.html:text/html}
}

@incollection{wu_multimodal_2018,
	title = {Multimodal Generative Models for Scalable Weakly-Supervised Learning},
	url = {http://papers.nips.cc/paper/7801-multimodal-generative-models-for-scalable-weakly-supervised-learning.pdf},
	pages = {5575--5585},
	booktitle = {Advances in Neural Information Processing Systems 31},
	publisher = {Curran Associates, Inc.},
	author = {Wu, Mike and Goodman, Noah},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	urldate = {2019-07-19},
	date = {2018},
	file = {NIPS Full Text PDF:C\:\\Users\\Vval\\Zotero\\storage\\IMWYZ7TC\\Wu et Goodman - 2018 - Multimodal Generative Models for Scalable Weakly-S.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\Z2A5HJLL\\7801-multimodal-generative-models-for-scalable-weakly-supervised-learning.html:text/html}
}

@incollection{wu_multimodal_2018-1,
	title = {Multimodal Generative Models for Scalable Weakly-Supervised Learning},
	url = {http://papers.nips.cc/paper/7801-multimodal-generative-models-for-scalable-weakly-supervised-learning.pdf},
	pages = {5575--5585},
	booktitle = {Advances in Neural Information Processing Systems 31},
	publisher = {Curran Associates, Inc.},
	author = {Wu, Mike and Goodman, Noah},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	urldate = {2019-07-19},
	date = {2018},
	file = {NIPS Full Text PDF:C\:\\Users\\Vval\\Zotero\\storage\\TAPFGTR6\\Wu et Goodman - 2018 - Multimodal Generative Models for Scalable Weakly-S.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\UKNK9876\\7801-multimodal-generative-models-for-scalable-weakly-supervised-learning.html:text/html}
}

@article{krishnan_structured_2016,
	title = {Structured Inference Networks for Nonlinear State Space Models},
	url = {http://arxiv.org/abs/1609.09869},
	abstract = {Gaussian state space models have been used for decades as generative models of sequential data. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption. We introduce a unified algorithm to efficiently learn a broad class of linear and non-linear state space models, including variants where the emission and transition distributions are modeled by deep neural networks. Our learning algorithm simultaneously learns a compiled inference network and the generative model, leveraging a structured variational approximation parameterized by recurrent neural networks to mimic the posterior distribution. We apply the learning algorithm to both synthetic and real-world datasets, demonstrating its scalability and versatility. We find that using the structured approximation to the posterior results in models with significantly higher held-out likelihood.},
	journaltitle = {{arXiv}:1609.09869 [cs, stat]},
	author = {Krishnan, Rahul G. and Shalit, Uri and Sontag, David},
	urldate = {2019-08-01},
	date = {2016-09-30},
	eprinttype = {arxiv},
	eprint = {1609.09869},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv\:1609.09869 PDF:C\:\\Users\\Vval\\Zotero\\storage\\UMISP9UH\\Krishnan et al. - 2016 - Structured Inference Networks for Nonlinear State .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\9KN7DMJU\\1609.html:text/html}
}

@article{schroecker_generative_2019,
	title = {Generative predecessor models for sample-efficient imitation learning},
	url = {http://arxiv.org/abs/1904.01139},
	abstract = {We propose Generative Predecessor Models for Imitation Learning ({GPRIL}), a novel imitation learning algorithm that matches the state-action distribution to the distribution observed in expert demonstrations, using generative models to reason probabilistically about alternative histories of demonstrated states. We show that this approach allows an agent to learn robust policies using only a small number of expert demonstrations and self-supervised interactions with the environment. We derive this approach from first principles and compare it empirically to a state-of-the-art imitation learning method, showing that it outperforms or matches its performance on two simulated robot manipulation tasks and demonstrate significantly higher sample efficiency by applying the algorithm on a real robot.},
	journaltitle = {{arXiv}:1904.01139 [cs, stat]},
	author = {Schroecker, Yannick and Vecerik, Mel and Scholz, Jonathan},
	urldate = {2019-12-16},
	date = {2019-04-01},
	eprinttype = {arxiv},
	eprint = {1904.01139},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\6MVHDRIW\\Schroecker et al. - 2019 - Generative predecessor models for sample-efficient.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\NNPDG9HH\\1904.html:text/html}
}

@article{papamakarios_normalizing_2019,
	title = {Normalizing Flows for Probabilistic Modeling and Inference},
	url = {http://arxiv.org/abs/1912.02762},
	abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
	journaltitle = {{arXiv}:1912.02762 [cs, stat]},
	author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
	urldate = {2019-12-16},
	date = {2019-12-05},
	eprinttype = {arxiv},
	eprint = {1912.02762},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\H5Z4NPL7\\Papamakarios et al. - 2019 - Normalizing Flows for Probabilistic Modeling and I.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\2XRAICLC\\1912.html:text/html}
}

@article{lipton_troubling_2018,
	title = {Troubling Trends in Machine Learning Scholarship},
	url = {http://arxiv.org/abs/1807.03341},
	abstract = {Collectively, machine learning ({ML}) researchers are engaged in the creation and dissemination of knowledge about data-driven algorithms. In a given paper, researchers might aspire to any subset of the following goals, among others: to theoretically characterize what is learnable, to obtain understanding through empirically rigorous experiments, or to build a working system that has high predictive accuracy. While determining which knowledge warrants inquiry may be subjective, once the topic is fixed, papers are most valuable to the community when they act in service of the reader, creating foundational knowledge and communicating as clearly as possible. Recent progress in machine learning comes despite frequent departures from these ideals. In this paper, we focus on the following four patterns that appear to us to be trending in {ML} scholarship: (i) failure to distinguish between explanation and speculation; (ii) failure to identify the sources of empirical gains, e.g., emphasizing unnecessary modifications to neural architectures when gains actually stem from hyper-parameter tuning; (iii) mathiness: the use of mathematics that obfuscates or impresses rather than clarifies, e.g., by confusing technical and non-technical concepts; and (iv) misuse of language, e.g., by choosing terms of art with colloquial connotations or by overloading established technical terms. While the causes behind these patterns are uncertain, possibilities include the rapid expansion of the community, the consequent thinness of the reviewer pool, and the often-misaligned incentives between scholarship and short-term measures of success (e.g., bibliometrics, attention, and entrepreneurial opportunity). While each pattern offers a corresponding remedy (don't do it), we also discuss some speculative suggestions for how the community might combat these trends.},
	journaltitle = {{arXiv}:1807.03341 [cs, stat]},
	author = {Lipton, Zachary C. and Steinhardt, Jacob},
	urldate = {2019-12-16},
	date = {2018-07-26},
	eprinttype = {arxiv},
	eprint = {1807.03341},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\38RXNU5P\\Lipton et Steinhardt - 2018 - Troubling Trends in Machine Learning Scholarship.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\38K7733Z\\1807.html:text/html}
}

@article{merity_single_2019,
	title = {Single Headed Attention {RNN}: Stop Thinking With Your Head},
	url = {http://arxiv.org/abs/1911.11423},
	shorttitle = {Single Headed Attention {RNN}},
	abstract = {The leading approaches in language modeling are all obsessed with {TV} shows of my youth - namely Transformers and Sesame Street. Transformers this, Transformers that, and over here a bonfire worth of {GPU}-{TPU}-neuromorphic wafer scale silicon. We opt for the lazy path of old and proven techniques with a fancy crypto inspired acronym: the Single Headed Attention {RNN} ({SHA}-{RNN}). The author's lone goal is to show that the entire field might have evolved a different direction if we had instead been obsessed with a slightly different acronym and slightly different result. We take a previously strong language model based only on boring {LSTMs} and get it to within a stone's throw of a stone's throw of state-of-the-art byte level language model results on enwik8. This work has undergone no intensive hyperparameter optimization and lived entirely on a commodity desktop machine that made the author's small studio apartment far too warm in the midst of a San Franciscan summer. The final results are achievable in plus or minus 24 hours on a single {GPU} as the author is impatient. The attention mechanism is also readily extended to large contexts with minimal computation. Take that Sesame Street.},
	journaltitle = {{arXiv}:1911.11423 [cs]},
	author = {Merity, Stephen},
	urldate = {2019-12-16},
	date = {2019-11-27},
	eprinttype = {arxiv},
	eprint = {1911.11423},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\4B6W2UHY\\Merity - 2019 - Single Headed Attention RNN Stop Thinking With Yo.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\JHTALPK6\\1911.html:text/html}
}

@article{lorraine_optimizing_2019,
	title = {Optimizing Millions of Hyperparameters by Implicit Differentiation},
	url = {http://arxiv.org/abs/1911.02590},
	abstract = {We propose an algorithm for inexpensive gradient-based hyperparameter optimization that combines the implicit function theorem ({IFT}) with efficient inverse Hessian approximations. We present results about the relationship between the {IFT} and differentiating through optimization, motivating our algorithm. We use the proposed approach to train modern network architectures with millions of weights and millions of hyper-parameters. For example, we learn a data-augmentation network - where every weight is a hyperparameter tuned for validation performance - outputting augmented training examples. Jointly tuning weights and hyperparameters with our approach is only a few times more costly in memory and compute than standard training.},
	journaltitle = {{arXiv}:1911.02590 [cs, stat]},
	author = {Lorraine, Jonathan and Vicol, Paul and Duvenaud, David},
	urldate = {2019-12-16},
	date = {2019-11-06},
	eprinttype = {arxiv},
	eprint = {1911.02590},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\RFLJUW2V\\Lorraine et al. - 2019 - Optimizing Millions of Hyperparameters by Implicit.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\98YZGY2F\\1911.html:text/html}
}

@inproceedings{deng_structure_2016,
	location = {Las Vegas, {NV}, {USA}},
	title = {Structure Inference Machines: Recurrent Neural Networks for Analyzing Relations in Group Activity Recognition},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780885/},
	doi = {10.1109/CVPR.2016.516},
	shorttitle = {Structure Inference Machines},
	abstract = {Rich semantic relations are important in a variety of visual recognition problems. As a concrete example, group activity recognition involves the interactions and relative spatial relations of a set of people in a scene. State of the art recognition methods center on deep learning approaches for training highly effective, complex classiÔ¨Åers for interpreting images. However, bridging the relatively low-level concepts output by these methods to interpret higher-level compositional scenes remains a challenge. Graphical models are a standard tool for this task. In this paper, we propose a method to integrate graphical models and deep neural networks into a joint framework. Instead of using a traditional inference method, we use a sequential inference modeled by a recurrent neural network. Beyond this, the appropriate structure for inference can be learned by imposing gates on edges between nodes. Empirical results on group activity recognition demonstrate the potential of this model to handle highly structured learning tasks.},
	eventtitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {4772--4781},
	booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	publisher = {{IEEE}},
	author = {Deng, Zhiwei and Vahdat, Arash and Hu, Hexiang and Mori, Greg},
	urldate = {2020-01-04},
	date = {2016-06},
	langid = {english},
	file = {Deng et al. - 2016 - Structure Inference Machines Recurrent Neural Net.pdf:C\:\\Users\\Vval\\Zotero\\storage\\QFMRZH3X\\Deng et al. - 2016 - Structure Inference Machines Recurrent Neural Net.pdf:application/pdf}
}

@article{frazier_tutorial_2018,
	title = {A Tutorial on Bayesian Optimization},
	url = {http://arxiv.org/abs/1807.02811},
	abstract = {Bayesian optimization is an approach to optimizing objective functions that take a long time (minutes or hours) to evaluate. It is best-suited for optimization over continuous domains of less than 20 dimensions, and tolerates stochastic noise in function evaluations. It builds a surrogate for the objective and quantifies the uncertainty in that surrogate using a Bayesian machine learning technique, Gaussian process regression, and then uses an acquisition function defined from this surrogate to decide where to sample. In this tutorial, we describe how Bayesian optimization works, including Gaussian process regression and three common acquisition functions: expected improvement, entropy search, and knowledge gradient. We then discuss more advanced techniques, including running multiple function evaluations in parallel, multi-fidelity and multi-information source optimization, expensive-to-evaluate constraints, random environmental conditions, multi-task Bayesian optimization, and the inclusion of derivative information. We conclude with a discussion of Bayesian optimization software and future research directions in the field. Within our tutorial material we provide a generalization of expected improvement to noisy evaluations, beyond the noise-free setting where it is more commonly applied. This generalization is justified by a formal decision-theoretic argument, standing in contrast to previous ad hoc modifications.},
	journaltitle = {{arXiv}:1807.02811 [cs, math, stat]},
	author = {Frazier, Peter I.},
	urldate = {2020-01-04},
	date = {2018-07-08},
	eprinttype = {arxiv},
	eprint = {1807.02811},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\YN25QUGG\\Frazier - 2018 - A Tutorial on Bayesian Optimization.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\X2Y7AJUD\\1807.html:text/html}
}

@article{schindler_deep_2020,
	title = {Deep Learning for {MIR} Tutorial},
	url = {http://arxiv.org/abs/2001.05266},
	abstract = {Deep Learning has become state of the art in visual computing and continuously emerges into the Music Information Retrieval ({MIR}) and audio retrieval domain. In order to bring attention to this topic we propose an introductory tutorial on deep learning for {MIR}. Besides a general introduction to neural networks, the proposed tutorial covers a wide range of {MIR} relevant deep learning approaches. {\textbackslash}textbf\{Convolutional Neural Networks\} are currently a de-facto standard for deep learning based audio retrieval. {\textbackslash}textbf\{Recurrent Neural Networks\} have proven to be effective in onset detection tasks such as beat or audio-event detection. {\textbackslash}textbf\{Siamese Networks\} have been shown effective in learning audio representations and distance functions specific for music similarity retrieval. We will incorporate both academic and industrial points of view into the tutorial. Accompanying the tutorial, we will create a Github repository for the content presented at the tutorial as well as references to state of the art work and literature for further reading. This repository will remain public after the conference.},
	journaltitle = {{arXiv}:2001.05266 [cs, eess]},
	author = {Schindler, Alexander and Lidy, Thomas and B√∂ck, Sebastian},
	urldate = {2020-01-27},
	date = {2020-01-15},
	eprinttype = {arxiv},
	eprint = {2001.05266},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\52GPB5V5\\Schindler et al. - 2020 - Deep Learning for MIR Tutorial.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\UGUXGJM8\\2001.html:text/html}
}

@inproceedings{fuentes_music_2019,
	title = {A Music Structure Informed Downbeat Tracking System Using Skip-chain Conditional Random Fields and Deep Learning},
	doi = {10.1109/ICASSP.2019.8682870},
	abstract = {In recent years the task of downbeat tracking has received increasing attention and the state of the art has been improved with the introduction of deep learning methods. Among proposed solutions, existing systems exploit short-term musical rules as part of their language modelling. In this work we show in an oracle scenario how including longer-term musical rules, in particular music structure, can enhance downbeat estimation. We introduce a skip-chain conditional random field language model for downbeat tracking designed to include section information in an unified and flexible framework. We combine this model with a state-of-the-art convolutional-recurrent network and we contrast the system's performance to the commonly used Bar Pointer model. Our experiments on the popular Beatles dataset show that incorporating structure information in the language model leads to more consistent and more robust downbeat estimations.},
	eventtitle = {{ICASSP} 2019 - 2019 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	pages = {481--485},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	author = {Fuentes, Magdalena and {McFee}, Brian and Crayencour, H√©l√®ne C. and Essid, Slim and Bello, Juan Pablo},
	date = {2019-05},
	note = {{ISSN}: 1520-6149},
	keywords = {Hidden Markov models, learning (artificial intelligence), Bars, convolutional-recurrent network, Convolutional-Recurrent Neural Networks, Deep learning, Deep Learning, deep learning methods, downbeat estimation, Downbeat Tracking, Estimation, estimation theory, language modelling, longer-term musical rules, music, Music, Music Structure, music structure informed downbeat tracking system, natural language processing, oracle scenario, random processes, recurrent neural nets, section information, short-term musical rules, skip-chain conditional random field language model, skip-chain conditional random fields, Skip-Chain Conditional Random Fields, structure information, Task analysis, Training},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Vval\\Zotero\\storage\\XZTVIEKN\\8682870.html:text/html}
}

@inproceedings{fuentes_music_2019-1,
	location = {Brighton, United Kingdom},
	title = {A Music Structure Informed Downbeat Tracking System Using Skip-chain Conditional Random Fields and Deep Learning},
	isbn = {978-1-4799-8131-1},
	url = {https://ieeexplore.ieee.org/document/8682870/},
	doi = {10.1109/ICASSP.2019.8682870},
	eventtitle = {{ICASSP} 2019 - 2019 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	pages = {481--485},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	publisher = {{IEEE}},
	author = {Fuentes, Magdalena and {McFee}, Brian and Crayencour, Helene C. and Essid, Slim and Bello, Juan Pablo},
	urldate = {2020-01-27},
	date = {2019-05}
}

@article{dai_music_2018,
	title = {Music Style Transfer: A Position Paper},
	url = {http://arxiv.org/abs/1803.06841},
	shorttitle = {Music Style Transfer},
	abstract = {Led by the success of neural style transfer on visual arts, there has been a rising trend very recently in the effort of music style transfer. However, "music style" is not yet a well-defined concept from a scientific point of view. The difficulty lies in the intrinsic multi-level and multi-modal character of music representation (which is very different from image representation). As a result, depending on their interpretation of "music style", current studies under the category of "music style transfer", are actually solving completely different problems that belong to a variety of sub-fields of Computer Music. Also, a vanilla end-to-end approach, which aims at dealing with all levels of music representation at once by directly adopting the method of image style transfer, leads to poor results. Thus, we vitally propose a more scientifically-viable definition of music style transfer by breaking it down into precise concepts of timbre style transfer, performance style transfer and composition style transfer, as well as to connect different aspects of music style transfer with existing well-established sub-fields of computer music studies. In addition, we discuss the current limitations of music style modeling and its future directions by drawing spirit from some deep generative models, especially the ones using unsupervised learning and disentanglement techniques.},
	journaltitle = {{arXiv}:1803.06841 [cs, eess]},
	author = {Dai, Shuqi and Zhang, Zheng and Xia, Gus G.},
	urldate = {2020-01-31},
	date = {2018-07-19},
	eprinttype = {arxiv},
	eprint = {1803.06841},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\ENNTZQ3S\\Dai et al. - 2018 - Music Style Transfer A Position Paper.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\FTVYFSHL\\1803.html:text/html}
}

@inproceedings{liang_automatic_2017,
	title = {Automatic Stylistic Composition of Bach Chorales with Deep {LSTM}},
	abstract = {This paper presents ‚Äú{BachBot}‚Äù: an end-to-end automatic composition system for composing and completing music in the style of Bach‚Äôs chorales using a deep long short-term memory ({LSTM}) generative model. We propose a new sequential encoding scheme for polyphonic music and a model for both composition and harmonization which can be efficiently sampled without expensive Markov Chain Monte Carlo ({MCMC}). Analysis of the trained model provides evidence of neurons specializing without prior knowledge or explicit supervision to detect common music-theoretic concepts such as tonics, chords, and cadences. To assess {BachBot}‚Äôs success, we conducted one of the largest musical discrimination tests on 2336 participants. Among the results, the proportion of responses correctly differentiating {BachBot} from Bach was only 1\% better than random guessing.},
	booktitle = {{ISMIR}},
	author = {Liang, Feynman T. and Gotham, Mark and Johnson, Matthew and Shotton, Jamie},
	date = {2017},
	keywords = {Long short-term memory, End-to-end principle, Generative model, Line code, Markov chain Monte Carlo, Monte Carlo method, Theory}
}

@online{noauthor_161201010_nodate,
	title = {[1612.01010] {DeepBach}: a Steerable Model for Bach Chorales Generation},
	url = {https://arxiv.org/abs/1612.01010},
	urldate = {2020-02-08},
	file = {[1612.01010] DeepBach\: a Steerable Model for Bach Chorales Generation:C\:\\Users\\Vval\\Zotero\\storage\\4DYI98C9\\1612.html:text/html}
}

@article{xie_embedding_2019,
	title = {Embedding Symbolic Knowledge into Deep Networks},
	url = {http://arxiv.org/abs/1909.01161},
	abstract = {In this work, we aim to leverage prior symbolic knowledge to improve the performance of deep models. We propose a graph embedding network that projects propositional formulae (and assignments) onto a manifold via an augmented Graph Convolutional Network ({GCN}). To generate semantically-faithful embeddings, we develop techniques to recognize node heterogeneity, and semantic regularization that incorporate structural constraints into the embedding. Experiments show that our approach improves the performance of models trained to perform entailment checking and visual relation prediction. Interestingly, we observe a connection between the tractability of the propositional theory representation and the ease of embedding. Future exploration of this connection may elucidate the relationship between knowledge compilation and vector representation learning.},
	journaltitle = {{arXiv}:1909.01161 [cs]},
	author = {Xie, Yaqi and Xu, Ziwei and Kankanhalli, Mohan S. and Meel, Kuldeep S. and Soh, Harold},
	urldate = {2020-02-08},
	date = {2019-10-29},
	eprinttype = {arxiv},
	eprint = {1909.01161},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Multimedia, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\VGWP98V7\\Xie et al. - 2019 - Embedding Symbolic Knowledge into Deep Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\TVEXP3LB\\1909.html:text/html}
}

@article{ying_hierarchical_2019,
	title = {Hierarchical Graph Representation Learning with Differentiable Pooling},
	url = {http://arxiv.org/abs/1806.08804},
	abstract = {Recently, graph neural networks ({GNNs}) have revolutionized the field of graph representation learning through effectively learned node embeddings, and achieved state-of-the-art results in tasks such as node classification and link prediction. However, current {GNN} methods are inherently flat and do not learn hierarchical representations of graphs---a limitation that is especially problematic for the task of graph classification, where the goal is to predict the label associated with an entire graph. Here we propose {DiffPool}, a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion. {DiffPool} learns a differentiable soft cluster assignment for nodes at each layer of a deep {GNN}, mapping nodes to a set of clusters, which then form the coarsened input for the next {GNN} layer. Our experimental results show that combining existing {GNN} methods with {DiffPool} yields an average improvement of 5-10\% accuracy on graph classification benchmarks, compared to all existing pooling approaches, achieving a new state-of-the-art on four out of five benchmark data sets.},
	journaltitle = {{arXiv}:1806.08804 [cs, stat]},
	author = {Ying, Rex and You, Jiaxuan and Morris, Christopher and Ren, Xiang and Hamilton, William L. and Leskovec, Jure},
	urldate = {2020-02-12},
	date = {2019-02-20},
	eprinttype = {arxiv},
	eprint = {1806.08804},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Social and Information Networks},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\YLP45K4C\\Ying et al. - 2019 - Hierarchical Graph Representation Learning with Di.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\B3QWUGMP\\1806.html:text/html}
}

@inproceedings{darwiche_new_2004,
	title = {New Advances in Compiling {CNF} into Decomposable Negation Normal Form},
	abstract = {We describe a new algorithm for compiling conjunctive normal form ({CNF}) into Deterministic Decomposable Negation Normal (d-{DNNF}), which is a tractable logical form that permits model counting in polynomial time. The new implementation is based on latest techniques from both the {SAT} and {OBDD} literatures, and appears to be orders of magnitude more efficient than previous algorithms for this purpose. We compare our compiler experimentally to state of the art model counters, {OBDD} compilers, and previous {CNF}2dDNNF compilers.},
	booktitle = {{ECAI}},
	author = {Darwiche, Adnan},
	date = {2004},
	keywords = {Algorithm, Backtracking, Benchmark (computing), Binary decision diagram, Cobham's thesis, Compiler, Conjunctive normal form, Experiment, Negation normal form, Polynomial, Preferential entailment, Time complexity, Turing completeness}
}

@article{huang_counterpoint_2017,
	title = {{COUNTERPOINT} {BY} {CONVOLUTION}},
	abstract = {Machine learning models of music typically break up the task of composition into a chronological process, composing a piece of music in a single pass from beginning to end. On the contrary, human composers write music in a nonlinear fashion, scribbling motifs here and there, often revisiting choices previously made. In order to better approximate this process, we train a convolutional neural network to complete partial musical scores, and explore the use of blocked Gibbs sampling as an analogue to rewriting. Neither the model nor the generative procedure are tied to a particular causal direction of composition.},
	pages = {8},
	author = {Huang, Cheng-Zhi Anna and Cooijmans, Tim and Roberts, Adam and Courville, Aaron and Eck, Douglas},
	date = {2017},
	langid = {english},
	file = {Huang et al. - 2017 - COUNTERPOINT BY CONVOLUTION.pdf:C\:\\Users\\Vval\\Zotero\\storage\\Q5VIYNGU\\Huang et al. - 2017 - COUNTERPOINT BY CONVOLUTION.pdf:application/pdf}
}

@article{uria_deep_2014,
	title = {A Deep and Tractable Density Estimator},
	url = {http://arxiv.org/abs/1310.1757},
	abstract = {The Neural Autoregressive Distribution Estimator ({NADE}) and its real-valued version {RNADE} are competitive density models of multidimensional data across a variety of domains. These models use a fixed, arbitrary ordering of the data dimensions. One can easily condition on variables at the beginning of the ordering, and marginalize out variables at the end of the ordering, however other inference tasks require approximate inference. In this work we introduce an efficient procedure to simultaneously train a {NADE} model for each possible ordering of the variables, by sharing parameters across all these models. We can thus use the most convenient model for each inference task at hand, and ensembles of such models with different orderings are immediately available. Moreover, unlike the original {NADE}, our training procedure scales to deep models. Empirically, ensembles of Deep {NADE} models obtain state of the art density estimation performance.},
	journaltitle = {{arXiv}:1310.1757 [cs, stat]},
	author = {Uria, Benigno and Murray, Iain and Larochelle, Hugo},
	urldate = {2020-02-12},
	date = {2014-01-11},
	eprinttype = {arxiv},
	eprint = {1310.1757},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\N4E2969X\\Uria et al. - 2014 - A Deep and Tractable Density Estimator.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\RSFYNKJM\\1310.html:text/html}
}

@article{uria_neural_2016,
	title = {Neural Autoregressive Distribution Estimation},
	url = {http://arxiv.org/abs/1605.02226},
	abstract = {We present Neural Autoregressive Distribution Estimation ({NADE}) models, which are neural network architectures applied to the problem of unsupervised distribution and density estimation. They leverage the probability product rule and a weight sharing scheme inspired from restricted Boltzmann machines, to yield an estimator that is both tractable and has good generalization performance. We discuss how they achieve competitive performance in modeling both binary and real-valued observations. We also present how deep {NADE} models can be trained to be agnostic to the ordering of input dimensions used by the autoregressive product rule decomposition. Finally, we also show how to exploit the topological structure of pixels in images using a deep convolutional architecture for {NADE}.},
	journaltitle = {{arXiv}:1605.02226 [cs]},
	author = {Uria, Benigno and C√¥t√©, Marc-Alexandre and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
	urldate = {2020-02-12},
	date = {2016-05-27},
	eprinttype = {arxiv},
	eprint = {1605.02226},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\4HBS62JY\\Uria et al. - 2016 - Neural Autoregressive Distribution Estimation.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\U5SEWKJN\\1605.html:text/html}
}

@inreference{noauthor_linear_2019,
	title = {Linear temporal logic},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Linear_temporal_logic&oldid=930563841},
	abstract = {In logic, linear temporal logic or linear-time temporal logic ({LTL}) is a modal temporal logic with modalities referring to time. In {LTL}, one can encode formulae about the future of paths, e.g., a condition will eventually be true, a condition will be true until another fact becomes true, etc. It is a fragment of the more complex {CTL}*, which additionally allows branching time and quantifiers. Subsequently {LTL} is sometimes called propositional temporal logic, abbreviated {PTL}.
Linear temporal logic ({LTL}) is a fragment of first-order logic.{LTL} was first proposed for the formal verification of computer programs by Amir Pnueli in 1977.},
	booktitle = {Wikipedia},
	urldate = {2020-02-14},
	date = {2019-12-13},
	langid = {english},
	note = {Page Version {ID}: 930563841},
	file = {Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\YKBRQIC9\\index.html:text/html}
}

@online{noauthor_graph_nodate,
	title = {Graph Convolutional Networks {\textbar}¬†Thomas Kipf {\textbar} {PhD} Student @ University of Amsterdam},
	url = {https://tkipf.github.io/graph-convolutional-networks/},
	urldate = {2020-02-14},
	file = {Graph Convolutional Networks |¬†Thomas Kipf | PhD Student @ University of Amsterdam:C\:\\Users\\Vval\\Zotero\\storage\\L3IWAUKE\\graph-convolutional-networks.html:text/html}
}

@article{kipf_semi-supervised_2017,
	title = {Semi-Supervised Classification with Graph Convolutional Networks},
	url = {http://arxiv.org/abs/1609.02907},
	abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
	journaltitle = {{arXiv}:1609.02907 [cs, stat]},
	author = {Kipf, Thomas N. and Welling, Max},
	urldate = {2020-02-14},
	date = {2017-02-22},
	eprinttype = {arxiv},
	eprint = {1609.02907},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\N6UG7ITN\\Kipf et Welling - 2017 - Semi-Supervised Classification with Graph Convolut.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\KK3DI2PB\\1609.html:text/html}
}

@article{defferrard_convolutional_2017,
	title = {Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering},
	url = {http://arxiv.org/abs/1606.09375},
	abstract = {In this work, we are interested in generalizing convolutional neural networks ({CNNs}) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of {CNNs} in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical {CNNs}, while being universal to any graph structure. Experiments on {MNIST} and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.},
	journaltitle = {{arXiv}:1606.09375 [cs, stat]},
	author = {Defferrard, Micha√´l and Bresson, Xavier and Vandergheynst, Pierre},
	urldate = {2020-02-14},
	date = {2017-02-05},
	eprinttype = {arxiv},
	eprint = {1606.09375},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Vval\\Zotero\\storage\\TFYD5RWA\\Defferrard et al. - 2017 - Convolutional Neural Networks on Graphs with Fast .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Vval\\Zotero\\storage\\HNNWWNJV\\1606.html:text/html}
}